--conf spark.driver.extraLibraryPath=/usr/lib/hadoop2/lib/native --conf spark.storage.level=MEMORY_AND_DISK_SER --conf spark.sql.shuffle.partitions=12000 --conf spark.sql.adaptive.enabled=true --conf spark.executor.cores=3 --conf spark.executor.memory=13945M --conf spark.executor.memoryOverhead=11000M --conf spark.driver.memory=13945M --conf spark.driver.memoryOverhead=11000M --conf spark.driver.cores=3 --conf spark.sql.autoBroadcastJoinThreshold=319430400 --conf spark.driver.maxResultSize=15g --conf spark.sql.crossJoin.enabled=true --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.initialExecutors=10 --conf spark.dynamicAllocation.minExecutors=5 --conf spark.dynamicAllocation.executorIdleTimeout=1500s --conf spark.hadoop.hive.exec.dynamic.partition=true --conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict --conf spark.hadoop.hive.exec.max.dynamic.partitions.pernode=1500 --conf spark.hadoop.hive.exec.max.dynamic.partitions=10000 --conf spark.sql.broadcastTimeout=7200 --conf spark.network.timeout=1800s --conf spark.executor.heartbeatInterval=900s --conf spark.storage.blockManagerSlaveTimeoutMs=1800s --conf spark.hadoop.fs.s3a.committer.name=partitioned --conf spark.hadoop.fs.s3a.committer.staging.conflict-mode=append --conf spark.hadoop.hive.qubole.consistent.loadpartition=true --conf spark.hadoop.mapreduce.output.textoutputformat.overwrite=true --conf spark.hadoop.orc.overwrite.output.file=true --conf spark.qubole.outputformat.overwriteFileInWrite=true --conf spark.qubole.sql.hive.useDirectWrites=true --conf spark.qubole.sql.hive.useDirectWrites.ctasCommand.enabled=true --conf spark.sql.qubole.directWrites.dynamicPartitionOverwrite.enabled=true --conf spark.sql.qubole.directWrites.hive.ctasCommand.enabled=true --conf spark.sql.qubole.dynamicFilter.enabled=true --conf spark.sql.qubole.dynamicFilter.pruning.enabled=true --conf spark.sql.qubole.dynamic.filter.enabled=true --conf spark.sql.qubole.partitionDiscoverer=true --conf spark.sql.orc.filterPushdown=true --conf spark.shuffle.registration.maxAttempts=4 --conf spark.shuffle.service.index.cache.size=2048 --conf spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2 --conf spark.shuffle.service.enabled=true --conf spark.sql.parquet.recordLevelFilter.enable=true --conf spark.max.fetch.failures.per.stage=10 --conf spark.shuffle.file.buffer=1MB --conf spark.shuffle.registration.timeout=1800 --conf spark.blacklist.enabled=false --conf spark.hadoop.hive.exec.compress.output=true --conf spark.hadoop.parquet.compression=snappy --conf spark.io.compression.lz4.blockSize=512KB --conf spark.rdd.compress=true --conf spark.shuffle.compress=true --conf spark.shuffle.spill.compress=true --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryoserializer.buffer.max=1024 --conf spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch --conf spark.sql.hive.convertMetastoreParquet=false --conf spark.sql.hive.convertMetastoreOrc=false --conf spark.hadoop.mapred.input.dir.recursive=true --conf spark.sql.qubole.list.files.v2=true --conf spark.hadoop.mapreduce.input.fileinputformat.input.dir.recursive=true --conf spark.hive.mapred.supports.subdirectories=true