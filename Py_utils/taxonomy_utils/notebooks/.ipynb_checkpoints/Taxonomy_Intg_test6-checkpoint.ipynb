{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, inspect, re\n",
    "sys.path.append(\"/home/vbhargava/feature_test0/msaction_backend/common/BU3.0_core/util/Py_utils/taxonomy_utils\")\n",
    "import time, logging\n",
    "import pandas as pd \n",
    "numeric_level = getattr(logging, 'INFO', None)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "logging.basicConfig(level=numeric_level,\n",
    "                        format='%(asctime)s %(levelname)s %(name)s: %(message)s',\n",
    "                        handlers=[stdout_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.s3_ops import S3_OPs\n",
    "from libs.s3_stream import S3Stream\n",
    "from libs.configs import Config\n",
    "from libs.nio_executor import NIO\n",
    "from libs import utils\n",
    "from libs import xml_writer \n",
    "from libs import decorator\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '/home/vbhargava/feature_test0/msaction_backend/customers/raj_ford_test/common/config/inputs/platform_config.xml'\n",
    "lmt_src = 's3://qubole-ford/taxonomy_cs/test1/src/'\n",
    "lmt_data = 's3://qubole-ford/taxonomy_cs/test1/data/'\n",
    "config_input_loc = '/home/vbhargava/feature_test0/temp/taxo_config_xmls/'\n",
    "config_file_name = 'test.xml'\n",
    "dn_version = '12.1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = Config.get_qubole_config(config)\n",
    "ACCESS_KEY=config_data['access_key']\n",
    "SECRET_KEY=config_data['secret_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TG_EXTRACT_REGEX = '^.*?/([a-zA-Z]+\\-?[0-9]*)/$' \n",
    "FILE_EXTRACT_REGEX = '^.*/([a-zA-Z0-9.\\-_]{0,255}.csv)$' #'^.*/([a-zA-Z0-9.\\-_]{0,255}.csv)$'\n",
    "TARGET_EXTRACT_REGEX ='^.*,?(target_[A-Za-z0-9_-]+).*$'\n",
    "VALID_FILE_KEY_REGEX = '^(.*/([a-zA-Z]+\\-?[0-9]*)?/)?(([a-zA-Z]+\\-?[0-9]*?)_([0-9]{4}-[0-9]{2}-[0-9]{2}?)_([a-zA-Z0-9.\\-_]+?).csv?)$'\n",
    "KEY_REGEX = '^[Kk]ey_[A-Za-z0-9_]{2,30}$'\n",
    "TARGET_REGEX = '^[Tt]arget_[A-Za-z0-9_]{2,30}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_ops = S3_OPs(ACCESS_KEY, SECRET_KEY)\n",
    "\n",
    "def filename_by_key(key):\n",
    "    return get_val_by_regex(key, FILE_EXTRACT_REGEX, error_msg=\"Not vaild key for taxonomy data csv file\")\n",
    "\n",
    "def find_by_data_tg(key, regex):\n",
    "    return get_val_by_regex(key, regex, error_msg=\"Not vaild taxonomy data dir\")\n",
    "\n",
    "        \n",
    "def get_val_by_regex(key, regex, error_msg=\"can't be extract a val.\"):\n",
    "    matched = re.findall(regex, key)\n",
    "    if len(matched) > 0:\n",
    "        return matched[0]\n",
    "    else:\n",
    "        raise Exception(error_msg)\n",
    "        \n",
    "def get_data_n_schema(tg, data_files_loc):\n",
    "    data_file_lock_detail = s3_ops.get_bucket_name(data_files_loc)\n",
    "    files = s3_ops.list_complete(data_file_lock_detail['bucket'], data_file_lock_detail['key'])\n",
    "    res = {}\n",
    "    if len(files)>0:\n",
    "        s3_stream = S3Stream(ACCESS_KEY, SECRET_KEY)\n",
    "        schema = s3_stream.get_header(s3_ops.get_full_s3_path(data_file_lock_detail['bucket'],files[0]['Key']))\n",
    "        #res[tg]={'schema':schema, 'files': files}\n",
    "        res['schema'] = {tg:schema}\n",
    "        res['files'] = {tg:files}\n",
    "    return res\n",
    "\n",
    "def extract_schema(schema):\n",
    "    return schema.replace(\" \",\"\").lower()\n",
    "\n",
    "def validate_schema(schema):\n",
    "    if schema=='': \n",
    "        return {'IsValid' : False, 'schema': schema, 'message' : \"Schema shouldn't be empty\"}\n",
    "    tokens = schema.split(',')\n",
    "    if len(tokens) < 2:\n",
    "         return {'IsValid' : False, 'schema': schema, 'message' : \"Schema should have at least 2 columns\"}\n",
    "    \n",
    "    key_cnt = 0\n",
    "    target_cnt = 0\n",
    "    invalid_headers = []\n",
    "    columns = defaultdict(list)\n",
    "    res = {}\n",
    "    target_col = None\n",
    "    key_cols_set = set()\n",
    "    for t in tokens:\n",
    "        t = t.strip()\n",
    "        if re.match(TARGET_REGEX, t):\n",
    "            target_cnt = target_cnt + 1\n",
    "            target_col = t\n",
    "        elif re.match(KEY_REGEX, t):\n",
    "            key_cnt = key_cnt + 1\n",
    "            key_cols_set.add(t)\n",
    "        else:\n",
    "            invalid_headers.append(t)\n",
    "        columns[t.lower()].append(1)\n",
    "\n",
    "    error_msgs=[]\n",
    "    if target_cnt != 1 :\n",
    "        error_msgs.append(\"Exact one Target column is required!\")\n",
    "    if key_cnt < 1 :\n",
    "        error_msgs.append(\"At least one Key column is required!\")\n",
    "    if len(invalid_headers) > 0 :\n",
    "        error_msgs.append(\"All given columns should Key or Target!\")\n",
    "    for k, v in columns.items():\n",
    "\n",
    "        if len(v) > 1:\n",
    "            print(\"--\")\n",
    "            error_msgs.append(\"Same name: {} should not represent more than one column in schema! cols names are case insensitive. \".format(k))\n",
    "\n",
    "    if len(error_msgs) > 0:\n",
    "        return {'IsValid' : False, 'schema': schema, 'errors' : \" \\n\".join(error_msgs)}\n",
    "    #print(str(key_cnt)+\":\"+str(target_cnt)+\":\"+str(invalid_headers)+\":\"+str(columns))\n",
    "    return {'IsValid' : True, 'Schema': schema.replace(\" \",\"\").lower(), \n",
    "            'TargetCol' : target_col, 'KeyColsSet' : key_cols_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_detail(lmt_src, lmt_data, access_key, secret_key):\n",
    "#     Valid data Taxonomy Grps\n",
    "    \n",
    "    #\n",
    "    lmt_data_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "    lmt_data_loc_bucket = lmt_data_loc_detail['bucket']\n",
    "    lmt_data_loc_key = lmt_data_loc_detail['key']\n",
    "    valid_tg_list_res = s3_ops.list_subdirs(lmt_data_loc_detail['bucket'],lmt_data_loc_detail['key'],)\n",
    "    \n",
    "    valid_tgrp_loc_list = [ [find_by_data_tg(item['Prefix'], TG_EXTRACT_REGEX), \n",
    "                         '{}{}'.format(lmt_data, find_by_data_tg(item['Prefix'], TG_EXTRACT_REGEX))] \n",
    "                       for item in valid_tg_list_res]\n",
    "    \n",
    "    collected = NIO.decorated_run_io(task=get_data_n_schema, task_n_args_list=valid_tgrp_loc_list, max_workers=25,)\n",
    "#     return collected\n",
    "    tg_data_schema_dict = {k:extract_schema(v)  for item in collected for k, v in item['result']['schema'].items()}\n",
    "    tg_data_files_dict = {k:{filename_by_key(u['Key']):u for u in v } for item in collected for k, v in item['result']['files'].items()}\n",
    "    target_data_tg_dict = {re.findall(TARGET_EXTRACT_REGEX,V)[0]: K for K, V in tg_data_schema_dict.items()}\n",
    "    \n",
    "    return tg_data_schema_dict, tg_data_files_dict,target_data_tg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_file(key:str='', regex = VALID_FILE_KEY_REGEX):\n",
    "    if re.match(regex, key) is None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_info(key:str='', regex = VALID_FILE_KEY_REGEX):\n",
    "    matched = re.findall(regex, key)\n",
    "    return {\n",
    "            'KeyDirPath' : matched[0][0],\n",
    "            'ParentDir' : matched[0][1],\n",
    "            'FileName' : matched[0][2],\n",
    "            'FileGrp' :  matched[0][3],\n",
    "            'Date' :  matched[0][4],\n",
    "            'ClientName' : matched[0][5]\n",
    "           }\n",
    "def extract_info_with_bucket(key:str='', bucket = ''):\n",
    "    res = extract_info(key)\n",
    "    res.update({'Bucket' : bucket})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_tg(collected, tg_files_dict_type='new_tg_files_dict'):\n",
    "    collect = defaultdict(dict)\n",
    "    tg_f_gen = (item['result'][tg_files_dict_type] for item in collected if len(item['result'][tg_files_dict_type]) > 0)\n",
    "    tg_f_gen2 = (collect[tg].update({filename: file_dict})  for item in tg_f_gen for tg, file_detail_dict in item.items() for filename, file_dict in file_detail_dict.items())\n",
    "    [ i for i in tg_f_gen2]\n",
    "    tg = dict(collect)\n",
    "    return tg\n",
    "\n",
    "def grouped_flag_dict(collected, flag_dict_type='schema_tg_dict'):\n",
    "    f_gen = (item['result'][flag_dict_type] for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    collect = defaultdict(set)\n",
    "    f_gen2 = (collect[K].add(V)  for item in f_gen for K, V in item.items())\n",
    "    [ i for i in f_gen2]\n",
    "    res = dict(collect)\n",
    "    return res\n",
    "\n",
    "def grouped_set_of_flags_dict(collected, flag_dict_type='schema_tg_dict'):\n",
    "    f_gen = (item['result'][flag_dict_type] for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    collect = defaultdict(set)\n",
    "    f_gen2 = (collect[K].update(V)  for item in f_gen for K, V in item.items())\n",
    "    [ i for i in f_gen2]\n",
    "    res = dict(collect)\n",
    "    return res\n",
    "\n",
    "def grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'):\n",
    "    res_set=set()\n",
    "    f_gen = (res_set.update(item['result'][flag_dict_type]) for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    [ i for i in f_gen]\n",
    "    return res_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def file_process_task(src_file_details):\n",
    "    \n",
    "    invalid_schema_files = set()\n",
    "\n",
    "    target_already_exist_files = set()\n",
    "    \n",
    "    ''' {'key_evt_advertiser_key,target_evt_advertiser_name': {'tg1', 'tg2', ...}}'''\n",
    "    schema_tg_dict = {}\n",
    "    \n",
    "    ''' {'target_evt_advertiser_name': {'tg1', 'tg2', ...}}'''\n",
    "    target_tg_dict = {}\n",
    "\n",
    "    ''' {'tg': {'key_evt_advertiser_key,target_evt_advertiser_name', '',...}}'''\n",
    "    new_tg_schema_dict = {}\n",
    "    ''' {'tg': {'AdvertiserReporting_2020-06-01_ford.csv': {file detailed obj dict} }  }'''\n",
    "    new_tg_files_dict = {}\n",
    "    ''' {'tg': {'AdvertiserReporting_2020-06-01_ford.csv': {file detailed obj dict} }  }'''\n",
    "    existing_tg_files_dict = {}\n",
    "\n",
    "\n",
    "    # tg_data_schema_dict = \n",
    "    # tg_data_files_dict = \n",
    "    # target_data_tg_dict = \n",
    "\n",
    "#     src_file_details = valid_file_arg[0]\n",
    "    src_file_loc = s3_ops.get_full_s3_path(src_file_details['Bucket'], src_file_details['Key'])\n",
    "\n",
    "    s3_stream = S3Stream(ACCESS_KEY, SECRET_KEY)\n",
    "    schema =  s3_stream.get_header(src_file_loc)\n",
    "    #schema = 'key_evt_advertiser_key, targe_evt_advertiser_name'\n",
    "    validate_res = validate_schema(schema)\n",
    "    if validate_res['IsValid']:\n",
    "        \n",
    "        \n",
    "        \n",
    "        tg = src_file_details['FileGrp']\n",
    "        file_name = src_file_details['FileName']\n",
    "        \n",
    "        if tg_data_schema_dict.get(tg) is None or tg_data_schema_dict.get(tg) != validate_res['Schema']:\n",
    "                \n",
    "#             data_tg_for_target = target_data_tg_dict.get(validate_res['TargetCol'])\n",
    "#             if  data_tg_for_target is not None:# and data_tg_for_target != tg:\n",
    "#                 target_already_exist_files.add((src_file_loc, data_tg_for_target))\n",
    "#             else:\n",
    "            new_tg_schema_dict[tg] = validate_res['Schema']\n",
    "            new_tg_files_dict[tg] = {file_name: src_file_details}\n",
    "        else:\n",
    "            existing_tg_files_dict[tg] = {file_name: src_file_details}\n",
    "        \n",
    "        src_file_details['Schema'] = validate_res['Schema']\n",
    "        schema_tg_dict[validate_res['Schema']] = tg\n",
    "        target_tg_dict[validate_res['TargetCol']] = tg\n",
    "\n",
    "    else:\n",
    "        invalid_schema_files.add((src_file_loc, schema, validate_res['errors']))\n",
    "\n",
    "    return {'invalid_schema_files': invalid_schema_files,\n",
    "#             'target_already_exist_files':target_already_exist_files,\n",
    "            'schema_tg_dict': schema_tg_dict,\n",
    "            'target_tg_dict':target_tg_dict,\n",
    "            'new_tg_schema_dict': new_tg_schema_dict,\n",
    "            'new_tg_files_dict' : new_tg_files_dict,\n",
    "            'existing_tg_files_dict' : existing_tg_files_dict\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "def src_list_page_process_task(list_page):\n",
    "    \n",
    "    lmt_src_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    lmt_src_loc_bucket = lmt_src_loc_detail['bucket']\n",
    "    lmt_src_loc_key = lmt_src_loc_detail['key']\n",
    "    \n",
    "    invalid_files_set = { s3_ops.get_full_s3_path(lmt_src_loc_detail['bucket'], item['Key']) for item in list_page if  not is_valid_file(key=item['Key'])}\n",
    "    valid_file_set = [[utils.dict_append(extract_info_with_bucket(item['Key'], lmt_src_loc_detail['bucket']),item)] for item in list_page if  is_valid_file(key=item['Key']) ]\n",
    "    collected = NIO.decorated_run_io(task=file_process_task, task_n_args_list=valid_file_set, max_workers=25,)\n",
    "#     return collected\n",
    "    return {'invalid_files_set' : invalid_files_set,\n",
    "            'invalid_schema_files': grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'),\n",
    "#             'target_already_exist_files' : grouped_set_of_flags(collected, flag_dict_type='target_already_exist_files'),\n",
    "            'schema_tg_dict': grouped_flag_dict(collected, flag_dict_type='schema_tg_dict'),\n",
    "            'target_tg_dict': grouped_flag_dict(collected, flag_dict_type='target_tg_dict'),\n",
    "            'new_tg_schema_dict': grouped_flag_dict(collected, flag_dict_type='new_tg_schema_dict'),\n",
    "            'new_tg_files_dict' : grouped_tg(collected, 'new_tg_files_dict'),\n",
    "            'existing_tg_files_dict' : grouped_tg(collected, 'existing_tg_files_dict')\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_src_detail(maxKeysPerReq=3):\n",
    "    lmt_src_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    lmt_src_loc_bucket = lmt_src_loc_detail['bucket']\n",
    "    lmt_src_loc_key = lmt_src_loc_detail['key']\n",
    "    page_generator = s3_ops.list_gen(lmt_src_loc_bucket, lmt_src_loc_key, maxKeysPerReq=maxKeysPerReq, )\n",
    "    page_args_generator = ([page] for page in page_generator)\n",
    "    #list_page = [i for i in page_generator][0]\n",
    "    collected = NIO.decorated_run_with_args_generator(task=src_list_page_process_task, args_generator=page_args_generator, is_kernal_thread=True,)\n",
    "    \n",
    "    return {'invalid_files_set' : grouped_set_of_flags(collected, flag_dict_type='invalid_files_set'),\n",
    "            'invalid_schema_files': grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'),\n",
    "#             'target_already_exist_files' : grouped_set_of_flags(collected, flag_dict_type='target_already_exist_files'),\n",
    "            'schema_tg_dict': grouped_set_of_flags_dict(collected, flag_dict_type='schema_tg_dict'),\n",
    "            'target_tg_dict': grouped_set_of_flags_dict(collected, flag_dict_type='target_tg_dict'),\n",
    "            'new_tg_schema_dict': grouped_set_of_flags_dict(collected, flag_dict_type='new_tg_schema_dict'),\n",
    "            'new_tg_files_dict' : grouped_tg(collected, 'new_tg_files_dict'),\n",
    "            'existing_tg_files_dict' : grouped_tg(collected, 'existing_tg_files_dict')\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' E.g. '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def s3_copy_into_data_loc_task(tg, file_name, src_file, src_size, dry_run=True):\n",
    "#     data_file_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "    src_file_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    src_s3 = 's3://{}/{}'.format(src_file_loc_detail['bucket'], src_file)\n",
    "    dest_s3 = '{}{}/{}'.format(lmt_data,tg, file_name)\n",
    "    if dry_run:\n",
    "        print(\"[dry_run]: S3 copy from {} to {}\".format(src_s3, dest_s3))\n",
    "    else:\n",
    "        pass\n",
    "        #s3_ops.copy(src=src_s3, dest = dest_s3, src_size=src_size)\n",
    "    return 'Copied Successfully! by task'\n",
    "\n",
    "\n",
    "def s3_remove_at_data_loc_task(file,  dry_run=True):\n",
    "    data_file_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "#     src_file_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "#     src_s3 = 's3://{}/{}'.format(lmt_src, src_file)\n",
    "    \n",
    "    if dry_run:\n",
    "        file_loc = 's3://{}/{}'.format(data_file_loc_detail['bucket'], file)\n",
    "        print(\"[dry_run]: S3 delete from {} \".format(file_loc))\n",
    "    else:\n",
    "        pass\n",
    "        #s3_ops.delete_file(data_file_loc_detail['bucket'], file)\n",
    "    return 'Deleted Successfully! by task'\n",
    "\n",
    "''' E.g. '''\n",
    "# s3_copy_into_data_loc_task('tg5', 'tg5_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 48 )\n",
    "# s3_remove_at_data_loc_task('taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 16:54:14,315:33792 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:54:14,316:33792 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,317:33792 ThreadPoolExecutor-0_0 (task-0): passed args :['tg1', 's3://qubole-ford/taxonomy_cs/test1/data/tg1']\n",
      "\n",
      "2020-11-02 16:54:14,318:33792 ThreadPoolExecutor-0_1 (task-1): passed args :['tg2', 's3://qubole-ford/taxonomy_cs/test1/data/tg2']\n",
      "\n",
      "2020-11-02 16:54:14,318:33792 ThreadPoolExecutor-0_0 (task-0): running\n",
      "\n",
      "2020-11-02 16:54:14,318:33792 ThreadPoolExecutor-0_2 (task-2): passed args :['tg3', 's3://qubole-ford/taxonomy_cs/test1/data/tg3']\n",
      "\n",
      "2020-11-02 16:54:14,320:33792 ThreadPoolExecutor-0_3 (task-3): passed args :['tg5', 's3://qubole-ford/taxonomy_cs/test1/data/tg5']\n",
      "\n",
      "2020-11-02 16:54:14,320:33792 ThreadPoolExecutor-0_4 (task-4): passed args :['tg6', 's3://qubole-ford/taxonomy_cs/test1/data/tg6']\n",
      "\n",
      "2020-11-02 16:54:14,321:33792 ThreadPoolExecutor-0_5 (task-5): passed args :['tg7', 's3://qubole-ford/taxonomy_cs/test1/data/tg7']\n",
      "\n",
      "2020-11-02 16:54:14,321:33792 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,321:33792 ThreadPoolExecutor-0_1 (task-1): running\n",
      "\n",
      "2020-11-02 16:54:14,324:33792 ThreadPoolExecutor-0_2 (task-2): running\n",
      "\n",
      "2020-11-02 16:54:14,325:33792 ThreadPoolExecutor-0_3 (task-3): running\n",
      "\n",
      "2020-11-02 16:54:14,325:33792 ThreadPoolExecutor-0_4 (task-4): running\n",
      "\n",
      "2020-11-02 16:54:14,327:33792 ThreadPoolExecutor-0_5 (task-5): running\n",
      "\n",
      "2020-11-02 16:54:14,422:33792 ThreadPoolExecutor-0_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:54:14,451:33792 ThreadPoolExecutor-0_2 (task-2): done\n",
      "\n",
      "2020-11-02 16:54:14,457:33792 ThreadPoolExecutor-0_3 (task-3): done\n",
      "\n",
      "2020-11-02 16:54:14,458:33792 ThreadPoolExecutor-0_5 (task-5): done\n",
      "\n",
      "2020-11-02 16:54:14,463:33792 ThreadPoolExecutor-0_1 (task-1): done\n",
      "\n",
      "2020-11-02 16:54:14,467:33792 ThreadPoolExecutor-0_4 (task-4): done\n",
      "\n",
      "2020-11-02 16:54:14,468:33792 MainThread run_blocking_tasks: exiting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Get Existing State of System'''\n",
    "tg_data = extract_data_detail(lmt_src, lmt_data, ACCESS_KEY, SECRET_KEY)\n",
    "tg_data_schema_dict = tg_data[0]\n",
    "tg_data_files_dict = tg_data[1]\n",
    "target_data_tg_dict = tg_data[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 16:54:14,477   process-id:33792 run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:54:14,477   process-id:33792 run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,537   process-id:33825   (task-0): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg0_202-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"535b60451f6d20c2826b045438a50fb9\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"7a5d08cbb4c718d16851d1f2b57ffc50\"', 'Size': 27, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 21, 21, 36, tzinfo=tzlocal()), 'ETag': '\"b19c288a2ef5e2ec6739cac3674391a6\"', 'Size': 28, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 16:54:14,540   process-id:33825   (task-0): running\n",
      "\n",
      "2020-11-02 16:54:14,543:33825 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:54:14,544:33825 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,546:33825 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg0_2020-11-02_ford.csv', 'FileGrp': 'tg0', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"7a5d08cbb4c718d16851d1f2b57ffc50\"', 'Size': 27, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,547:33825 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg0_2020-11-03_ford.csv', 'FileGrp': 'tg0', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 21, 21, 36, tzinfo=tzlocal()), 'ETag': '\"b19c288a2ef5e2ec6739cac3674391a6\"', 'Size': 28, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,547:33825 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,547:33825 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 16:54:14,548:33825 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 16:54:14,573   process-id:33826   (task-1): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg10_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"1aa284fe1180b5d4d776e26ff8a03358\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 16:54:14,576   process-id:33826   (task-1): running\n",
      "\n",
      "2020-11-02 16:54:14,579:33826 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:54:14,580:33826 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,581:33826 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg10_2020-11-01_ford.csv', 'FileGrp': 'tg10', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg10_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"1aa284fe1180b5d4d776e26ff8a03358\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,582:33826 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg11_2020-11-01_ford.csv', 'FileGrp': 'tg11', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,582:33826 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg1_2020-11-01_ford.csv', 'FileGrp': 'tg1', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,583:33826 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,583:33826 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 16:54:14,584:33826 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 16:54:14,585:33826 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 16:54:14,606:33825 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 16:54:14,608:33825 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:54:14,611:33825 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 16:54:14,612   process-id:33825   (task-0): done\n",
      "\n",
      "2020-11-02 16:54:14,642:33826 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 16:54:14,651:33826 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:54:14,656:33826 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 16:54:14,657:33826 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 16:54:14,658   process-id:33826   (task-1): done\n",
      "\n",
      "2020-11-02 16:54:14,686   process-id:33827   (task-2): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 16:54:14,689   process-id:33827   (task-2): running\n",
      "\n",
      "2020-11-02 16:54:14,691:33827 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:54:14,692:33827 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,694:33827 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg1_2020-11-02_ford.csv', 'FileGrp': 'tg1', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,695:33827 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-01_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,695:33827 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-02_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,695:33827 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,695:33827 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 16:54:14,696:33827 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 16:54:14,697:33827 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 16:54:14,721   process-id:33828   (task-3): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 16:54:14,724   process-id:33828   (task-3): running\n",
      "\n",
      "2020-11-02 16:54:14,727:33828 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:54:14,728:33828 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,729:33828 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-03_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,731:33828 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-01_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,731:33828 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-02_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,731:33828 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,732:33828 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 16:54:14,733:33828 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 16:54:14,741   process-id:33829   (task-4): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 16:54:14,743   process-id:33829   (task-4): running\n",
      "\n",
      "2020-11-02 16:54:14,746:33829 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:54:14,747:33829 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,749:33829 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-03_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,749:33827 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:54:14,750:33829 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-01_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,750:33829 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-02_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,753:33827 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 16:54:14,750:33829 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,751:33829 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 16:54:14,734:33828 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 16:54:14,758   process-id:33830   (task-5): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-04_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"f87ad6041aa111ac6b6d0776be1c774f\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 16:54:14,752:33829 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 16:54:14,761   process-id:33830   (task-5): running\n",
      "\n",
      "2020-11-02 16:54:14,764:33830 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:54:14,766:33830 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,767:33830 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-03_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,768:33830 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-04_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-04', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-04_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"f87ad6041aa111ac6b6d0776be1c774f\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,769:33830 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 16:54:14,772:33827 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 16:54:14,753:33829 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 16:54:14,774:33827 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 16:54:14,769:33830 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg6_2020-11-01_ford.csv', 'FileGrp': 'tg6', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 16:54:14,778   process-id:33827   (task-2): done\n",
      "\n",
      "2020-11-02 16:54:14,782   process-id:33831   (task-6): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 16:54:14,770:33830 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,785   process-id:33831   (task-6): running\n",
      "\n",
      "2020-11-02 16:54:14,770:33830 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 16:54:14,788:33831 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:54:14,789:33831 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,791:33831 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg6_2020-11-02_ford.csv', 'FileGrp': 'tg6', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,792:33831 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-01_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,778:33830 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 16:54:14,792:33831 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-02_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,792:33831 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,793:33831 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 16:54:14,801   process-id:33792 run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,803   process-id:33832   (task-7): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg7_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"d54b283d90621ca6b19c85f4c96d4b8f\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"8d575874cb97b2d601ae8542aaf11431\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 16:54:14,804:33828 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 16:54:14,806   process-id:33832   (task-7): running\n",
      "\n",
      "2020-11-02 16:54:14,806:33828 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:54:14,794:33831 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 16:54:14,809:33832 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:54:14,811:33832 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,811:33828 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 16:54:14,813:33828 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 16:54:14,812:33832 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-03_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"d54b283d90621ca6b19c85f4c96d4b8f\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,814   process-id:33828   (task-3): done\n",
      "\n",
      "2020-11-02 16:54:14,814:33832 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg8_2020-11-01_ford.csv', 'FileGrp': 'tg8', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"8d575874cb97b2d601ae8542aaf11431\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,814:33829 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:54:14,816:33832 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 16:54:14,816:33829 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 16:54:14,795:33831 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 16:54:14,814:33832 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:54:14,822:33829 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 16:54:14,824:33829 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 16:54:14,825   process-id:33829   (task-4): done\n",
      "\n",
      "2020-11-02 16:54:14,824:33830 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:54:14,828:33830 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 16:54:14,815:33832 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 16:54:14,814:33832 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg9_2020-11-01_ford.csv', 'FileGrp': 'tg9', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 16:54:14,840:33832 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 16:54:14,851:33830 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 16:54:14,853:33830 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 16:54:14,853:33831 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 16:54:14,855   process-id:33830   (task-5): done\n",
      "\n",
      "2020-11-02 16:54:14,858:33831 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:54:14,860:33831 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 16:54:14,862:33831 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 16:54:14,863   process-id:33831   (task-6): done\n",
      "\n",
      "2020-11-02 16:54:14,869:33832 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:54:14,871:33832 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 16:54:14,947:33832 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 16:54:14,948:33832 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 16:54:14,950   process-id:33832   (task-7): done\n",
      "\n",
      "2020-11-02 16:54:14,952   process-id:33792 run_blocking_tasks: exiting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_delta = extract_src_detail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_target_splitter(schema = ''):\n",
    "    tokens = schema.split(',')\n",
    "    key_cols = []\n",
    "    for t in tokens:\n",
    "        t = t.strip()\n",
    "        if re.match(TARGET_REGEX, t):\n",
    "            target_col = t\n",
    "        elif re.match(KEY_REGEX, t):\n",
    "            key_cols.append(t)\n",
    "        else:\n",
    "            raise Exception(\"Not a valid schema\")\n",
    "    return [{'target': target_col, 'key_cols' : key_cols}]\n",
    "\n",
    "\n",
    "@decorator.box_logged\n",
    "def log_report(report_title='Report', list_of_row_dict=[], columns:list=[], header_align = 'right', sort_by= None, ascending = True):\n",
    "    pd.set_option(\"display.colheader_justify\", header_align)\n",
    "    df = pd.DataFrame(list_of_row_dict, columns=columns) \n",
    "    if sort_by is not None:\n",
    "        df = df.sort_values(by=sort_by, ascending=ascending)\n",
    "        df = df.reset_index()\n",
    "        df = df.drop(columns=['index'])\n",
    "        df = df.set_index(' **      ' + df.index.astype(str) )\n",
    "#     df1 = df.reindex(columns=['Taxonomy_Grp','File','Date', 'Schema'])\n",
    "    #df[df.columns[new_order]]\n",
    "    #df = df.transpose()\n",
    "    logging.info(\"\")\n",
    "    logging.info(report_title)\n",
    "    logging.info(\"\")\n",
    "    logging.info(str(df))\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"\")\n",
    "\n",
    "\n",
    "class Taxonomy_Grp:\n",
    "    \n",
    "    def __init__(self, tg_name, key_cols=[], target_col='', data_location=''):\n",
    "        self.tg_name = tg_name\n",
    "        self.key_cols = key_cols\n",
    "        self.target_col = target_col\n",
    "        self.location =os.path.join(data_location, tg_name)\n",
    "        \n",
    "    def get_dict(self):\n",
    "        if self.target_col == '':\n",
    "            return {'tg_name': self.tg_name}\n",
    "        \n",
    "        return {'tg_name': self.tg_name, \n",
    "                'key_cols': self.key_cols, \n",
    "                'target_col': self.target_col, \n",
    "                'location': self.location}\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.target_col == '':\n",
    "            return 'tg_name: {}'.format(self.tg_name)\n",
    "        return 'tg_name: {}, key_cols: {}, target_col: {}, location: {}'.format(self.tg_name, self.key_cols, self.target_col,self.location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 16:58:13,010:33792 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:58:13,011:33792 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:58:13,012:33792 ThreadPoolExecutor-3_0 (task-0): passed args :['taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv']\n",
      "\n",
      "2020-11-02 16:58:13,012:33792 ThreadPoolExecutor-3_1 (task-1): passed args :['taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv']\n",
      "\n",
      "2020-11-02 16:58:13,013:33792 ThreadPoolExecutor-3_2 (task-2): passed args :['taxonomy_cs/test1/data/tg3/tg3_2020-11-01_ford.csv']\n",
      "\n",
      "2020-11-02 16:58:13,013:33792 ThreadPoolExecutor-3_3 (task-3): passed args :['taxonomy_cs/test1/data/tg3/tg3_2020-11-02_ford.csv']\n",
      "\n",
      "2020-11-02 16:58:13,013:33792 ThreadPoolExecutor-3_0 (task-0): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv \n",
      "2020-11-02 16:58:13,014:33792 ThreadPoolExecutor-3_4 (task-4): passed args :['taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv']\n",
      "\n",
      "2020-11-02 16:58:13,014:33792 ThreadPoolExecutor-3_5 (task-5): passed args :['taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv']\n",
      "\n",
      "2020-11-02 16:58:13,015:33792 ThreadPoolExecutor-3_6 (task-6): passed args :['taxonomy_cs/test1/data/tg5/tg5_2020-11-05_ford.csv']\n",
      "\n",
      "2020-11-02 16:58:13,015:33792 ThreadPoolExecutor-3_1 (task-1): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv \n",
      "2020-11-02 16:58:13,015:33792 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:58:13,016:33792 ThreadPoolExecutor-3_2 (task-2): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg3/tg3_2020-11-01_ford.csv 2020-11-02 16:58:13,017:33792 ThreadPoolExecutor-3_3 (task-3): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg3/tg3_2020-11-02_ford.csv 2020-11-02 16:58:13,019:33792 ThreadPoolExecutor-3_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:58:13,019:33792 ThreadPoolExecutor-3_4 (task-4): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv \n",
      "2020-11-02 16:58:13,020:33792 ThreadPoolExecutor-3_5 (task-5): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv \n",
      "2020-11-02 16:58:13,021:33792 ThreadPoolExecutor-3_6 (task-6): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-05_ford.csv 2020-11-02 16:58:13,022:33792 ThreadPoolExecutor-3_1 (task-1): done\n",
      "\n",
      "\n",
      "2020-11-02 16:58:13,023:33792 ThreadPoolExecutor-3_2 (task-2): done\n",
      "\n",
      "2020-11-02 16:58:13,025:33792 ThreadPoolExecutor-3_3 (task-3): done\n",
      "\n",
      "2020-11-02 16:58:13,026:33792 ThreadPoolExecutor-3_4 (task-4): done\n",
      "\n",
      "2020-11-02 16:58:13,027:33792 ThreadPoolExecutor-3_5 (task-5): done\n",
      "\n",
      "2020-11-02 16:58:13,028:33792 ThreadPoolExecutor-3_6 (task-6): done\n",
      "\n",
      "2020-11-02 16:58:13,033:33792 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 16:58:13,034:33792 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 16:58:13,035:33792 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 16:58:13,035:33792 ThreadPoolExecutor-4_0 (task-0): passed args :['tg6', 'tg6_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 49]\n",
      "\n",
      "2020-11-02 16:58:13,036:33792 ThreadPoolExecutor-4_0 (task-0): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv\n",
      "2020-11-02 16:58:13,037:33792 ThreadPoolExecutor-4_0 (task-0): done\n",
      "\n",
      "2020-11-02 16:58:13,037:33792 ThreadPoolExecutor-4_1 (task-1): passed args :['tg6', 'tg6_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 49]\n",
      "\n",
      "2020-11-02 16:58:13,037:33792 ThreadPoolExecutor-4_2 (task-2): passed args :['tg4', 'tg4_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-02 16:58:13,038:33792 ThreadPoolExecutor-4_3 (task-3): passed args :['tg4', 'tg4_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-02 16:58:13,038:33792 ThreadPoolExecutor-4_4 (task-4): passed args :['tg4', 'tg4_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 48]\n",
      "\n",
      "2020-11-02 16:58:13,039:33792 ThreadPoolExecutor-4_5 (task-5): passed args :['tg0', 'tg0_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 27]\n",
      "\n",
      "2020-11-02 16:58:13,040:33792 ThreadPoolExecutor-4_6 (task-6): passed args :['tg2', 'tg2_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 50]\n",
      "\n",
      "2020-11-02 16:58:13,041:33792 ThreadPoolExecutor-4_7 (task-7): passed args :['tg2', 'tg2_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 50]\n",
      "\n",
      "2020-11-02 16:58:13,042:33792 ThreadPoolExecutor-4_8 (task-8): passed args :['tg2', 'tg2_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 50]\n",
      "\n",
      "2020-11-02 16:58:13,042:33792 ThreadPoolExecutor-4_1 (task-1): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv\n",
      "2020-11-02 16:58:13,042:33792 ThreadPoolExecutor-4_9 (task-9): passed args :['tg1', 'tg1_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-02 16:58:13,042:33792 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 16:58:13,043:33792 ThreadPoolExecutor-4_0 (task-10): passed args :['tg1', 'tg1_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-02 16:58:13,043:33792 ThreadPoolExecutor-4_2 (task-2): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-01_ford.csv2020-11-02 16:58:13,044:33792 ThreadPoolExecutor-4_3 (task-3): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-02_ford.csv2020-11-02 16:58:13,045:33792 ThreadPoolExecutor-4_4 (task-4): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-03_ford.csv\n",
      "2020-11-02 16:58:13,046:33792 ThreadPoolExecutor-4_5 (task-5): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg0/tg0_2020-11-02_ford.csv2020-11-02 16:58:13,046:33792 ThreadPoolExecutor-4_6 (task-6): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv\n",
      "2020-11-02 16:58:13,047:33792 ThreadPoolExecutor-4_7 (task-7): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv\n",
      "2020-11-02 16:58:13,048:33792 ThreadPoolExecutor-4_8 (task-8): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-03_ford.csv\n",
      "2020-11-02 16:58:13,049:33792 ThreadPoolExecutor-4_1 (task-1): done\n",
      "\n",
      "2020-11-02 16:58:13,049:33792 ThreadPoolExecutor-4_9 (task-9): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg1/tg1_2020-11-02_ford.csv\n",
      "2020-11-02 16:58:13,051:33792 ThreadPoolExecutor-4_0 (task-10): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg1/tg1_2020-11-01_ford.csv2020-11-02 16:58:13,052:33792 ThreadPoolExecutor-4_2 (task-2): done\n",
      "\n",
      "\n",
      "2020-11-02 16:58:13,053:33792 ThreadPoolExecutor-4_3 (task-3): done\n",
      "\n",
      "2020-11-02 16:58:13,053:33792 ThreadPoolExecutor-4_4 (task-4): done\n",
      "\n",
      "2020-11-02 16:58:13,054:33792 ThreadPoolExecutor-4_5 (task-5): done\n",
      "\n",
      "2020-11-02 16:58:13,056:33792 ThreadPoolExecutor-4_6 (task-6): done\n",
      "\n",
      "2020-11-02 16:58:13,056:33792 ThreadPoolExecutor-4_7 (task-7): done\n",
      "\n",
      "2020-11-02 16:58:13,057:33792 ThreadPoolExecutor-4_8 (task-8): done\n",
      "\n",
      "2020-11-02 16:58:13,058:33792 ThreadPoolExecutor-4_1 (task-11): passed args :['tg7', 'tg7_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 48]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 16:58:13,058:33792 ThreadPoolExecutor-4_9 (task-9): done\n",
      "\n",
      "2020-11-02 16:58:13,060:33792 ThreadPoolExecutor-4_0 (task-10): done\n",
      "\n",
      "2020-11-02 16:58:13,060:33792 ThreadPoolExecutor-4_2 (task-12): passed args :['tg7', 'tg7_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-02 16:58:13,060:33792 ThreadPoolExecutor-4_3 (task-13): passed args :['tg5', 'tg5_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 48]\n",
      "\n",
      "2020-11-02 16:58:13,061:33792 ThreadPoolExecutor-4_4 (task-14): passed args :['tg5', 'tg5_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-02 16:58:13,062:33792 ThreadPoolExecutor-4_5 (task-15): passed args :['tg5', 'tg5_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-02 16:58:13,064:33792 ThreadPoolExecutor-4_1 (task-11): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg7/tg7_2020-11-01_ford.csv\n",
      "2020-11-02 16:58:13,069:33792 ThreadPoolExecutor-4_2 (task-12): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg7/tg7_2020-11-02_ford.csv\n",
      "2020-11-02 16:58:13,069:33792 ThreadPoolExecutor-4_3 (task-13): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-03_ford.csv2020-11-02 16:58:13,070:33792 ThreadPoolExecutor-4_4 (task-14): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-01_ford.csv2020-11-02 16:58:13,070:33792 ThreadPoolExecutor-4_5 (task-15): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-02_ford.csv\n",
      "2020-11-02 16:58:13,071:33792 ThreadPoolExecutor-4_1 (task-11): done\n",
      "\n",
      "2020-11-02 16:58:13,073:33792 ThreadPoolExecutor-4_2 (task-12): done\n",
      "\n",
      "2020-11-02 16:58:13,073:33792 ThreadPoolExecutor-4_3 (task-13): done\n",
      "\n",
      "2020-11-02 16:58:13,075:33792 ThreadPoolExecutor-4_4 (task-14): done\n",
      "\n",
      "2020-11-02 16:58:13,075:33792 ThreadPoolExecutor-4_5 (task-15): done\n",
      "\n",
      "2020-11-02 16:58:13,080:33792 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 16:58:13,086 INFO libs.xml_writer: \n",
      "<configroot version=\"12.1\">\n",
      "\t<set>\n",
      "\t\t<name>CS_TAXONOMY_LMT_SCHEMA_SET</name>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg6</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a6</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a61</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg6/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg4</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a4</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a4</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg4/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg0</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a0</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a0</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg0/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg2</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a21</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a21</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg2/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg7</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a7</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a7</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg7/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg1</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a1</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a1</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg1/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg5</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a5</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a5</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg5/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "    </set>\n",
      "    <set>\n",
      "\t\t<name>DROP_CS_TAXONOMY_LMT_SCHEMA_SET</name>\n",
      "        \n",
      "\t\t <val> tg6</val>\n",
      "        \n",
      "\t\t <val> tg3</val>\n",
      "        \n",
      "\t\t <val> tg2</val>\n",
      "        \n",
      "    </set>\n",
      "    \n",
      "</configroot>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 16:58:13,088 INFO libs.xml_writer: test.xml has been generated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Extract Info needed to expose configs and show in logs and reports'''\n",
    "\n",
    "invalid_files_set = src_delta['invalid_files_set']\n",
    "invalid_schema_files = src_delta['invalid_schema_files']\n",
    "\n",
    "tg_data = { k for k in tg_data_files_dict.keys()}\n",
    "tg_existing = { k for k in src_delta['existing_tg_files_dict'].keys()}\n",
    "tg_new ={ k for k in src_delta['new_tg_files_dict'].keys()}\n",
    "tg_all = tg_new.union(tg_existing)\n",
    "\n",
    "many_tg4schema_check_gen = (v for k, v in src_delta['schema_tg_dict'].items() if len(v) > 1)\n",
    "many_tg4target_check_gen = (v for k, v in src_delta['target_tg_dict'].items() if len(v) > 1)\n",
    "newTg4schema = {k for k, v in src_delta['new_tg_schema_dict'].items() if len(v) > 1}\n",
    "\n",
    "\n",
    "tg4schema = set()\n",
    "[tg4schema.update(i) for i in many_tg4schema_check_gen]\n",
    "tg4target = set()\n",
    "[tg4target.update(i) for i in many_tg4target_check_gen]\n",
    "\n",
    "\n",
    "invalid_tg_with_dup_schema = (tg4schema.union(newTg4schema)).difference(tg_existing)\n",
    "\n",
    "invalid_tg_with_dup_target = tg4target.difference(tg_existing)\n",
    "\n",
    "invalid_tg_all = invalid_tg_with_dup_schema.union(invalid_tg_with_dup_target)\n",
    "\n",
    "tg_delta = tg_new.difference(invalid_tg_all)\n",
    "\n",
    "tg_delta_create = tg_delta.difference(tg_data)\n",
    "\n",
    "tg_delta_drop_n_create = (tg_delta.intersection(tg_data)).difference(tg_existing)\n",
    "\n",
    "tg_dropped = tg_data.difference(tg_all)\n",
    "\n",
    "tg_dropped_all = tg_dropped.union(tg_delta_drop_n_create)\n",
    "tg_create_all = tg_delta_create.union(tg_delta_drop_n_create)\n",
    "\n",
    "''' File Sync'''\n",
    "files_to_be_dropped = [[f['Key']] for i in  tg_dropped_all \n",
    "                       for fn, f in tg_data_files_dict.get(i).items()]\n",
    "files_not_retained_existing_tg =[[tg_data_files_dict.get(tg).get(fn)['Key']]\n",
    "                                 for tg in tg_existing \n",
    "                                 for fn in set(tg_data_files_dict.get(tg).keys()).difference(set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "file_drop_args = []\n",
    "file_drop_args.extend(files_to_be_dropped )\n",
    "file_drop_args.extend(files_not_retained_existing_tg )\n",
    "\n",
    "\n",
    "files_to_be_created = [[i, f['FileName'], f['Key'], f['Size']] \n",
    "                       for i in  tg_create_all \n",
    "                       for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "files_to_be_copied = [[k, f_dict['FileName'], f_dict['Key'], f_dict['Size']] \n",
    "                      for k, v in src_delta['existing_tg_files_dict'].items() \n",
    "                      for f, f_dict in v.items()] #['Key']]\n",
    "file_copy_args = []\n",
    "file_copy_args.extend(files_to_be_created )\n",
    "file_copy_args.extend(files_to_be_copied )\n",
    "\n",
    "collected = NIO.decorated_run_io(task=s3_remove_at_data_loc_task, task_n_args_list=file_drop_args, \n",
    "                                 is_kernal_thread=False,)\n",
    "\n",
    "collected = NIO.decorated_run_io(task=s3_copy_into_data_loc_task, task_n_args_list=file_copy_args, \n",
    "                                 is_kernal_thread=False,)\n",
    "\n",
    "\n",
    "\n",
    "''' Expose details to generate configs'''\n",
    "tg_create_all_n_schema = {tg: schema \n",
    "                          for tg in tg_create_all \n",
    "                          for schema in src_delta['new_tg_schema_dict'].get(tg)}\n",
    "tg_retain_all_n_schema = {tg: tg_data_schema_dict.get(tg) \n",
    "                          for tg in tg_existing}\n",
    "tg_dropped_all_n_schema = {tg: tg_data_schema_dict.get(tg) \n",
    "                           for tg in tg_dropped_all }\n",
    "\n",
    "\n",
    "'''New and Drop_n_create(With new attributes like schema) Taxonomy Grps'''\n",
    "exposed_tg_all = [Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target'], lmt_data) \n",
    "                  for tg ,schema in tg_create_all_n_schema.items() \n",
    "                  for schema_dict in key_target_splitter(schema)]\n",
    "'''Retaining Taxonomy Grps with either NO CHANGES or Create and Drop some files in a retained group'''\n",
    "exposed_tg_all.extend([Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target'], lmt_data) \n",
    "                       for tg ,schema in tg_retain_all_n_schema.items() \n",
    "                       for schema_dict in key_target_splitter(schema)])\n",
    "\n",
    "\n",
    "'''Dropped and Drop_n_create(With old attributes like schema) Taxonomy Grps'''\n",
    "exposed_dropped_tg_all = [Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target'], lmt_data) \n",
    "                          for tg ,schema in tg_dropped_all_n_schema.items() \n",
    "                          for schema_dict in key_target_splitter(schema)]\n",
    "\n",
    "\n",
    "''' Generating output config xml'''\n",
    "xml_writer.generate_output_config(exposed_tg_all, exposed_dropped_tg_all, dn_version, config_input_loc, config_file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ''' Report '''\n",
    "# # invalid files Schema delails\n",
    "# # To Be\n",
    "\n",
    "# # TG level\n",
    "# tg_create_n_schema = [(i, src_delta['new_tg_schema_dict'].get(i)) for i in tg_delta_create]\n",
    "# tg_drop_create_n_schema = [(tg, tg_data_schema_dict.get(tg), schema_new) for tg in tg_delta_drop_n_create for schema_new in src_delta['new_tg_schema_dict'].get(tg)]\n",
    "# tg_drop_n_schema = [(i, tg_data_schema_dict.get(i)) for i in tg_dropped]\n",
    "# tg_retain_n_schema = [(i, tg_data_schema_dict.get(i)) for i in tg_existing]\n",
    "\n",
    "# # File Level\n",
    "# files_to_be_dropped = [ f['Key'] for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items()]\n",
    "# files_to_be_dropped_schema_change = [ f['Key'] for i in  tg_delta_drop_n_create for fn, f in tg_data_files_dict.get(i).items()]\n",
    "# files_to_be_created = {f['Key'] :f['Schema'] for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()}\n",
    "# files_to_be_created_schema_change = {f['Key'] :f['Schema'] for i in  tg_delta_drop_n_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Report '''\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "# '''  Exposed TG Report  '''\n",
    "\n",
    "# exposed_tg_report_data = [i.get_dict() for i in exposed_tg_all]\n",
    "# log_report(exposed_tg_report_data,  columns=['tg_name', 'key_cols','target_col', 'location'], sort_by='tg_name')\n",
    "\n",
    "\n",
    "# '''  Exposed Dropped TG Report '''\n",
    "\n",
    "# exposed_tg_dropped_report_data = [i.get_dict() for i in exposed_dropped_tg_all]\n",
    "# log_report(exposed_tg_dropped_report_data, columns=['tg_name', 'key_cols','target_col', 'location'], sort_by='tg_name')\n",
    "\n",
    "\n",
    "# '''  Invalid TG due to schema conflict/already used Report '''\n",
    "\n",
    "# invalid_tg_with_dup_schema_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "#                     for i in  invalid_tg_with_dup_schema for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "# log_report(invalid_tg_with_dup_schema_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "# '''  Invalid TG due to Target Column conflict/already used Report '''\n",
    "\n",
    "# invalid_tg_with_dup_target_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "#                     for i in  invalid_tg_with_dup_target for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "# log_report(invalid_tg_with_dup_target_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "# '''  Invalid files from retained grp due to schema or target mismatch with previously delivered files for same'''\n",
    "\n",
    "# partially_invalid_tg_set = tg_new.intersection(tg_existing)\n",
    "# partially_invalid_tg_report = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Grp_Schema': tg_data_schema_dict[i] } \n",
    "#                                for i in partially_invalid_tg_set  \n",
    "#                                for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "# log_report(partially_invalid_tg_report, columns=['Taxonomy_Grp','File_Name','Date', 'Schema','Grp_Schema']) \n",
    "\n",
    "\n",
    "# '''   Invalid file not match with required file pattern '''\n",
    "\n",
    "# invalid_files_report_data = [{'File_Name' : i} for i in invalid_files_set]\n",
    "# log_report(invalid_files_report_data,  columns=['File_Name'])\n",
    "\n",
    "\n",
    "# '''   Invalid file not match with required schema pattern '''\n",
    "\n",
    "# invalid_schema_files_rep_data = [{'File_Name' : i[0], 'Schema': i[1], 'Reason' : i[2]} for i in invalid_schema_files]\n",
    "# log_report(invalid_schema_files_rep_data,  columns=['File_Name', 'Schema','Reason'], header_align='left')\n",
    "\n",
    "\n",
    "# '''   Dropped TG Completely '''\n",
    "\n",
    "# tg_dropped_rep_gen =(extract_info(f['Key']) for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items())\n",
    "# tg_dropped_report_dict =  [{'Taxonomy_Grp':i['FileGrp'], 'File_Name':i['FileName'], 'Date':i['Date'], 'Schema': tg_data_schema_dict[i['FileGrp']] }\n",
    "#                            for i in tg_dropped_rep_gen] \n",
    "# log_report(list_of_row_dict=tg_dropped_report_dict,columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "# '''   Dropped TG to change schema '''\n",
    "\n",
    "# tg_drop_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), schema_new, extract_info(f['Key'])) \n",
    "#                              for tg in tg_delta_drop_n_create \n",
    "#                              for schema_new in src_delta['new_tg_schema_dict'].get(tg)\n",
    "#                              for fn, f in tg_data_files_dict.get(tg).items())\n",
    "# tg_drop_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2]} \n",
    "#                                      for i in tg_drop_schema_change_rep]\n",
    "# log_report(list_of_row_dict=tg_drop_schema_change_report_dict,columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema']) \n",
    "\n",
    "\n",
    "# '''   Created TG Absolute New '''\n",
    "\n",
    "# tg_newly_created_report_data = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "#                     for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "# log_report(tg_newly_created_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "# '''   Created TG to change schema(with new Schema) '''\n",
    "\n",
    "# tg_re_created_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), f['Schema'], extract_info(f['Key']), 'Re-delivered') \n",
    "#                                    if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "#                                    else (tg, 'NAN', f['Schema'], extract_info(f['Key']), 'New File')\n",
    "                                    \n",
    "#                                    for tg in tg_delta_drop_n_create \n",
    "#                                    #for schema_new in src_delta['new_tg_schema_dict'].get(tg) \n",
    "                                   \n",
    "#                                    for fn, f in src_delta['new_tg_files_dict'].get(tg).items() \n",
    "#                                    )\n",
    "\n",
    "# tg_recreated_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2], 'Desc': i[4]} \n",
    "#                                           for i in tg_re_created_schema_change_rep]\n",
    "\n",
    "# log_report(list_of_row_dict=tg_recreated_schema_change_report_dict,columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema', 'Desc'])\n",
    "\n",
    "\n",
    "# '''   Retained TG with retained files, new files and dropped files '''\n",
    "\n",
    "# tg_retained_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'Retained' }\n",
    "                           \n",
    "#                             if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "#                             else {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'New File' }\n",
    "#                             for tg in tg_existing \n",
    "#                             for fn, f in src_delta['existing_tg_files_dict'].get(tg).items()]\n",
    "\n",
    "# tg_retained_dropped_files = [extract_info(tg_data_files_dict.get(tg).get(fn)['Key']) \n",
    "#                              for tg in tg_existing \n",
    "#                              for fn in set(tg_data_files_dict.get(tg).keys()).difference(set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "                             \n",
    "# tg_retained_dropped_files_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': tg_data_schema_dict[f['FileGrp']], 'Desc' : 'Dropped' }\n",
    "#                                           for f in tg_retained_dropped_files]\n",
    "\n",
    "# tg_retained_report_data.extend(tg_retained_dropped_files_report_data)\n",
    "# log_report(tg_retained_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema', 'Desc'], sort_by = ['Taxonomy_Grp','Date']) \n",
    "\n",
    "# #''' End '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gen_Report(exposed_tg_all = None, \n",
    "               exposed_dropped_tg_all = None,\n",
    "               invalid_tg_with_dup_schema = None, \n",
    "               invalid_tg_with_dup_target = None,\n",
    "               src_delta  = None,\n",
    "               tg_new  = None, \n",
    "               tg_existing = None,\n",
    "               invalid_files_set = None,\n",
    "               invalid_schema_files = None,\n",
    "               tg_data_files_dict = None, \n",
    "               tg_data_schema_dict = None, \n",
    "               tg_delta_drop_n_create = None, \n",
    "               tg_delta_create = None):\n",
    "\n",
    "    ''' Report '''\n",
    "    \n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "    '''  Exposed TG Report  '''\n",
    "\n",
    "    exposed_tg_report_data = [i.get_dict() for i in exposed_tg_all]\n",
    "    log_report(exposed_tg_report_data,  columns=['tg_name', 'key_cols','target_col', 'location'], sort_by='tg_name')\n",
    "\n",
    "\n",
    "    '''  Exposed Dropped TG Report '''\n",
    "\n",
    "    exposed_tg_dropped_report_data = [i.get_dict() for i in exposed_dropped_tg_all]\n",
    "    log_report(exposed_tg_dropped_report_data, columns=['tg_name', 'key_cols','target_col', 'location'], sort_by='tg_name')\n",
    "\n",
    "\n",
    "    '''  Invalid TG due to schema conflict/already used Report '''\n",
    "\n",
    "    invalid_tg_with_dup_schema_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                        for i in  invalid_tg_with_dup_schema for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    log_report(invalid_tg_with_dup_schema_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], sort_by = ['Taxonomy_Grp','Date']) \n",
    "\n",
    "\n",
    "    '''  Invalid TG due to Target Column conflict/already used Report '''\n",
    "\n",
    "    invalid_tg_with_dup_target_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                        for i in  invalid_tg_with_dup_target for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    log_report(invalid_tg_with_dup_target_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], sort_by = ['Taxonomy_Grp','Date']) \n",
    "\n",
    "\n",
    "    ''' Invalid files from retained grp due to schema or target mismatch with previously delivered files for same'''\n",
    "\n",
    "    partially_invalid_tg_set = tg_new.intersection(tg_existing)\n",
    "    partially_invalid_tg_report = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Grp_Schema': tg_data_schema_dict[i] } \n",
    "                                   for i in partially_invalid_tg_set  \n",
    "                                   for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    log_report(partially_invalid_tg_report, columns=['Taxonomy_Grp','File_Name','Date', 'Schema','Grp_Schema'], sort_by = ['Taxonomy_Grp','Date']) \n",
    "\n",
    "\n",
    "    '''   Invalid file not match with required file pattern '''\n",
    "\n",
    "    invalid_files_report_data = [{'File_Name' : i} for i in invalid_files_set]\n",
    "    log_report(invalid_files_report_data,  columns=['File_Name'], sort_by = ['File_Name'])\n",
    "\n",
    "\n",
    "    '''   Invalid file not match with required schema pattern '''\n",
    "\n",
    "    invalid_schema_files_rep_data = [{'File_Name' : i[0], 'Schema': i[1], 'Reason' : i[2]} for i in invalid_schema_files]\n",
    "    log_report(invalid_schema_files_rep_data,  columns=['File_Name', 'Schema','Reason'], header_align='left', sort_by = ['File_Name'])\n",
    "\n",
    "\n",
    "    '''   Dropped TG Completely '''\n",
    "\n",
    "    tg_dropped_rep_gen =(extract_info(f['Key']) for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items())\n",
    "    tg_dropped_report_dict =  [{'Taxonomy_Grp':i['FileGrp'], 'File_Name':i['FileName'], 'Date':i['Date'], 'Schema': tg_data_schema_dict[i['FileGrp']] }\n",
    "                               for i in tg_dropped_rep_gen] \n",
    "    log_report(list_of_row_dict=tg_dropped_report_dict,columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], sort_by = ['Taxonomy_Grp','Date']) \n",
    "\n",
    "\n",
    "    '''   Dropped TG to change schema '''\n",
    "\n",
    "    tg_drop_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), schema_new, extract_info(f['Key'])) \n",
    "                                 for tg in tg_delta_drop_n_create \n",
    "                                 for schema_new in src_delta['new_tg_schema_dict'].get(tg)\n",
    "                                 for fn, f in tg_data_files_dict.get(tg).items())\n",
    "    tg_drop_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2]} \n",
    "                                         for i in tg_drop_schema_change_rep]\n",
    "    log_report(list_of_row_dict=tg_drop_schema_change_report_dict,columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema'], sort_by = ['Grp','Date']) \n",
    "\n",
    "\n",
    "    '''   Created TG Absolute New '''\n",
    "\n",
    "    tg_newly_created_report_data = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                        for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    log_report(tg_newly_created_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], sort_by = ['Taxonomy_Grp','Date']) \n",
    "\n",
    "\n",
    "    '''   Created TG to change schema(with new Schema) '''\n",
    "\n",
    "    tg_re_created_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), f['Schema'], extract_info(f['Key']), 'Re-delivered') \n",
    "                                       if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                       else (tg, 'NAN', f['Schema'], extract_info(f['Key']), 'New File')\n",
    "\n",
    "                                       for tg in tg_delta_drop_n_create \n",
    "                                       #for schema_new in src_delta['new_tg_schema_dict'].get(tg) \n",
    "\n",
    "                                       for fn, f in src_delta['new_tg_files_dict'].get(tg).items() \n",
    "                                       )\n",
    "\n",
    "    tg_recreated_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2], 'Desc': i[4]} \n",
    "                                              for i in tg_re_created_schema_change_rep]\n",
    "\n",
    "    log_report(list_of_row_dict=tg_recreated_schema_change_report_dict,columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema', 'Desc'], sort_by = ['Grp','Date'])\n",
    "\n",
    "\n",
    "    '''   Retained TG with retained files, new files and dropped files '''\n",
    "\n",
    "    tg_retained_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'Retained' }\n",
    "\n",
    "                                if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                else {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'New File' }\n",
    "                                for tg in tg_existing \n",
    "                                for fn, f in src_delta['existing_tg_files_dict'].get(tg).items()]\n",
    "\n",
    "    tg_retained_dropped_files = [extract_info(tg_data_files_dict.get(tg).get(fn)['Key']) \n",
    "                                 for tg in tg_existing \n",
    "                                 for fn in set(tg_data_files_dict.get(tg).keys()).difference(set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "\n",
    "    tg_retained_dropped_files_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': tg_data_schema_dict[f['FileGrp']], 'Desc' : 'Dropped' }\n",
    "                                              for f in tg_retained_dropped_files]\n",
    "\n",
    "    tg_retained_report_data.extend(tg_retained_dropped_files_report_data)\n",
    "    log_report(tg_retained_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema', 'Desc'], sort_by = ['Taxonomy_Grp','Date']) \n",
    "\n",
    "    #''' End '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **[{'tg_name': 'tg6', 'key_cols': ['key_a6'], 'target_col': 'target_a61', 'location': 's3://qubole-ford/taxonomy_cs/test1/data/tg6'}, {'tg_name': 'tg4', 'key_cols': ['key_a4'], 'target_col': 'target_a4', 'location': 's3://qubole-ford/taxonomy_cs/test1/data/tg4'}, {'tg_name': 'tg0', 'key_cols': ['key_a0'], 'target_col': 'target_a0', 'location': 's3://qubole-ford/taxonomy_cs/test1/data/tg0'}, {'tg_name': 'tg2', 'key_cols': ['key_a21'], 'target_col': 'target_a21', 'location': 's3://qubole-ford/taxonomy_cs/test1/data/tg2'}, {'tg_name': 'tg7', 'key_cols': ['key_a7'], 'target_col': 'target_a7', 'location': 's3://qubole-ford/taxonomy_cs/test1/data/tg7'}, {'tg_name': 'tg1', 'key_cols': ['key_a1'], 'target_col': 'target_a1', 'location': 's3://qubole-ford/taxonomy_cs/test1/data/tg1'}, {'tg_name': 'tg5', 'key_cols': ['key_a5'], 'target_col': 'target_a5', 'location': 's3://qubole-ford/taxonomy_cs/test1/data/tg5'}]\n",
      " **          \n",
      " **Empty DataFrame\n",
      "Columns: [tg_name, key_cols, target_col, location]\n",
      "Index: []\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n",
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **[{'tg_name': 'tg6', 'key_cols': ['key_a6'], 'target_col': 'target_a6', 'location': 's3://qubole-ford/taxonomy_cs/test1/data/tg6'}, {'tg_name': 'tg3', 'key_cols': ['key_a3'], 'target_col': 'target_a3', 'location': 's3://qubole-ford/taxonomy_cs/test1/data/tg3'}, {'tg_name': 'tg2', 'key_cols': ['key_a2'], 'target_col': 'target_a2', 'location': 's3://qubole-ford/taxonomy_cs/test1/data/tg2'}]\n",
      " **          \n",
      " **Empty DataFrame\n",
      "Columns: [tg_name, key_cols, target_col, location]\n",
      "Index: []\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n",
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **[{'Taxonomy_Grp': 'tg9', 'File_Name': 'tg9_2020-11-01_ford.csv', 'Date': '2020-11-01', 'Schema': 'key_a9,target_a9'}, {'Taxonomy_Grp': 'tg11', 'File_Name': 'tg11_2020-11-01_ford.csv', 'Date': '2020-11-01', 'Schema': 'key_a9,target_a9'}]\n",
      " **          \n",
      " **Empty DataFrame\n",
      "Columns: [Taxonomy_Grp, File_Name, Date, Schema]\n",
      "Index: []\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n",
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **[{'Taxonomy_Grp': 'tg10', 'File_Name': 'tg10_2020-11-01_ford.csv', 'Date': '2020-11-01', 'Schema': 'key_a10,target_a9'}, {'Taxonomy_Grp': 'tg11', 'File_Name': 'tg11_2020-11-01_ford.csv', 'Date': '2020-11-01', 'Schema': 'key_a9,target_a9'}, {'Taxonomy_Grp': 'tg9', 'File_Name': 'tg9_2020-11-01_ford.csv', 'Date': '2020-11-01', 'Schema': 'key_a9,target_a9'}, {'Taxonomy_Grp': 'tg8', 'File_Name': 'tg8_2020-11-01_ford.csv', 'Date': '2020-11-01', 'Schema': 'key_a8,target_a7'}]\n",
      " **          \n",
      " **Empty DataFrame\n",
      "Columns: [Taxonomy_Grp, File_Name, Date, Schema]\n",
      "Index: []\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n",
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **[{'Taxonomy_Grp': 'tg7', 'File_Name': 'tg7_2020-11-03_ford.csv', 'Date': '2020-11-03', 'Schema': 'key_a7,target_a71', 'Grp_Schema': 'key_a7,target_a7'}, {'Taxonomy_Grp': 'tg5', 'File_Name': 'tg5_2020-11-04_ford.csv', 'Date': '2020-11-04', 'Schema': 'key_a51,target_a51', 'Grp_Schema': 'key_a5,target_a5'}]\n",
      " **          \n",
      " **Empty DataFrame\n",
      "Columns: [Taxonomy_Grp, File_Name, Date, Schema, Grp_Schema]\n",
      "Index: []\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n",
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **[{'File_Name': 's3://qubole-ford/taxonomy_cs/test1/src/tg0_202-11-01_ford.csv'}]\n",
      " **          \n",
      " **Empty DataFrame\n",
      "Columns: [File_Name]\n",
      "Index: []\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n",
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **[{'File_Name': 's3://qubole-ford/taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv', 'Schema': 'key_a0, targe_a0', 'Reason': 'Exact one Target column is required! \\nAll given columns should Key or Target!'}]\n",
      " **          \n",
      " **Empty DataFrame\n",
      "Columns: [File_Name, Schema, Reason]\n",
      "Index: []\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n",
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp                File_Name        Date            Schema\n",
      " **      0          tg3  tg3_2020-11-01_ford.csv  2020-11-01  key_a3,target_a3\n",
      " **      1          tg3  tg3_2020-11-02_ford.csv  2020-11-02  key_a3,target_a3\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n",
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " **          \n",
      " **            Grp                File_Name        Date        Old_Schema          New_Schema\n",
      " **      0  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21\n",
      " **      2  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61\n",
      " **      3  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n",
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **[{'Taxonomy_Grp': 'tg4', 'File_Name': 'tg4_2020-11-01_ford.csv', 'Date': '2020-11-01', 'Schema': 'key_a4,target_a4'}, {'Taxonomy_Grp': 'tg4', 'File_Name': 'tg4_2020-11-02_ford.csv', 'Date': '2020-11-02', 'Schema': 'key_a4,target_a4'}, {'Taxonomy_Grp': 'tg4', 'File_Name': 'tg4_2020-11-03_ford.csv', 'Date': '2020-11-03', 'Schema': 'key_a4,target_a4'}, {'Taxonomy_Grp': 'tg0', 'File_Name': 'tg0_2020-11-02_ford.csv', 'Date': '2020-11-02', 'Schema': 'key_a0,target_a0'}]\n",
      " **          \n",
      " **Empty DataFrame\n",
      "Columns: [Taxonomy_Grp, File_Name, Date, Schema]\n",
      "Index: []\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n",
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " **          \n",
      " **            Grp                File_Name        Date        Old_Schema          New_Schema          Desc\n",
      " **      0  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      2  tg2  tg2_2020-11-03_ford.csv  2020-11-03               NAN  key_a21,target_a21      New File\n",
      " **      3  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **      4  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ****************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **[{'Taxonomy_Grp': 'tg7', 'File_Name': 'tg7_2020-11-01_ford.csv', 'Date': '2020-11-01', 'Schema': 'key_a7,target_a7', 'Desc': 'Retained'}, {'Taxonomy_Grp': 'tg7', 'File_Name': 'tg7_2020-11-02_ford.csv', 'Date': '2020-11-02', 'Schema': 'key_a7,target_a7', 'Desc': 'Retained'}, {'Taxonomy_Grp': 'tg1', 'File_Name': 'tg1_2020-11-02_ford.csv', 'Date': '2020-11-02', 'Schema': 'key_a1,target_a1', 'Desc': 'New File'}, {'Taxonomy_Grp': 'tg1', 'File_Name': 'tg1_2020-11-01_ford.csv', 'Date': '2020-11-01', 'Schema': 'key_a1,target_a1', 'Desc': 'Retained'}, {'Taxonomy_Grp': 'tg5', 'File_Name': 'tg5_2020-11-03_ford.csv', 'Date': '2020-11-03', 'Schema': 'key_a5,target_a5', 'Desc': 'New File'}, {'Taxonomy_Grp': 'tg5', 'File_Name': 'tg5_2020-11-01_ford.csv', 'Date': '2020-11-01', 'Schema': 'key_a5,target_a5', 'Desc': 'Retained'}, {'Taxonomy_Grp': 'tg5', 'File_Name': 'tg5_2020-11-02_ford.csv', 'Date': '2020-11-02', 'Schema': 'key_a5,target_a5', 'Desc': 'Retained'}, {'Taxonomy_Grp': 'tg5', 'File_Name': 'tg5_2020-11-05_ford.csv', 'Date': '2020-11-05', 'Schema': 'key_a5,target_a5', 'Desc': 'Dropped'}]\n",
      " **          \n",
      " **Empty DataFrame\n",
      "Columns: [Taxonomy_Grp, File_Name, Date, Schema, Desc]\n",
      "Index: []\n",
      " **          \n",
      " **          \n",
      " **          \n",
      " ****************************************************************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Gen_Report(exposed_tg_all = exposed_tg_all, \n",
    "               exposed_dropped_tg_all = exposed_dropped_tg_all,\n",
    "               invalid_tg_with_dup_schema = invalid_tg_with_dup_schema, \n",
    "               invalid_tg_with_dup_target = invalid_tg_with_dup_target,\n",
    "               src_delta  = src_delta,\n",
    "               tg_new  = tg_new, \n",
    "               tg_existing = tg_existing,\n",
    "               invalid_files_set = invalid_files_set,\n",
    "               invalid_schema_files = invalid_schema_files,\n",
    "               tg_data_files_dict = tg_data_files_dict, \n",
    "               tg_data_schema_dict = tg_data_schema_dict, \n",
    "               tg_delta_drop_n_create = tg_delta_drop_n_create, \n",
    "               tg_delta_create = tg_delta_create\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_util",
   "language": "python",
   "name": "aws_util"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
