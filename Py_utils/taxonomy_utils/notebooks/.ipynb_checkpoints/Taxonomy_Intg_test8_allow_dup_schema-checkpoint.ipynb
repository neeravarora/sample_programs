{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, inspect, re\n",
    "import functools\n",
    "sys.path.append(\"/home/vbhargava/feature_test0/msaction_backend/common/BU3.0_core/util/Py_utils/taxonomy_utils\")\n",
    "import time, logging\n",
    "import pandas as pd \n",
    "numeric_level = getattr(logging, 'INFO', None)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "logging.basicConfig(level=numeric_level,\n",
    "                        format='%(asctime)s %(levelname)s %(name)s: %(message)s',\n",
    "                        handlers=[stdout_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.s3_ops import S3_OPs\n",
    "from libs.s3_stream import S3Stream\n",
    "from libs.configs import Config\n",
    "from libs.nio_executor import NIO\n",
    "from libs import utils\n",
    "from libs import xml_writer \n",
    "from libs import decorator\n",
    "from collections import defaultdict\n",
    "#from model.models import Taxonomy_Grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Taxonomy_Grp:\n",
    "    \n",
    "    def __init__(self, tg_name, key_cols=[], target_cols=[], data_location=''):\n",
    "        self.tg_name = tg_name\n",
    "        self.key_cols = key_cols\n",
    "        self.target_cols = target_cols\n",
    "        self.location =os.path.join(data_location, tg_name)\n",
    "        \n",
    "    def get_dict(self):\n",
    "        if len(self.target_cols) == 0:\n",
    "            return {'tg_name': self.tg_name}\n",
    "        \n",
    "        return {'tg_name': self.tg_name, \n",
    "                'key_cols': self.key_cols, \n",
    "                'target_cols': self.target_cols, \n",
    "                'location': self.location}\n",
    "\n",
    "    def __str__(self):\n",
    "        if len(self.target_cols) == 0:\n",
    "            return 'tg_name: {}'.format(self.tg_name)\n",
    "        return 'tg_name: {}, key_cols: {}, target_cols: {}, location: {}'.format(self.tg_name, self.key_cols, self.target_cols,self.location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '/home/vbhargava/feature_test0/msaction_backend/customers/raj_ford_test/common/config/inputs/platform_config.xml'\n",
    "lmt_src = 's3://qubole-ford/taxonomy_cs/test1/src/'\n",
    "lmt_data = 's3://qubole-ford/taxonomy_cs/test1/data/'\n",
    "config_input_loc = '/home/vbhargava/feature_test0/temp/taxo_config_xmls/'\n",
    "config_file_name = 'test.xml'\n",
    "dn_version = '12.1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = Config.get_qubole_config(config)\n",
    "ACCESS_KEY=config_data['access_key']\n",
    "SECRET_KEY=config_data['secret_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TG_EXTRACT_REGEX = '^.*?/([a-zA-Z]+\\-?[0-9]*)/$' \n",
    "FILE_EXTRACT_REGEX = '^.*/([a-zA-Z0-9.\\-_]{0,255}.csv)$' #'^.*/([a-zA-Z0-9.\\-_]{0,255}.csv)$'\n",
    "TARGET_EXTRACT_REGEX ='^.*,?(target_[A-Za-z0-9_-]+).*$'\n",
    "TARGET_EXTRACT_2_REGEX ='(target_[A-Za-z0-9_-]+)'\n",
    "VALID_FILE_KEY_REGEX = '^(.*/([a-zA-Z]+\\-?[0-9]*)?/)?(([a-zA-Z]+\\-?[0-9]*?)_([0-9]{4}-[0-9]{2}-[0-9]{2}?)_([a-zA-Z0-9.\\-_]+?).csv?)$'\n",
    "KEY_REGEX = '^[Kk]ey_[A-Za-z0-9_]{2,30}$'\n",
    "TARGET_REGEX = '^[Tt]arget_[A-Za-z0-9_]{2,30}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_ops = S3_OPs(ACCESS_KEY, SECRET_KEY)\n",
    "\n",
    "def filename_by_key(key):\n",
    "    return get_val_by_regex(key, FILE_EXTRACT_REGEX, error_msg=\"Not vaild key for taxonomy data csv file\")\n",
    "\n",
    "def find_by_data_tg(key, regex):\n",
    "    return get_val_by_regex(key, regex, error_msg=\"Not vaild taxonomy data dir\")\n",
    "\n",
    "        \n",
    "def get_val_by_regex(key, regex, error_msg=\"can't be extract a val.\"):\n",
    "    matched = re.findall(regex, key)\n",
    "    if len(matched) > 0:\n",
    "        return matched[0]\n",
    "    else:\n",
    "        raise Exception(error_msg)\n",
    "        \n",
    "def get_data_n_schema(tg, data_files_loc):\n",
    "    data_file_lock_detail = s3_ops.get_bucket_name(data_files_loc)\n",
    "    files = s3_ops.list_complete(data_file_lock_detail['bucket'], data_file_lock_detail['key'])\n",
    "    res = {}\n",
    "    if len(files)>0:\n",
    "        s3_stream = S3Stream(ACCESS_KEY, SECRET_KEY)\n",
    "        schema = s3_stream.get_header(s3_ops.get_full_s3_path(data_file_lock_detail['bucket'],files[0]['Key']))\n",
    "        #res[tg]={'schema':schema, 'files': files}\n",
    "        res['schema'] = {tg:schema}\n",
    "        res['files'] = {tg:files}\n",
    "    return res\n",
    "\n",
    "def extract_schema(schema):\n",
    "    return schema.replace(\" \",\"\").lower()\n",
    "\n",
    "def validate_schema(schema):\n",
    "    if schema=='': \n",
    "        return {'IsValid' : False, 'schema': schema, 'message' : \"Schema shouldn't be empty\"}\n",
    "    tokens = schema.split(',')\n",
    "    if len(tokens) < 2:\n",
    "         return {'IsValid' : False, 'schema': schema, 'message' : \"Schema should have at least 2 columns\"}\n",
    "    \n",
    "    key_cnt = 0\n",
    "    target_cnt = 0\n",
    "    invalid_headers = []\n",
    "    columns = defaultdict(list)\n",
    "    res = {}\n",
    "    target_cols_set = set()\n",
    "    key_cols_set = set()\n",
    "    for t in tokens:\n",
    "        t = t.strip()\n",
    "        if re.match(TARGET_REGEX, t):\n",
    "            target_cnt = target_cnt + 1\n",
    "            target_cols_set.add(t)\n",
    "        elif re.match(KEY_REGEX, t):\n",
    "            key_cnt = key_cnt + 1\n",
    "            key_cols_set.add(t)\n",
    "        else:\n",
    "            invalid_headers.append(t)\n",
    "        columns[t.lower()].append(1)\n",
    "\n",
    "    error_msgs=[]\n",
    "    if target_cnt < 1 :\n",
    "        error_msgs.append(\"At least one Target column is required!\")\n",
    "    if key_cnt < 1 :\n",
    "        error_msgs.append(\"At least one Key column is required!\")\n",
    "    if len(invalid_headers) > 0 :\n",
    "        error_msgs.append(\"All given columns should Key or Target!\")\n",
    "    for k, v in columns.items():\n",
    "\n",
    "        if len(v) > 1:\n",
    "            print(\"--\")\n",
    "            error_msgs.append(\"Same name: {} should not represent more than one column in schema! cols names are case insensitive. \".format(k))\n",
    "\n",
    "    if len(error_msgs) > 0:\n",
    "        return {'IsValid' : False, 'schema': schema, 'errors' : \" \\n\".join(error_msgs)}\n",
    "    #print(str(key_cnt)+\":\"+str(target_cnt)+\":\"+str(invalid_headers)+\":\"+str(columns))\n",
    "    return {'IsValid' : True, 'Schema': schema.replace(\" \",\"\").lower(), \n",
    "            'TargetColsSet' : target_cols_set, 'KeyColsSet' : key_cols_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator.elapsed_time(func_name='extract_data_detail')\n",
    "def extract_data_detail(lmt_src, lmt_data, access_key, secret_key):\n",
    "#     Valid data Taxonomy Grps\n",
    "    \n",
    "    #\n",
    "    lmt_data_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "    lmt_data_loc_bucket = lmt_data_loc_detail['bucket']\n",
    "    lmt_data_loc_key = lmt_data_loc_detail['key']\n",
    "    valid_tg_list_res = s3_ops.list_subdirs(lmt_data_loc_detail['bucket'],lmt_data_loc_detail['key'],)\n",
    "    \n",
    "    valid_tgrp_loc_list = [ [find_by_data_tg(item['Prefix'], TG_EXTRACT_REGEX), \n",
    "                         '{}{}/'.format(lmt_data, find_by_data_tg(item['Prefix'], TG_EXTRACT_REGEX))] \n",
    "                       for item in valid_tg_list_res]\n",
    "    \n",
    "    collected = NIO.decorated_run_io(task=get_data_n_schema, task_n_args_list=valid_tgrp_loc_list, max_workers=25,)\n",
    "#     return collected\n",
    "    tg_data_schema_dict = {k:extract_schema(v)  for item in collected for k, v in item['result']['schema'].items()}\n",
    "    tg_data_files_dict = {k:{filename_by_key(u['Key']):u for u in v } for item in collected for k, v in item['result']['files'].items()}\n",
    "    #target_data_tg_dict = {re.findall(TARGET_EXTRACT_REGEX,V)[0]: K for K, V in tg_data_schema_dict.items()}\n",
    "    target_data_tg_dict = {target : K for K, V in tg_data_schema_dict.items() \n",
    "                           for target in re.findall(TARGET_EXTRACT_2_REGEX,V)}\n",
    "    \n",
    "    return tg_data_schema_dict, tg_data_files_dict,target_data_tg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_file(key:str='', regex = VALID_FILE_KEY_REGEX):\n",
    "    if re.match(regex, key) is None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_info(key:str='', regex = VALID_FILE_KEY_REGEX):\n",
    "    matched = re.findall(regex, key)\n",
    "    return {\n",
    "            'KeyDirPath' : matched[0][0],\n",
    "            'ParentDir' : matched[0][1],\n",
    "            'FileName' : matched[0][2],\n",
    "            'FileGrp' :  matched[0][3],\n",
    "            'Date' :  matched[0][4],\n",
    "            'ClientName' : matched[0][5]\n",
    "           }\n",
    "def extract_info_with_bucket(key:str='', bucket = ''):\n",
    "    res = extract_info(key)\n",
    "    res.update({'Bucket' : bucket})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_tg(collected, tg_files_dict_type='new_tg_files_dict'):\n",
    "    collect = defaultdict(dict)\n",
    "    tg_f_gen = (item['result'][tg_files_dict_type] for item in collected if len(item['result'][tg_files_dict_type]) > 0)\n",
    "    tg_f_gen2 = (collect[tg].update({filename: file_dict})  for item in tg_f_gen for tg, file_detail_dict in item.items() for filename, file_dict in file_detail_dict.items())\n",
    "    [ i for i in tg_f_gen2]\n",
    "    tg = dict(collect)\n",
    "    return tg\n",
    "\n",
    "def grouped_flag_dict(collected, flag_dict_type='schema_tg_dict'):\n",
    "    f_gen = (item['result'][flag_dict_type] for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    collect = defaultdict(set)\n",
    "    f_gen2 = (collect[K].add(V)  for item in f_gen for K, V in item.items())\n",
    "    [ i for i in f_gen2]\n",
    "    res = dict(collect)\n",
    "    return res\n",
    "\n",
    "def grouped_set_of_flags_dict(collected, flag_dict_type='schema_tg_dict'):\n",
    "    f_gen = (item['result'][flag_dict_type] for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    collect = defaultdict(set)\n",
    "    f_gen2 = (collect[K].update(V)  for item in f_gen for K, V in item.items())\n",
    "    [ i for i in f_gen2]\n",
    "    res = dict(collect)\n",
    "    return res\n",
    "\n",
    "def grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'):\n",
    "    res_set=set()\n",
    "    f_gen = (res_set.update(item['result'][flag_dict_type]) for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    [ i for i in f_gen]\n",
    "    return res_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def file_process_task(src_file_details):\n",
    "    \n",
    "    invalid_schema_files = set()\n",
    "\n",
    "    target_already_exist_files = set()\n",
    "    \n",
    "    ''' {'key_evt_advertiser_key,target_evt_advertiser_name': {'tg1', 'tg2', ...}}'''\n",
    "    schema_tg_dict = {}\n",
    "    \n",
    "    ''' {'target_evt_advertiser_name': {'tg1', 'tg2', ...}}'''\n",
    "    target_tg_dict = {}\n",
    "\n",
    "    ''' {'tg': {'key_evt_advertiser_key,target_evt_advertiser_name', '',...}}'''\n",
    "    new_tg_schema_dict = {}\n",
    "    ''' {'tg': {'AdvertiserReporting_2020-06-01_ford.csv': {file detailed obj dict} }  }'''\n",
    "    new_tg_files_dict = {}\n",
    "    ''' {'tg': {'AdvertiserReporting_2020-06-01_ford.csv': {file detailed obj dict} }  }'''\n",
    "    existing_tg_files_dict = {}\n",
    "\n",
    "\n",
    "    # tg_data_schema_dict = \n",
    "    # tg_data_files_dict = \n",
    "    # target_data_tg_dict = \n",
    "\n",
    "#     src_file_details = valid_file_arg[0]\n",
    "    src_file_loc = s3_ops.get_full_s3_path(src_file_details['Bucket'], src_file_details['Key'])\n",
    "\n",
    "    s3_stream = S3Stream(ACCESS_KEY, SECRET_KEY)\n",
    "    schema =  s3_stream.get_header(src_file_loc)\n",
    "    #schema = 'key_evt_advertiser_key, targe_evt_advertiser_name'\n",
    "    validate_res = validate_schema(schema)\n",
    "    if validate_res['IsValid']:\n",
    "        \n",
    "        \n",
    "        \n",
    "        tg = src_file_details['FileGrp']\n",
    "        file_name = src_file_details['FileName']\n",
    "        \n",
    "        if tg_data_schema_dict.get(tg) is None or tg_data_schema_dict.get(tg) != validate_res['Schema']:\n",
    "                \n",
    "#             data_tg_for_target = target_data_tg_dict.get(validate_res['TargetCol'])\n",
    "#             if  data_tg_for_target is not None:# and data_tg_for_target != tg:\n",
    "#                 target_already_exist_files.add((src_file_loc, data_tg_for_target))\n",
    "#             else:\n",
    "            new_tg_schema_dict[tg] = validate_res['Schema']\n",
    "            new_tg_files_dict[tg] = {file_name: src_file_details}\n",
    "        else:\n",
    "            existing_tg_files_dict[tg] = {file_name: src_file_details}\n",
    "        \n",
    "        src_file_details['Schema'] = validate_res['Schema']\n",
    "        schema_tg_dict[validate_res['Schema']] = tg\n",
    "        \n",
    "        #target_tg_dict[validate_res['TargetCol']] = tg\n",
    "        target_tg_dict = {target:tg for target in validate_res['TargetColsSet']}\n",
    "\n",
    "    else:\n",
    "        invalid_schema_files.add((src_file_loc, schema, validate_res['errors']))\n",
    "\n",
    "    return {'invalid_schema_files': invalid_schema_files,\n",
    "#             'target_already_exist_files':target_already_exist_files,\n",
    "            'schema_tg_dict': schema_tg_dict,\n",
    "            'target_tg_dict':target_tg_dict,\n",
    "            'new_tg_schema_dict': new_tg_schema_dict,\n",
    "            'new_tg_files_dict' : new_tg_files_dict,\n",
    "            'existing_tg_files_dict' : existing_tg_files_dict\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "def src_list_page_process_task(list_page):\n",
    "    \n",
    "    lmt_src_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    lmt_src_loc_bucket = lmt_src_loc_detail['bucket']\n",
    "    lmt_src_loc_key = lmt_src_loc_detail['key']\n",
    "    \n",
    "    invalid_files_set = { s3_ops.get_full_s3_path(lmt_src_loc_detail['bucket'], item['Key']) for item in list_page if  not is_valid_file(key=item['Key'])}\n",
    "    valid_file_set = [[utils.dict_append(extract_info_with_bucket(item['Key'], lmt_src_loc_detail['bucket']),item)] for item in list_page if  is_valid_file(key=item['Key']) ]\n",
    "    collected = NIO.decorated_run_io(task=file_process_task, task_n_args_list=valid_file_set, max_workers=25,)\n",
    "#     return collected\n",
    "    return {'invalid_files_set' : invalid_files_set,\n",
    "            'invalid_schema_files': grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'),\n",
    "#             'target_already_exist_files' : grouped_set_of_flags(collected, flag_dict_type='target_already_exist_files'),\n",
    "            'schema_tg_dict': grouped_flag_dict(collected, flag_dict_type='schema_tg_dict'),\n",
    "            'target_tg_dict': grouped_flag_dict(collected, flag_dict_type='target_tg_dict'),\n",
    "            'new_tg_schema_dict': grouped_flag_dict(collected, flag_dict_type='new_tg_schema_dict'),\n",
    "            'new_tg_files_dict' : grouped_tg(collected, 'new_tg_files_dict'),\n",
    "            'existing_tg_files_dict' : grouped_tg(collected, 'existing_tg_files_dict')\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_src_detail(maxKeysPerReq=3):\n",
    "    lmt_src_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    lmt_src_loc_bucket = lmt_src_loc_detail['bucket']\n",
    "    lmt_src_loc_key = lmt_src_loc_detail['key']\n",
    "    page_generator = s3_ops.list_gen(lmt_src_loc_bucket, lmt_src_loc_key, maxKeysPerReq=maxKeysPerReq, )\n",
    "    page_args_generator = ([page] for page in page_generator)\n",
    "    #list_page = [i for i in page_generator][0]\n",
    "    collected = NIO.decorated_run_with_args_generator(task=src_list_page_process_task, args_generator=page_args_generator, is_kernal_thread=True,)\n",
    "    \n",
    "    return {'invalid_files_set' : grouped_set_of_flags(collected, flag_dict_type='invalid_files_set'),\n",
    "            'invalid_schema_files': grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'),\n",
    "#             'target_already_exist_files' : grouped_set_of_flags(collected, flag_dict_type='target_already_exist_files'),\n",
    "            'schema_tg_dict': grouped_set_of_flags_dict(collected, flag_dict_type='schema_tg_dict'),\n",
    "            'target_tg_dict': grouped_set_of_flags_dict(collected, flag_dict_type='target_tg_dict'),\n",
    "            'new_tg_schema_dict': grouped_set_of_flags_dict(collected, flag_dict_type='new_tg_schema_dict'),\n",
    "            'new_tg_files_dict' : grouped_tg(collected, 'new_tg_files_dict'),\n",
    "            'existing_tg_files_dict' : grouped_tg(collected, 'existing_tg_files_dict')\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' E.g. '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def s3_copy_into_data_loc_task(tg, file_name, src_file, src_size, dry_run=True):\n",
    "#     data_file_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "    src_file_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    src_s3 = 's3://{}/{}'.format(src_file_loc_detail['bucket'], src_file)\n",
    "    dest_s3 = '{}{}/{}'.format(lmt_data,tg, file_name)\n",
    "    if dry_run:\n",
    "        print(\"[dry_run]: S3 copy from {} to {}\".format(src_s3, dest_s3))\n",
    "    else:\n",
    "        pass\n",
    "        #s3_ops.copy(src=src_s3, dest = dest_s3, src_size=src_size)\n",
    "    return 'Copied Successfully! by task'\n",
    "\n",
    "\n",
    "def s3_remove_at_data_loc_task(file,  dry_run=True):\n",
    "    data_file_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "#     src_file_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "#     src_s3 = 's3://{}/{}'.format(lmt_src, src_file)\n",
    "    \n",
    "    if dry_run:\n",
    "        file_loc = 's3://{}/{}'.format(data_file_loc_detail['bucket'], file)\n",
    "        print(\"[dry_run]: S3 delete from {} \".format(file_loc))\n",
    "    else:\n",
    "        pass\n",
    "        #s3_ops.delete_file(data_file_loc_detail['bucket'], file)\n",
    "    return 'Deleted Successfully! by task'\n",
    "\n",
    "''' E.g. '''\n",
    "# s3_copy_into_data_loc_task('tg5', 'tg5_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 48 )\n",
    "# s3_remove_at_data_loc_task('taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_data_detail **Start Time = 2020-11-20 11:23:23.765507\n",
      "\n",
      "2020-11-20 11:23:23,812:42409 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:23,813:42409 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:23,814:42409 ThreadPoolExecutor-0_0 (task-0): passed args :['tg1', 's3://qubole-ford/taxonomy_cs/test1/data/tg1/']\n",
      "\n",
      "2020-11-20 11:23:23,815:42409 ThreadPoolExecutor-0_1 (task-1): passed args :['tg15', 's3://qubole-ford/taxonomy_cs/test1/data/tg15/']\n",
      "\n",
      "2020-11-20 11:23:23,816:42409 ThreadPoolExecutor-0_2 (task-2): passed args :['tg2', 's3://qubole-ford/taxonomy_cs/test1/data/tg2/']\n",
      "\n",
      "2020-11-20 11:23:23,816:42409 ThreadPoolExecutor-0_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:23,816:42409 ThreadPoolExecutor-0_3 (task-3): passed args :['tg3', 's3://qubole-ford/taxonomy_cs/test1/data/tg3/']\n",
      "\n",
      "2020-11-20 11:23:23,817:42409 ThreadPoolExecutor-0_4 (task-4): passed args :['tg5', 's3://qubole-ford/taxonomy_cs/test1/data/tg5/']\n",
      "\n",
      "2020-11-20 11:23:23,818:42409 ThreadPoolExecutor-0_5 (task-5): passed args :['tg6', 's3://qubole-ford/taxonomy_cs/test1/data/tg6/']\n",
      "\n",
      "2020-11-20 11:23:23,818:42409 ThreadPoolExecutor-0_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:23,818:42409 ThreadPoolExecutor-0_6 (task-6): passed args :['tg7', 's3://qubole-ford/taxonomy_cs/test1/data/tg7/']\n",
      "\n",
      "2020-11-20 11:23:23,818:42409 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:23,819:42409 ThreadPoolExecutor-0_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:23,822:42409 ThreadPoolExecutor-0_3 (task-3): running\n",
      "\n",
      "2020-11-20 11:23:23,823:42409 ThreadPoolExecutor-0_4 (task-4): running\n",
      "\n",
      "2020-11-20 11:23:23,824:42409 ThreadPoolExecutor-0_5 (task-5): running\n",
      "\n",
      "2020-11-20 11:23:23,827:42409 ThreadPoolExecutor-0_6 (task-6): running\n",
      "\n",
      "2020-11-20 11:23:23,912:42409 ThreadPoolExecutor-0_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:23,936:42409 ThreadPoolExecutor-0_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:23,938:42409 ThreadPoolExecutor-0_5 (task-5): done\n",
      "\n",
      "2020-11-20 11:23:23,956:42409 ThreadPoolExecutor-0_6 (task-6): done\n",
      "\n",
      "2020-11-20 11:23:23,974:42409 ThreadPoolExecutor-0_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:23,976:42409 ThreadPoolExecutor-0_3 (task-3): done\n",
      "\n",
      "2020-11-20 11:23:24,004:42409 ThreadPoolExecutor-0_4 (task-4): done\n",
      "\n",
      "2020-11-20 11:23:24,005:42409 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "extract_data_detail **End Time = 2020-11-20 11:23:24.007197\n",
      "extract_data_detail **Elapsed Time = 0:00:00.241690\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'target_a2': 'tg2',\n",
       " 'target_a7': 'tg7',\n",
       " 'target_a3': 'tg3',\n",
       " 'target_a1': 'tg1',\n",
       " 'target_a5': 'tg5',\n",
       " 'target_a6': 'tg6',\n",
       " 'target_a15': 'tg15'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Get Existing State of System'''\n",
    "tg_data = extract_data_detail(lmt_src, lmt_data, ACCESS_KEY, SECRET_KEY)\n",
    "tg_data_schema_dict = tg_data[0]\n",
    "tg_data_files_dict = tg_data[1]\n",
    "target_data_tg_dict = tg_data[2]\n",
    "\n",
    "target_data_tg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tg2': {'tg2_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"df46527d151cbac56cf5d648af64f146\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg2_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"df46527d151cbac56cf5d648af64f146\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg2_2020-11-04_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg2/tg2_2020-11-04_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"df46527d151cbac56cf5d648af64f146\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg7': {'tg7_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg7/tg7_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 7, 23, 2, 38, tzinfo=tzlocal()),\n",
       "   'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg7_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg7/tg7_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 7, 23, 2, 38, tzinfo=tzlocal()),\n",
       "   'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg3': {'tg3_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg3/tg3_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"ca7090e60660707cd776ede900dc0a1a\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg3_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg3/tg3_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"ca7090e60660707cd776ede900dc0a1a\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg1': {'tg1_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg1/tg1_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 7, 23, 2, 38, tzinfo=tzlocal()),\n",
       "   'ETag': '\"e74387593f23233a61d30b719b79a381\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg5': {'tg5_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg5/tg5_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 7, 23, 2, 38, tzinfo=tzlocal()),\n",
       "   'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg5_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg5/tg5_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 7, 23, 2, 38, tzinfo=tzlocal()),\n",
       "   'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg5_2020-11-05_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg5/tg5_2020-11-05_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg6': {'tg6_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"243cd8420357c06c56877d990740b865\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg6_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"243cd8420357c06c56877d990740b865\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg15': {'tg15_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg15/tg15_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 4, 20, 7, 44, tzinfo=tzlocal()),\n",
       "   'ETag': '\"0497fdd31d3a2e3f3b4d12de70ef6640\"',\n",
       "   'Size': 52,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg15_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg15/tg15_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 4, 20, 7, 44, tzinfo=tzlocal()),\n",
       "   'ETag': '\"0497fdd31d3a2e3f3b4d12de70ef6640\"',\n",
       "   'Size': 52,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg15_2020-11-03_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg15/tg15_2020-11-03_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 4, 20, 7, 44, tzinfo=tzlocal()),\n",
       "   'ETag': '\"0497fdd31d3a2e3f3b4d12de70ef6640\"',\n",
       "   'Size': 52,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg15_2020-11-04_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg15/tg15_2020-11-04_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 4, 20, 7, 44, tzinfo=tzlocal()),\n",
       "   'ETag': '\"0497fdd31d3a2e3f3b4d12de70ef6640\"',\n",
       "   'Size': 52,\n",
       "   'StorageClass': 'STANDARD'}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg_data_files_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 11:23:24,028   process-id:42409 run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,028   process-id:42409 run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,096   process-id:42443   (task-0): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg0_202-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"535b60451f6d20c2826b045438a50fb9\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"7a5d08cbb4c718d16851d1f2b57ffc50\"', 'Size': 27, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 21, 21, 36, tzinfo=tzlocal()), 'ETag': '\"b19c288a2ef5e2ec6739cac3674391a6\"', 'Size': 28, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,099   process-id:42443   (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,103:42443 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,104:42443 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,105:42443 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg0_2020-11-02_ford.csv', 'FileGrp': 'tg0', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"7a5d08cbb4c718d16851d1f2b57ffc50\"', 'Size': 27, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,106:42443 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg0_2020-11-03_ford.csv', 'FileGrp': 'tg0', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 21, 21, 36, tzinfo=tzlocal()), 'ETag': '\"b19c288a2ef5e2ec6739cac3674391a6\"', 'Size': 28, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,106:42443 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,106:42443 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,107:42443 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,116   process-id:42444   (task-1): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg10_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"1aa284fe1180b5d4d776e26ff8a03358\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg14_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 23, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,120   process-id:42444   (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,123:42444 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,124:42444 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,125:42444 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg10_2020-11-01_ford.csv', 'FileGrp': 'tg10', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg10_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"1aa284fe1180b5d4d776e26ff8a03358\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,126:42444 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg11_2020-11-01_ford.csv', 'FileGrp': 'tg11', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,126:42444 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg14_2020-11-01_ford.csv', 'FileGrp': 'tg14', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg14_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 23, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,126:42444 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,127:42444 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,134   process-id:42445   (task-2): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg14_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 38, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg15_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 10, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg15_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 33, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,138   process-id:42445   (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,128:42444 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,141:42445 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,143:42445 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,144:42445 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg14_2020-11-02_ford.csv', 'FileGrp': 'tg14', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg14_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 38, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,145:42445 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg15_2020-11-01_ford.csv', 'FileGrp': 'tg15', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg15_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 10, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,145:42445 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg15_2020-11-02_ford.csv', 'FileGrp': 'tg15', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg15_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 33, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,145:42445 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,146:42445 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,129:42444 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,156   process-id:42446   (task-3): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg15_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 50, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg15_2020-11-05_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 7, 6, tzinfo=tzlocal()), 'ETag': '\"0521e9fc7fc22768af589b5badb6d3bd\"', 'Size': 54, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg16_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 8, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 11:23:24,159   process-id:42446   (task-3): running\n",
      "\n",
      "2020-11-20 11:23:24,147:42445 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,162:42446 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,163:42446 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,165:42446 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg15_2020-11-03_ford.csv', 'FileGrp': 'tg15', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg15_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 50, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,148:42445 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,169:42443 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,166:42446 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg15_2020-11-05_ford.csv', 'FileGrp': 'tg15', 'Date': '2020-11-05', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg15_2020-11-05_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 7, 6, tzinfo=tzlocal()), 'ETag': '\"0521e9fc7fc22768af589b5badb6d3bd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,166:42446 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg16_2020-11-01_ford.csv', 'FileGrp': 'tg16', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg16_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 8, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,166:42446 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,170:42446 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,175   process-id:42447   (task-4): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg16_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 19, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg16_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 30, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg17_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 22, 23, 51, tzinfo=tzlocal()), 'ETag': '\"13a19c147582aef1aa046ecef791bc75\"', 'Size': 59, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,177:42443 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,179:42443 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,178   process-id:42447   (task-4): running\n",
      "\n",
      "2020-11-20 11:23:24,181   process-id:42443   (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,181:42444 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,183:42447 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,184:42447 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,171:42446 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,185:42447 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg16_2020-11-02_ford.csv', 'FileGrp': 'tg16', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg16_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 19, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,186:42447 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg16_2020-11-03_ford.csv', 'FileGrp': 'tg16', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg16_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 30, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,187:42447 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg17_2020-11-01_ford.csv', 'FileGrp': 'tg17', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg17_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 22, 23, 51, tzinfo=tzlocal()), 'ETag': '\"13a19c147582aef1aa046ecef791bc75\"', 'Size': 59, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,187:42447 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,187:42447 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,173:42446 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,195   process-id:42448   (task-5): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,188:42447 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,198   process-id:42448   (task-5): running\n",
      "\n",
      "2020-11-20 11:23:24,201:42448 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,201:42444 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,202:42448 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,190:42447 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,204:42448 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg1_2020-11-01_ford.csv', 'FileGrp': 'tg1', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,205:42444 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,205:42448 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg1_2020-11-02_ford.csv', 'FileGrp': 'tg1', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,207:42444 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,205:42448 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-01_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,208   process-id:42444   (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,206:42448 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,206:42448 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,210:42445 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,213:42445 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 11:23:24,207:42448 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,215   process-id:42449   (task-6): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,218   process-id:42449   (task-6): running\n",
      "\n",
      "2020-11-20 11:23:24,221:42449 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,222:42449 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,224:42449 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-02_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,225:42449 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-03_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,225:42449 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-01_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,225:42449 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,225:42449 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,208:42448 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,227:42449 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,238   process-id:42450   (task-7): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,239:42445 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,241   process-id:42450   (task-7): running\n",
      "\n",
      "2020-11-20 11:23:24,242:42445 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,243:42446 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,244   process-id:42445   (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,244:42450 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,246:42450 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,247:42450 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-02_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,248:42450 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-03_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,249:42450 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-01_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,250:42446 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,249:42450 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,228:42449 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,256:42446 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,256   process-id:42451   (task-8): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-04_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"f87ad6041aa111ac6b6d0776be1c774f\"', 'Size': 50, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,249:42450 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,256:42447 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,258:42446 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,260   process-id:42451   (task-8): running\n",
      "\n",
      "2020-11-20 11:23:24,261   process-id:42446   (task-3): done\n",
      "\n",
      "2020-11-20 11:23:24,258:42447 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,264:42451 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,250:42450 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,265:42448 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,265:42447 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,266:42451 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,269:42447 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,269:42451 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-02_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,269:42448 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,270   process-id:42447   (task-4): done\n",
      "\n",
      "2020-11-20 11:23:24,271:42451 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-03_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 11:23:24,271:42451 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,251:42450 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,271:42451 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-04_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-04', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-04_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"f87ad6041aa111ac6b6d0776be1c774f\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,278   process-id:42452   (task-9): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,280   process-id:42452   (task-9): running\n",
      "\n",
      "2020-11-20 11:23:24,272:42451 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,283:42452 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,283:42448 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,284:42452 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,285:42448 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,285:42452 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg6_2020-11-01_ford.csv', 'FileGrp': 'tg6', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,286   process-id:42448   (task-5): done\n",
      "\n",
      "2020-11-20 11:23:24,287:42452 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg6_2020-11-02_ford.csv', 'FileGrp': 'tg6', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,287:42452 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,273:42451 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,294:42449 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,296   process-id:42443  (task-10): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg7_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"d54b283d90621ca6b19c85f4c96d4b8f\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"8d575874cb97b2d601ae8542aaf11431\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,297   process-id:42443  (task-10): running\n",
      "\n",
      "2020-11-20 11:23:24,299:42443 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,288:42452 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-01_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,300:42443 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,288:42452 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,301:42443 ThreadPoolExecutor-2_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-02_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,301:42449 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,302:42443 ThreadPoolExecutor-2_0 (task-0): running\n",
      "\n",
      "2020-11-20 11:23:24,281:42451 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,306:42450 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,302:42443 ThreadPoolExecutor-2_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg8_2020-11-01_ford.csv', 'FileGrp': 'tg8', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"8d575874cb97b2d601ae8542aaf11431\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,308:42449 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,310:42449 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,311   process-id:42449   (task-6): done\n",
      "\n",
      "2020-11-20 11:23:24,302:42443 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,288:42452 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,302:42443 ThreadPoolExecutor-2_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-03_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"d54b283d90621ca6b19c85f4c96d4b8f\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,300:42452 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,321   process-id:42409 run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,321:42450 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,322   process-id:42444  (task-11): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-20 11:23:24,311:42443 ThreadPoolExecutor-2_2 (task-2): running\n",
      "\n",
      "2020-11-20 11:23:24,323   process-id:42444  (task-11): running\n",
      "\n",
      "2020-11-20 11:23:24,325:42444 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,326:42444 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,327:42444 ThreadPoolExecutor-2_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg9_2020-11-01_ford.csv', 'FileGrp': 'tg9', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-20 11:23:24,328:42444 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,319:42443 ThreadPoolExecutor-2_1 (task-1): running\n",
      "\n",
      "2020-11-20 11:23:24,328:42444 ThreadPoolExecutor-2_0 (task-0): running\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 11:23:24,332:42450 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,334:42450 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,335   process-id:42450   (task-7): done\n",
      "\n",
      "2020-11-20 11:23:24,345:42451 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,348:42451 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,356:42451 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,358:42451 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,359   process-id:42451   (task-8): done\n",
      "\n",
      "2020-11-20 11:23:24,362:42443 ThreadPoolExecutor-2_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,368:42452 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,369:42443 ThreadPoolExecutor-2_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,372:42452 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,372:42443 ThreadPoolExecutor-2_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,373:42444 ThreadPoolExecutor-2_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,374:42443 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,374:42444 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,375   process-id:42443  (task-10): done\n",
      "\n",
      "2020-11-20 11:23:24,375:42452 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,376   process-id:42444  (task-11): done\n",
      "\n",
      "2020-11-20 11:23:24,377:42452 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,378   process-id:42452   (task-9): done\n",
      "\n",
      "2020-11-20 11:23:24,381   process-id:42409 run_blocking_tasks: exiting\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'invalid_files_set': {'s3://qubole-ford/taxonomy_cs/test1/src/tg0_202-11-01_ford.csv'},\n",
       " 'invalid_schema_files': {('s3://qubole-ford/taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv',\n",
       "   'key_a0, targe_a0',\n",
       "   'At least one Target column is required! \\nAll given columns should Key or Target!')},\n",
       " 'schema_tg_dict': {'key_a152,target_a152': {'tg15'},\n",
       "  'key_a16,target_a16,target_a162': {'tg16'},\n",
       "  'key_a151,target_a151': {'tg15'},\n",
       "  'key_a0,target_a0': {'tg0'},\n",
       "  'key_a6,target_a61': {'tg6'},\n",
       "  'key_a7,target_a7': {'tg14', 'tg7'},\n",
       "  'key_a21,target_a21': {'tg2'},\n",
       "  'key_a4,target_a4': {'tg4'},\n",
       "  'key_a5,target_a5': {'tg5'},\n",
       "  'key_a51,target_a51': {'tg5'},\n",
       "  'key_a16,target_a16,target_a9': {'tg17'},\n",
       "  'key_a1,target_a1': {'tg1'},\n",
       "  'key_a8,target_a7': {'tg8'},\n",
       "  'key_a7,target_a71': {'tg7'},\n",
       "  'key_a9,target_a9': {'tg11', 'tg9'},\n",
       "  'key_a10,target_a9': {'tg10'}},\n",
       " 'target_tg_dict': {'target_a152': {'tg15'},\n",
       "  'target_a162': {'tg16'},\n",
       "  'target_a16': {'tg16', 'tg17'},\n",
       "  'target_a151': {'tg15'},\n",
       "  'target_a0': {'tg0'},\n",
       "  'target_a61': {'tg6'},\n",
       "  'target_a7': {'tg14', 'tg7', 'tg8'},\n",
       "  'target_a21': {'tg2'},\n",
       "  'target_a4': {'tg4'},\n",
       "  'target_a5': {'tg5'},\n",
       "  'target_a51': {'tg5'},\n",
       "  'target_a9': {'tg10', 'tg11', 'tg17', 'tg9'},\n",
       "  'target_a1': {'tg1'},\n",
       "  'target_a71': {'tg7'}},\n",
       " 'new_tg_schema_dict': {'tg15': {'key_a151,target_a151',\n",
       "   'key_a152,target_a152'},\n",
       "  'tg16': {'key_a16,target_a16,target_a162'},\n",
       "  'tg0': {'key_a0,target_a0'},\n",
       "  'tg6': {'key_a6,target_a61'},\n",
       "  'tg14': {'key_a7,target_a7'},\n",
       "  'tg2': {'key_a21,target_a21'},\n",
       "  'tg4': {'key_a4,target_a4'},\n",
       "  'tg5': {'key_a51,target_a51'},\n",
       "  'tg17': {'key_a16,target_a16,target_a9'},\n",
       "  'tg8': {'key_a8,target_a7'},\n",
       "  'tg7': {'key_a7,target_a71'},\n",
       "  'tg9': {'key_a9,target_a9'},\n",
       "  'tg11': {'key_a9,target_a9'},\n",
       "  'tg10': {'key_a10,target_a9'}},\n",
       " 'new_tg_files_dict': {'tg15': {'tg15_2020-11-05_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg15_2020-11-05_ford.csv',\n",
       "    'FileGrp': 'tg15',\n",
       "    'Date': '2020-11-05',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg15_2020-11-05_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 7, 6, tzinfo=tzlocal()),\n",
       "    'ETag': '\"0521e9fc7fc22768af589b5badb6d3bd\"',\n",
       "    'Size': 54,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a152,target_a152'},\n",
       "   'tg15_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg15_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg15',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg15_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 50, tzinfo=tzlocal()),\n",
       "    'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"',\n",
       "    'Size': 54,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a151,target_a151'},\n",
       "   'tg15_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg15_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg15',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg15_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 10, tzinfo=tzlocal()),\n",
       "    'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"',\n",
       "    'Size': 54,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a151,target_a151'},\n",
       "   'tg15_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg15_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg15',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg15_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 33, tzinfo=tzlocal()),\n",
       "    'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"',\n",
       "    'Size': 54,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a151,target_a151'}},\n",
       "  'tg16': {'tg16_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg16_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg16',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg16_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 8, tzinfo=tzlocal()),\n",
       "    'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"',\n",
       "    'Size': 61,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a16,target_a16,target_a162'},\n",
       "   'tg16_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg16_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg16',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg16_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 30, tzinfo=tzlocal()),\n",
       "    'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"',\n",
       "    'Size': 61,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a16,target_a16,target_a162'},\n",
       "   'tg16_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg16_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg16',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg16_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 19, tzinfo=tzlocal()),\n",
       "    'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"',\n",
       "    'Size': 61,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a16,target_a16,target_a162'}},\n",
       "  'tg0': {'tg0_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg0_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg0',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"7a5d08cbb4c718d16851d1f2b57ffc50\"',\n",
       "    'Size': 27,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a0,target_a0'}},\n",
       "  'tg6': {'tg6_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg6_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg6',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"',\n",
       "    'Size': 49,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a6,target_a61'},\n",
       "   'tg6_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg6_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg6',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"',\n",
       "    'Size': 49,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a6,target_a61'}},\n",
       "  'tg14': {'tg14_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg14_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg14',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg14_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 38, tzinfo=tzlocal()),\n",
       "    'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a7,target_a7'},\n",
       "   'tg14_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg14_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg14',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg14_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 23, tzinfo=tzlocal()),\n",
       "    'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a7,target_a7'}},\n",
       "  'tg2': {'tg2_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg2_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg2',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"',\n",
       "    'Size': 50,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a21,target_a21'},\n",
       "   'tg2_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg2_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg2',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"',\n",
       "    'Size': 50,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a21,target_a21'},\n",
       "   'tg2_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg2_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg2',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"',\n",
       "    'Size': 50,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a21,target_a21'}},\n",
       "  'tg4': {'tg4_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg4_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg4',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a4,target_a4'},\n",
       "   'tg4_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg4_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg4',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a4,target_a4'},\n",
       "   'tg4_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg4_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg4',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a4,target_a4'}},\n",
       "  'tg5': {'tg5_2020-11-04_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg5_2020-11-04_ford.csv',\n",
       "    'FileGrp': 'tg5',\n",
       "    'Date': '2020-11-04',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg5_2020-11-04_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"f87ad6041aa111ac6b6d0776be1c774f\"',\n",
       "    'Size': 50,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a51,target_a51'}},\n",
       "  'tg17': {'tg17_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg17_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg17',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg17_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 9, 22, 23, 51, tzinfo=tzlocal()),\n",
       "    'ETag': '\"13a19c147582aef1aa046ecef791bc75\"',\n",
       "    'Size': 59,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a16,target_a16,target_a9'}},\n",
       "  'tg8': {'tg8_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg8_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg8',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"8d575874cb97b2d601ae8542aaf11431\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a8,target_a7'}},\n",
       "  'tg7': {'tg7_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg7_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg7',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg7_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"d54b283d90621ca6b19c85f4c96d4b8f\"',\n",
       "    'Size': 49,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a7,target_a71'}},\n",
       "  'tg9': {'tg9_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg9_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg9',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a9,target_a9'}},\n",
       "  'tg11': {'tg11_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg11_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg11',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a9,target_a9'}},\n",
       "  'tg10': {'tg10_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg10_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg10',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg10_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"1aa284fe1180b5d4d776e26ff8a03358\"',\n",
       "    'Size': 49,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a10,target_a9'}}},\n",
       " 'existing_tg_files_dict': {'tg7': {'tg7_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg7_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg7',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a7,target_a7'},\n",
       "   'tg7_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg7_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg7',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a7,target_a7'}},\n",
       "  'tg5': {'tg5_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg5_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg5',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a5,target_a5'},\n",
       "   'tg5_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg5_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg5',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a5,target_a5'},\n",
       "   'tg5_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg5_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg5',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a5,target_a5'}},\n",
       "  'tg1': {'tg1_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg1_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg1',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"e74387593f23233a61d30b719b79a381\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a1,target_a1'},\n",
       "   'tg1_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg1_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg1',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"e74387593f23233a61d30b719b79a381\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a1,target_a1'}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_delta = extract_src_detail()\n",
    "src_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_target_splitter(schema = ''):\n",
    "    tokens = schema.split(',')\n",
    "    key_cols = []\n",
    "    target_cols = []\n",
    "    for t in tokens:\n",
    "        t = t.strip()\n",
    "        if re.match(TARGET_REGEX, t):\n",
    "            target_cols.append(t)\n",
    "        elif re.match(KEY_REGEX, t):\n",
    "            key_cols.append(t)\n",
    "        else:\n",
    "            raise Exception(\"Not a valid schema\")\n",
    "    return [{'target_cols': target_cols, 'key_cols' : key_cols}]\n",
    "\n",
    "\n",
    "@decorator.box_logged\n",
    "def log_report(list_of_row_dict=[], columns:list=[], header_align = 'left', sort_by= None, ascending = True, report_title='', ):\n",
    "    pd.set_option(\"display.colheader_justify\", header_align)\n",
    "    df = pd.DataFrame(list_of_row_dict, columns=columns) \n",
    "    if sort_by is not None:\n",
    "        df = df.sort_values(by=sort_by, ascending=ascending)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=['index'])\n",
    "    #df = df.set_index(' **      ' + df.index.astype(str) )\n",
    "    df = df.rename(' **      {}'.format)\n",
    "    \n",
    "#   df.style.set_properties(**{'text-align': 'left'}).set_table_styles([ dict(selector='td', props=[('text-align', 'left')] ) ])\n",
    "#     df1 = df.reindex(columns=['Taxonomy_Grp','File','Date', 'Schema'])\n",
    "    #df[df.columns[new_order]]\n",
    "    #df = df.transpose()\n",
    "    if report_title != '': \n",
    "        report_titled(report_title)\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"\")\n",
    "    logging.info(str(df))\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"\")\n",
    "\n",
    "\n",
    "def left_justified(df):\n",
    "    formatters = {}\n",
    "    for li in list(df.columns):\n",
    "        max = df[li].str.len().max()\n",
    "        form = \"{{:<{}s}}\".format(max)\n",
    "        formatters[li] = functools.partial(str.format, form)\n",
    "    return df.to_string(formatters=formatters, index=False)   \n",
    "    \n",
    "@decorator.box_titled\n",
    "def report_titled(title:str=''):\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"    \"+title)\n",
    "    logging.info(\"\")\n",
    "    \n",
    "    \n",
    "# class Taxonomy_Grp:\n",
    "    \n",
    "#     def __init__(self, tg_name, key_cols=[], target_col='', data_location=''):\n",
    "#         self.tg_name = tg_name\n",
    "#         self.key_cols = key_cols\n",
    "#         self.target_col = target_col\n",
    "#         self.location =os.path.join(data_location, tg_name)\n",
    "        \n",
    "#     def get_dict(self):\n",
    "#         if self.target_col == '':\n",
    "#             return {'tg_name': self.tg_name}\n",
    "        \n",
    "#         return {'tg_name': self.tg_name, \n",
    "#                 'key_cols': self.key_cols, \n",
    "#                 'target_col': self.target_col, \n",
    "#                 'location': self.location}\n",
    "\n",
    "#     def __str__(self):\n",
    "#         if self.target_col == '':\n",
    "#             return 'tg_name: {}'.format(self.tg_name)\n",
    "#         return 'tg_name: {}, key_cols: {}, target_col: {}, location: {}'.format(self.tg_name, self.key_cols, self.target_col,self.location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 11:23:24,432:42409 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,433:42409 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,434:42409 ThreadPoolExecutor-1_0 (task-0): passed args :['taxonomy_cs/test1/data/tg3/tg3_2020-11-01_ford.csv']\n",
      "\n",
      "2020-11-20 11:23:24,434:42409 ThreadPoolExecutor-1_1 (task-1): passed args :['taxonomy_cs/test1/data/tg3/tg3_2020-11-02_ford.csv']\n",
      "\n",
      "2020-11-20 11:23:24,434:42409 ThreadPoolExecutor-1_2 (task-2): passed args :['taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv']\n",
      "\n",
      "2020-11-20 11:23:24,435:42409 ThreadPoolExecutor-1_3 (task-3): passed args :['taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv']\n",
      "\n",
      "2020-11-20 11:23:24,435:42409 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg3/tg3_2020-11-01_ford.csv \n",
      "2020-11-20 11:23:24,435:42409 ThreadPoolExecutor-1_4 (task-4): passed args :['taxonomy_cs/test1/data/tg2/tg2_2020-11-04_ford.csv']\n",
      "\n",
      "2020-11-20 11:23:24,436:42409 ThreadPoolExecutor-1_5 (task-5): passed args :['taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv']\n",
      "\n",
      "2020-11-20 11:23:24,437:42409 ThreadPoolExecutor-1_6 (task-6): passed args :['taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv']\n",
      "\n",
      "2020-11-20 11:23:24,437:42409 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg3/tg3_2020-11-02_ford.csv \n",
      "2020-11-20 11:23:24,437:42409 ThreadPoolExecutor-1_7 (task-7): passed args :['taxonomy_cs/test1/data/tg5/tg5_2020-11-05_ford.csv']\n",
      "\n",
      "2020-11-20 11:23:24,437:42409 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,438:42409 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv 2020-11-20 11:23:24,439:42409 ThreadPoolExecutor-1_3 (task-3): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv \n",
      "2020-11-20 11:23:24,439:42409 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,440:42409 ThreadPoolExecutor-1_4 (task-4): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-04_ford.csv 2020-11-20 11:23:24,441:42409 ThreadPoolExecutor-1_5 (task-5): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv 2020-11-20 11:23:24,442:42409 ThreadPoolExecutor-1_6 (task-6): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv \n",
      "2020-11-20 11:23:24,443:42409 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,444:42409 ThreadPoolExecutor-1_7 (task-7): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-05_ford.csv 2020-11-20 11:23:24,446:42409 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "\n",
      "2020-11-20 11:23:24,446:42409 ThreadPoolExecutor-1_3 (task-3): done\n",
      "\n",
      "2020-11-20 11:23:24,449:42409 ThreadPoolExecutor-1_4 (task-4): done\n",
      "\n",
      "2020-11-20 11:23:24,449:42409 ThreadPoolExecutor-1_5 (task-5): done\n",
      "\n",
      "2020-11-20 11:23:24,450:42409 ThreadPoolExecutor-1_6 (task-6): done\n",
      "\n",
      "2020-11-20 11:23:24,452:42409 ThreadPoolExecutor-1_7 (task-7): done\n",
      "\n",
      "2020-11-20 11:23:24,456:42409 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,458:42409 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-20 11:23:24,459:42409 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,460:42409 ThreadPoolExecutor-2_0 (task-0): passed args :['tg4', 'tg4_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-20 11:23:24,461:42409 ThreadPoolExecutor-2_0 (task-0): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-01_ford.csv\n",
      "2020-11-20 11:23:24,462:42409 ThreadPoolExecutor-2_1 (task-1): passed args :['tg4', 'tg4_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 48]\n",
      "\n",
      "2020-11-20 11:23:24,462:42409 ThreadPoolExecutor-2_2 (task-2): passed args :['tg4', 'tg4_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-20 11:23:24,463:42409 ThreadPoolExecutor-2_3 (task-3): passed args :['tg0', 'tg0_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 27]\n",
      "\n",
      "2020-11-20 11:23:24,463:42409 ThreadPoolExecutor-2_0 (task-0): done\n",
      "\n",
      "2020-11-20 11:23:24,463:42409 ThreadPoolExecutor-2_4 (task-4): passed args :['tg2', 'tg2_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 50]\n",
      "\n",
      "2020-11-20 11:23:24,463:42409 ThreadPoolExecutor-2_5 (task-5): passed args :['tg2', 'tg2_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 50]\n",
      "\n",
      "2020-11-20 11:23:24,464:42409 ThreadPoolExecutor-2_6 (task-6): passed args :['tg2', 'tg2_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 50]\n",
      "\n",
      "2020-11-20 11:23:24,464:42409 ThreadPoolExecutor-2_1 (task-1): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-03_ford.csv\n",
      "2020-11-20 11:23:24,464:42409 ThreadPoolExecutor-2_7 (task-7): passed args :['tg6', 'tg6_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 49]\n",
      "\n",
      "2020-11-20 11:23:24,465:42409 ThreadPoolExecutor-2_8 (task-8): passed args :['tg6', 'tg6_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 49]\n",
      "\n",
      "2020-11-20 11:23:24,466:42409 ThreadPoolExecutor-2_9 (task-9): passed args :['tg7', 'tg7_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-20 11:23:24,466:42409 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-20 11:23:24,467:42409 ThreadPoolExecutor-2_2 (task-2): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-02_ford.csv\n",
      "2020-11-20 11:23:24,467:42409 ThreadPoolExecutor-2_3 (task-3): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg0/tg0_2020-11-02_ford.csv\n",
      "2020-11-20 11:23:24,468:42409 ThreadPoolExecutor-2_0 (task-10): passed args :['tg7', 'tg7_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-20 11:23:24,469:42409 ThreadPoolExecutor-2_4 (task-4): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-03_ford.csv\n",
      "2020-11-20 11:23:24,470:42409 ThreadPoolExecutor-2_5 (task-5): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv\n",
      "2020-11-20 11:23:24,471:42409 ThreadPoolExecutor-2_6 (task-6): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv\n",
      "2020-11-20 11:23:24,471:42409 ThreadPoolExecutor-2_1 (task-1): done\n",
      "\n",
      "2020-11-20 11:23:24,472:42409 ThreadPoolExecutor-2_7 (task-7): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv\n",
      "2020-11-20 11:23:24,473:42409 ThreadPoolExecutor-2_8 (task-8): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv\n",
      "2020-11-20 11:23:24,474:42409 ThreadPoolExecutor-2_9 (task-9): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg7/tg7_2020-11-01_ford.csv\n",
      "2020-11-20 11:23:24,476:42409 ThreadPoolExecutor-2_2 (task-2): done\n",
      "\n",
      "2020-11-20 11:23:24,477:42409 ThreadPoolExecutor-2_3 (task-3): done\n",
      "\n",
      "2020-11-20 11:23:24,477:42409 ThreadPoolExecutor-2_0 (task-10): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg7/tg7_2020-11-02_ford.csv2020-11-20 11:23:24,478:42409 ThreadPoolExecutor-2_4 (task-4): done\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-11-20 11:23:24,480:42409 ThreadPoolExecutor-2_5 (task-5): done\n",
      "\n",
      "2020-11-20 11:23:24,480:42409 ThreadPoolExecutor-2_6 (task-6): done\n",
      "\n",
      "2020-11-20 11:23:24,481:42409 ThreadPoolExecutor-2_1 (task-11): passed args :['tg5', 'tg5_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 48]\n",
      "\n",
      "2020-11-20 11:23:24,482:42409 ThreadPoolExecutor-2_7 (task-7): done\n",
      "\n",
      "2020-11-20 11:23:24,482:42409 ThreadPoolExecutor-2_8 (task-8): done\n",
      "\n",
      "2020-11-20 11:23:24,483:42409 ThreadPoolExecutor-2_9 (task-9): done\n",
      "\n",
      "2020-11-20 11:23:24,484:42409 ThreadPoolExecutor-2_2 (task-12): passed args :['tg5', 'tg5_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-20 11:23:24,484:42409 ThreadPoolExecutor-2_3 (task-13): passed args :['tg5', 'tg5_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-20 11:23:24,486:42409 ThreadPoolExecutor-2_0 (task-10): done\n",
      "\n",
      "2020-11-20 11:23:24,486:42409 ThreadPoolExecutor-2_4 (task-14): passed args :['tg1', 'tg1_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-20 11:23:24,486:42409 ThreadPoolExecutor-2_5 (task-15): passed args :['tg1', 'tg1_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-20 11:23:24,490:42409 ThreadPoolExecutor-2_1 (task-11): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-03_ford.csv\n",
      "2020-11-20 11:23:24,493:42409 ThreadPoolExecutor-2_2 (task-12): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-02_ford.csv\n",
      "2020-11-20 11:23:24,493:42409 ThreadPoolExecutor-2_3 (task-13): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-01_ford.csv\n",
      "2020-11-20 11:23:24,495:42409 ThreadPoolExecutor-2_4 (task-14): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg1/tg1_2020-11-02_ford.csv2020-11-20 11:23:24,496:42409 ThreadPoolExecutor-2_5 (task-15): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg1/tg1_2020-11-01_ford.csv\n",
      "2020-11-20 11:23:24,496:42409 ThreadPoolExecutor-2_1 (task-11): done\n",
      "\n",
      "2020-11-20 11:23:24,497:42409 ThreadPoolExecutor-2_2 (task-12): done\n",
      "\n",
      "2020-11-20 11:23:24,497:42409 ThreadPoolExecutor-2_3 (task-13): done\n",
      "\n",
      "2020-11-20 11:23:24,498:42409 ThreadPoolExecutor-2_4 (task-14): done\n",
      "\n",
      "2020-11-20 11:23:24,499:42409 ThreadPoolExecutor-2_5 (task-15): done\n",
      "\n",
      "2020-11-20 11:23:24,505:42409 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-20 11:23:24,519 INFO libs.xml_writer: \n",
      "<configroot version=\"12.1\">\n",
      "\t<set>\n",
      "\t\t<name>CS_TAXONOMY_LMT_SCHEMA_SET</name>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg4</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a4</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\"></attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg4/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg0</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a0</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\"></attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg0/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg2</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a21</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\"></attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg2/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg6</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a6</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\"></attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg6/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg1</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a1</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\"></attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg1/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg7</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a7</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\"></attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg7/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg5</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a5</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\"></attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg5/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "    </set>\n",
      "   \n",
      "    \n",
      "    <set>\n",
      "\t\t<name>DROP_CS_TAXONOMY_LMT_SCHEMA_SET</name>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg3</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr>key_a3</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr></attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg2</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr>key_a2</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr></attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg6</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr>key_a6</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr></attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t</elements>\n",
      "        \n",
      "    </set>\n",
      "    \n",
      "</configroot>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 11:23:24,526 INFO libs.xml_writer: test.xml has been generated.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Extract Info needed to expose configs and show in logs and reports'''\n",
    "\n",
    "invalid_files_set = src_delta['invalid_files_set']\n",
    "invalid_schema_files = src_delta['invalid_schema_files']\n",
    "\n",
    "tg_data = { k for k in tg_data_files_dict.keys()}\n",
    "tg_existing = { k for k in src_delta['existing_tg_files_dict'].keys()}\n",
    "tg_new ={ k for k in src_delta['new_tg_files_dict'].keys()}\n",
    "tg_all = tg_new.union(tg_existing)\n",
    "\n",
    "many_tg4schema_check_gen = (v for k, v in src_delta['schema_tg_dict'].items() if len(v) > 1)\n",
    "many_tg4target_check_gen = (v for k, v in src_delta['target_tg_dict'].items() if len(v) > 1)\n",
    "newTg4schema = {k for k, v in src_delta['new_tg_schema_dict'].items() if len(v) > 1}\n",
    "\n",
    "\n",
    "tg4schema = set()\n",
    "[tg4schema.update(i) for i in many_tg4schema_check_gen]\n",
    "tg4target = set()\n",
    "[tg4target.update(i) for i in many_tg4target_check_gen]\n",
    "\n",
    "\n",
    "invalid_tg_with_dup_schema = (tg4schema.union(newTg4schema)).difference(tg_existing)\n",
    "\n",
    "invalid_tg_with_dup_target = tg4target.difference(tg_existing)\n",
    "\n",
    "invalid_tg_all = invalid_tg_with_dup_schema.union(invalid_tg_with_dup_target)\n",
    "\n",
    "tg_delta = tg_new.difference(invalid_tg_all)\n",
    "\n",
    "tg_delta_create = tg_delta.difference(tg_data)\n",
    "\n",
    "tg_delta_drop_n_create = (tg_delta.intersection(tg_data)).difference(tg_existing)\n",
    "\n",
    "tg_dropped = tg_data.difference(tg_all)\n",
    "\n",
    "tg_dropped_all = tg_dropped.union(tg_delta_drop_n_create)\n",
    "tg_create_all = tg_delta_create.union(tg_delta_drop_n_create)\n",
    "\n",
    "''' File Sync'''\n",
    "files_to_be_dropped = [[f['Key']] for i in  tg_dropped_all \n",
    "                       for fn, f in tg_data_files_dict.get(i).items()]\n",
    "files_not_retained_existing_tg =[[tg_data_files_dict.get(tg).get(fn)['Key']]\n",
    "                                 for tg in tg_existing \n",
    "                                 for fn in set(tg_data_files_dict.get(tg).keys()).difference(set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "file_drop_args = []\n",
    "file_drop_args.extend(files_to_be_dropped )\n",
    "file_drop_args.extend(files_not_retained_existing_tg )\n",
    "\n",
    "\n",
    "files_to_be_created = [[i, f['FileName'], f['Key'], f['Size']] \n",
    "                       for i in  tg_create_all \n",
    "                       for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "files_to_be_copied = [[k, f_dict['FileName'], f_dict['Key'], f_dict['Size']] \n",
    "                      for k, v in src_delta['existing_tg_files_dict'].items() \n",
    "                      for f, f_dict in v.items()] #['Key']]\n",
    "file_copy_args = []\n",
    "file_copy_args.extend(files_to_be_created )\n",
    "file_copy_args.extend(files_to_be_copied )\n",
    "\n",
    "collected = NIO.decorated_run_io(task=s3_remove_at_data_loc_task, task_n_args_list=file_drop_args, \n",
    "                                 is_kernal_thread=False,)\n",
    "\n",
    "collected = NIO.decorated_run_io(task=s3_copy_into_data_loc_task, task_n_args_list=file_copy_args, \n",
    "                                 is_kernal_thread=False,)\n",
    "\n",
    "\n",
    "\n",
    "''' Expose details to generate configs'''\n",
    "tg_create_all_n_schema = {tg: schema \n",
    "                          for tg in tg_create_all \n",
    "                          for schema in src_delta['new_tg_schema_dict'].get(tg)}\n",
    "tg_retain_all_n_schema = {tg: tg_data_schema_dict.get(tg) \n",
    "                          for tg in tg_existing}\n",
    "tg_dropped_all_n_schema = {tg: tg_data_schema_dict.get(tg) \n",
    "                           for tg in tg_dropped_all }\n",
    "\n",
    "\n",
    "'''New and Drop_n_create(With new attributes like schema) Taxonomy Grps'''\n",
    "exposed_tg_all = [Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target_cols'], lmt_data) \n",
    "                  for tg ,schema in tg_create_all_n_schema.items() \n",
    "                  for schema_dict in key_target_splitter(schema)]\n",
    "'''Retaining Taxonomy Grps with either NO CHANGES or Create and Drop some files in a retained group'''\n",
    "exposed_tg_all.extend([Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target_cols'], lmt_data) \n",
    "                       for tg ,schema in tg_retain_all_n_schema.items() \n",
    "                       for schema_dict in key_target_splitter(schema)])\n",
    "\n",
    "\n",
    "'''Dropped and Drop_n_create(With old attributes like schema) Taxonomy Grps'''\n",
    "exposed_dropped_tg_all = [Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target_cols'], lmt_data) \n",
    "                          for tg ,schema in tg_dropped_all_n_schema.items() \n",
    "                          for schema_dict in key_target_splitter(schema)]\n",
    "\n",
    "\n",
    "''' Generating output config xml'''\n",
    "xml_writer.generate_output_config(exposed_tg_all, exposed_dropped_tg_all, dn_version, config_input_loc, config_file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ''' Report '''\n",
    "# # invalid files Schema delails\n",
    "# # To Be\n",
    "\n",
    "# # TG level\n",
    "# tg_create_n_schema = [(i, src_delta['new_tg_schema_dict'].get(i)) for i in tg_delta_create]\n",
    "# tg_drop_create_n_schema = [(tg, tg_data_schema_dict.get(tg), schema_new) for tg in tg_delta_drop_n_create for schema_new in src_delta['new_tg_schema_dict'].get(tg)]\n",
    "# tg_drop_n_schema = [(i, tg_data_schema_dict.get(i)) for i in tg_dropped]\n",
    "# tg_retain_n_schema = [(i, tg_data_schema_dict.get(i)) for i in tg_existing]\n",
    "\n",
    "# # File Level\n",
    "# files_to_be_dropped = [ f['Key'] for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items()]\n",
    "# files_to_be_dropped_schema_change = [ f['Key'] for i in  tg_delta_drop_n_create for fn, f in tg_data_files_dict.get(i).items()]\n",
    "# files_to_be_created = {f['Key'] :f['Schema'] for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()}\n",
    "# files_to_be_created_schema_change = {f['Key'] :f['Schema'] for i in  tg_delta_drop_n_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'tg15'}, 'target_a152'),\n",
       " ({'tg16'}, 'target_a162'),\n",
       " ({'tg16', 'tg17'}, 'target_a16'),\n",
       " ({'tg15'}, 'target_a151'),\n",
       " ({'tg0'}, 'target_a0'),\n",
       " ({'tg6'}, 'target_a61'),\n",
       " ({'tg14', 'tg7', 'tg8'}, 'target_a7'),\n",
       " ({'tg2'}, 'target_a21'),\n",
       " ({'tg4'}, 'target_a4'),\n",
       " ({'tg5'}, 'target_a5'),\n",
       " ({'tg5'}, 'target_a51'),\n",
       " ({'tg10', 'tg11', 'tg17', 'tg9'}, 'target_a9'),\n",
       " ({'tg1'}, 'target_a1'),\n",
       " ({'tg7'}, 'target_a71')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "many_tg4target_check_gen = (v for k, v in src_delta['target_tg_dict'].items() if len(v) > 1)\n",
    "tg4target = set()\n",
    "[tg4target.update(i) for i in many_tg4target_check_gen]\n",
    "tg4target\n",
    "[(v,k) for k, v in src_delta['target_tg_dict'].items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           tg_name key_cols   target_cols   location                                    \n",
      " **      0  tg0      [key_a0]   [target_a0]  s3://qubole-ford/taxonomy_cs/test1/data/tg0\n",
      " **      1  tg1      [key_a1]   [target_a1]  s3://qubole-ford/taxonomy_cs/test1/data/tg1\n",
      " **      2  tg2     [key_a21]  [target_a21]  s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      3  tg4      [key_a4]   [target_a4]  s3://qubole-ford/taxonomy_cs/test1/data/tg4\n",
      " **      4  tg5      [key_a5]   [target_a5]  s3://qubole-ford/taxonomy_cs/test1/data/tg5\n",
      " **      5  tg6      [key_a6]  [target_a61]  s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **      6  tg7      [key_a7]   [target_a7]  s3://qubole-ford/taxonomy_cs/test1/data/tg7\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           tg_name key_cols  target_cols  location                                    \n",
      " **      0  tg2     [key_a2]  [target_a2]  s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      1  tg3     [key_a3]  [target_a3]  s3://qubole-ford/taxonomy_cs/test1/data/tg3\n",
      " **      2  tg6     [key_a6]  [target_a6]  s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                 Date        Schema               \n",
      " **      0  tg15         tg15_2020-11-05_ford.csv  2020-11-05  key_a152,target_a152\n",
      " **      1  tg15         tg15_2020-11-03_ford.csv  2020-11-03  key_a151,target_a151\n",
      " **      2  tg15         tg15_2020-11-01_ford.csv  2020-11-01  key_a151,target_a151\n",
      " **      3  tg15         tg15_2020-11-02_ford.csv  2020-11-02  key_a151,target_a151\n",
      " **      4  tg11         tg11_2020-11-01_ford.csv  2020-11-01      key_a9,target_a9\n",
      " **      5   tg9          tg9_2020-11-01_ford.csv  2020-11-01      key_a9,target_a9\n",
      " **      6  tg14         tg14_2020-11-02_ford.csv  2020-11-02      key_a7,target_a7\n",
      " **      7  tg14         tg14_2020-11-01_ford.csv  2020-11-01      key_a7,target_a7\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                 Date        Schema                         \n",
      " **      0  tg16         tg16_2020-11-01_ford.csv  2020-11-01  key_a16,target_a16,target_a162\n",
      " **      1  tg16         tg16_2020-11-03_ford.csv  2020-11-03  key_a16,target_a16,target_a162\n",
      " **      2  tg16         tg16_2020-11-02_ford.csv  2020-11-02  key_a16,target_a16,target_a162\n",
      " **      3  tg10         tg10_2020-11-01_ford.csv  2020-11-01               key_a10,target_a9\n",
      " **      4   tg9          tg9_2020-11-01_ford.csv  2020-11-01                key_a9,target_a9\n",
      " **      5  tg17         tg17_2020-11-01_ford.csv  2020-11-01    key_a16,target_a16,target_a9\n",
      " **      6  tg14         tg14_2020-11-02_ford.csv  2020-11-02                key_a7,target_a7\n",
      " **      7  tg14         tg14_2020-11-01_ford.csv  2020-11-01                key_a7,target_a7\n",
      " **      8   tg8          tg8_2020-11-01_ford.csv  2020-11-01                key_a8,target_a7\n",
      " **      9  tg11         tg11_2020-11-01_ford.csv  2020-11-01                key_a9,target_a9\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema              Grp_Schema       \n",
      " **      0  tg7          tg7_2020-11-03_ford.csv  2020-11-03   key_a7,target_a71  key_a7,target_a7\n",
      " **      1  tg5          tg5_2020-11-04_ford.csv  2020-11-04  key_a51,target_a51  key_a5,target_a5\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name                                                     \n",
      " **      0  s3://qubole-ford/taxonomy_cs/test1/src/tg0_202-11-01_ford.csv\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name                                                       Schema            Reason                                                                            \n",
      " **      0  s3://qubole-ford/taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv  key_a0, targe_a0  At least one Target column is required! \\nAll given columns should Key or Target!\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema           \n",
      " **      0  tg3          tg3_2020-11-01_ford.csv  2020-11-01  key_a3,target_a3\n",
      " **      1  tg3          tg3_2020-11-02_ford.csv  2020-11-02  key_a3,target_a3\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema         \n",
      " **      0  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61\n",
      " **      1  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61\n",
      " **      2  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21\n",
      " **      3  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21\n",
      " **      4  tg2  tg2_2020-11-04_ford.csv  2020-11-04  key_a2,target_a2  key_a21,target_a21\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema           \n",
      " **      0  tg4          tg4_2020-11-01_ford.csv  2020-11-01  key_a4,target_a4\n",
      " **      1  tg4          tg4_2020-11-03_ford.csv  2020-11-03  key_a4,target_a4\n",
      " **      2  tg4          tg4_2020-11-02_ford.csv  2020-11-02  key_a4,target_a4\n",
      " **      3  tg0          tg0_2020-11-02_ford.csv  2020-11-02  key_a0,target_a0\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema          Desc         \n",
      " **      0  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **      1  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **      2  tg2  tg2_2020-11-03_ford.csv  2020-11-03               NAN  key_a21,target_a21      New File\n",
      " **      3  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      4  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21  Re-delivered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema            Desc     \n",
      " **      0  tg1          tg1_2020-11-01_ford.csv  2020-11-01  key_a1,target_a1  Retained\n",
      " **      1  tg1          tg1_2020-11-02_ford.csv  2020-11-02  key_a1,target_a1  New File\n",
      " **      2  tg5          tg5_2020-11-01_ford.csv  2020-11-01  key_a5,target_a5  Retained\n",
      " **      3  tg5          tg5_2020-11-02_ford.csv  2020-11-02  key_a5,target_a5  Retained\n",
      " **      4  tg5          tg5_2020-11-03_ford.csv  2020-11-03  key_a5,target_a5  New File\n",
      " **      5  tg5          tg5_2020-11-05_ford.csv  2020-11-05  key_a5,target_a5   Dropped\n",
      " **      6  tg7          tg7_2020-11-01_ford.csv  2020-11-01  key_a7,target_a7  Retained\n",
      " **      7  tg7          tg7_2020-11-02_ford.csv  2020-11-02  key_a7,target_a7  Retained\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ''' Report '''\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "\n",
    "'''  Exposed TG Report  '''\n",
    "\n",
    "exposed_tg_report_data = [i.get_dict() for i in exposed_tg_all]\n",
    "log_report(exposed_tg_report_data,  columns=['tg_name', 'key_cols','target_cols', 'location'], sort_by='tg_name')\n",
    "\n",
    "\n",
    "'''  Exposed Dropped TG Report '''\n",
    "\n",
    "exposed_tg_dropped_report_data = [i.get_dict() for i in exposed_dropped_tg_all]\n",
    "log_report(exposed_tg_dropped_report_data, columns=['tg_name', 'key_cols','target_cols', 'location'], sort_by='tg_name')\n",
    "\n",
    "\n",
    "'''  Invalid TG due to schema conflict/already used Report '''\n",
    "\n",
    "invalid_tg_with_dup_schema_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                    for i in  invalid_tg_with_dup_schema for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(invalid_tg_with_dup_schema_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''  Invalid TG due to Target Column conflict/already used Report '''\n",
    "\n",
    "invalid_tg_with_dup_target_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                    for i in  invalid_tg_with_dup_target for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(invalid_tg_with_dup_target_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''  Invalid files from retained grp due to schema or target mismatch with previously delivered files for same'''\n",
    "\n",
    "partially_invalid_tg_set = tg_new.intersection(tg_existing)\n",
    "partially_invalid_tg_report = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Grp_Schema': tg_data_schema_dict[i] } \n",
    "                               for i in partially_invalid_tg_set  \n",
    "                               for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(partially_invalid_tg_report, columns=['Taxonomy_Grp','File_Name','Date', 'Schema','Grp_Schema']) \n",
    "\n",
    "\n",
    "'''   Invalid file not match with required file pattern '''\n",
    "\n",
    "invalid_files_report_data = [{'File_Name' : i} for i in invalid_files_set]\n",
    "log_report(invalid_files_report_data,  columns=['File_Name'])\n",
    "\n",
    "\n",
    "'''   Invalid file not match with required schema pattern '''\n",
    "\n",
    "invalid_schema_files_rep_data = [{'File_Name' : i[0], 'Schema': i[1], 'Reason' : i[2]} for i in invalid_schema_files]\n",
    "log_report(invalid_schema_files_rep_data,  columns=['File_Name', 'Schema','Reason'], header_align='left')\n",
    "\n",
    "\n",
    "'''   Dropped TG Completely '''\n",
    "\n",
    "tg_dropped_rep_gen =(extract_info(f['Key']) for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items())\n",
    "tg_dropped_report_dict =  [{'Taxonomy_Grp':i['FileGrp'], 'File_Name':i['FileName'], 'Date':i['Date'], 'Schema': tg_data_schema_dict[i['FileGrp']] }\n",
    "                           for i in tg_dropped_rep_gen] \n",
    "log_report(list_of_row_dict=tg_dropped_report_dict,columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''   Dropped TG to change schema '''\n",
    "\n",
    "tg_drop_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), schema_new, extract_info(f['Key'])) \n",
    "                             for tg in tg_delta_drop_n_create \n",
    "                             for schema_new in src_delta['new_tg_schema_dict'].get(tg)\n",
    "                             for fn, f in tg_data_files_dict.get(tg).items())\n",
    "tg_drop_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2]} \n",
    "                                     for i in tg_drop_schema_change_rep]\n",
    "log_report(list_of_row_dict=tg_drop_schema_change_report_dict,columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema']) \n",
    "\n",
    "\n",
    "'''   Created TG Absolute New '''\n",
    "\n",
    "tg_newly_created_report_data = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                    for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(tg_newly_created_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''   Created TG to change schema(with new Schema) '''\n",
    "\n",
    "tg_re_created_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), f['Schema'], extract_info(f['Key']), 'Re-delivered') \n",
    "                                   if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                   else (tg, 'NAN', f['Schema'], extract_info(f['Key']), 'New File')\n",
    "                                    \n",
    "                                   for tg in tg_delta_drop_n_create \n",
    "                                   #for schema_new in src_delta['new_tg_schema_dict'].get(tg) \n",
    "                                   \n",
    "                                   for fn, f in src_delta['new_tg_files_dict'].get(tg).items() \n",
    "                                   )\n",
    "\n",
    "tg_recreated_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2], 'Desc': i[4]} \n",
    "                                          for i in tg_re_created_schema_change_rep]\n",
    "\n",
    "log_report(list_of_row_dict=tg_recreated_schema_change_report_dict,columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema', 'Desc'])\n",
    "\n",
    "\n",
    "'''   Retained TG with retained files, new files and dropped files '''\n",
    "\n",
    "tg_retained_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'Retained' }\n",
    "                           \n",
    "                            if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                            else {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'New File' }\n",
    "                            for tg in tg_existing \n",
    "                            for fn, f in src_delta['existing_tg_files_dict'].get(tg).items()]\n",
    "\n",
    "tg_retained_dropped_files = [extract_info(tg_data_files_dict.get(tg).get(fn)['Key']) \n",
    "                             for tg in tg_existing \n",
    "                             for fn in set(tg_data_files_dict.get(tg).keys()).difference(set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "                             \n",
    "tg_retained_dropped_files_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': tg_data_schema_dict[f['FileGrp']], 'Desc' : 'Dropped' }\n",
    "                                          for f in tg_retained_dropped_files]\n",
    "\n",
    "tg_retained_report_data.extend(tg_retained_dropped_files_report_data)\n",
    "log_report(tg_retained_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema', 'Desc'], sort_by = ['Taxonomy_Grp','Date']) \n",
    "\n",
    "#''' End '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute.use_bottleneck : bool\n",
      "    Use the bottleneck library to accelerate if it is installed,\n",
      "    the default is True\n",
      "    Valid values: False,True\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "compute.use_numexpr : bool\n",
      "    Use the numexpr library to accelerate computation if it is installed,\n",
      "    the default is True\n",
      "    Valid values: False,True\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.chop_threshold : float or None\n",
      "    if set to a float value, all float values smaller then the given threshold\n",
      "    will be displayed as exactly 0 by repr and friends.\n",
      "    [default: None] [currently: None]\n",
      "\n",
      "display.colheader_justify : 'left'/'right'\n",
      "    Controls the justification of column headers. used by DataFrameFormatter.\n",
      "    [default: right] [currently: left]\n",
      "\n",
      "display.column_space No description available.\n",
      "    [default: 12] [currently: 12]\n",
      "\n",
      "display.date_dayfirst : boolean\n",
      "    When True, prints and parses dates with the day first, eg 20/01/2005\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.date_yearfirst : boolean\n",
      "    When True, prints and parses dates with the year first, eg 2005/01/20\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.encoding : str/unicode\n",
      "    Defaults to the detected encoding of the console.\n",
      "    Specifies the encoding to be used for strings returned by to_string,\n",
      "    these are generally strings meant to be displayed on the console.\n",
      "    [default: UTF-8] [currently: UTF-8]\n",
      "\n",
      "display.expand_frame_repr : boolean\n",
      "    Whether to print out the full DataFrame repr for wide DataFrames across\n",
      "    multiple lines, `max_columns` is still respected, but the output will\n",
      "    wrap-around across multiple \"pages\" if its width exceeds `display.width`.\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.float_format : callable\n",
      "    The callable should accept a floating point number and return\n",
      "    a string with the desired format of the number. This is used\n",
      "    in some places like SeriesFormatter.\n",
      "    See formats.format.EngFormatter for an example.\n",
      "    [default: None] [currently: None]\n",
      "\n",
      "display.html.border : int\n",
      "    A ``border=value`` attribute is inserted in the ``<table>`` tag\n",
      "    for the DataFrame HTML repr.\n",
      "    [default: 1] [currently: 1]\n",
      "\n",
      "display.html.table_schema : boolean\n",
      "    Whether to publish a Table Schema representation for frontends\n",
      "    that support it.\n",
      "    (default: False)\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.html.use_mathjax : boolean\n",
      "    When True, Jupyter notebook will process table contents using MathJax,\n",
      "    rendering mathematical expressions enclosed by the dollar symbol.\n",
      "    (default: True)\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.large_repr : 'truncate'/'info'\n",
      "    For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can\n",
      "    show a truncated table (the default from 0.13), or switch to the view from\n",
      "    df.info() (the behaviour in earlier versions of pandas).\n",
      "    [default: truncate] [currently: truncate]\n",
      "\n",
      "display.latex.escape : bool\n",
      "    This specifies if the to_latex method of a Dataframe uses escapes special\n",
      "    characters.\n",
      "    Valid values: False,True\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.latex.longtable :bool\n",
      "    This specifies if the to_latex method of a Dataframe uses the longtable\n",
      "    format.\n",
      "    Valid values: False,True\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.latex.multicolumn : bool\n",
      "    This specifies if the to_latex method of a Dataframe uses multicolumns\n",
      "    to pretty-print MultiIndex columns.\n",
      "    Valid values: False,True\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.latex.multicolumn_format : bool\n",
      "    This specifies if the to_latex method of a Dataframe uses multicolumns\n",
      "    to pretty-print MultiIndex columns.\n",
      "    Valid values: False,True\n",
      "    [default: l] [currently: l]\n",
      "\n",
      "display.latex.multirow : bool\n",
      "    This specifies if the to_latex method of a Dataframe uses multirows\n",
      "    to pretty-print MultiIndex rows.\n",
      "    Valid values: False,True\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.latex.repr : boolean\n",
      "    Whether to produce a latex DataFrame representation for jupyter\n",
      "    environments that support it.\n",
      "    (default: False)\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.max_categories : int\n",
      "    This sets the maximum number of categories pandas should output when\n",
      "    printing out a `Categorical` or a Series of dtype \"category\".\n",
      "    [default: 8] [currently: 8]\n",
      "\n",
      "display.max_columns : int\n",
      "    If max_cols is exceeded, switch to truncate view. Depending on\n",
      "    `large_repr`, objects are either centrally truncated or printed as\n",
      "    a summary view. 'None' value means unlimited.\n",
      "\n",
      "    In case python/IPython is running in a terminal and `large_repr`\n",
      "    equals 'truncate' this can be set to 0 and pandas will auto-detect\n",
      "    the width of the terminal and print a truncated object which fits\n",
      "    the screen width. The IPython notebook, IPython qtconsole, or IDLE\n",
      "    do not run in a terminal and hence it is not possible to do\n",
      "    correct auto-detection.\n",
      "    [default: 20] [currently: None]\n",
      "\n",
      "display.max_colwidth : int\n",
      "    The maximum width in characters of a column in the repr of\n",
      "    a pandas data structure. When the column overflows, a \"...\"\n",
      "    placeholder is embedded in the output.\n",
      "    [default: 50] [currently: 1000]\n",
      "\n",
      "display.max_info_columns : int\n",
      "    max_info_columns is used in DataFrame.info method to decide if\n",
      "    per column information will be printed.\n",
      "    [default: 100] [currently: 100]\n",
      "\n",
      "display.max_info_rows : int or None\n",
      "    df.info() will usually show null-counts for each column.\n",
      "    For large frames this can be quite slow. max_info_rows and max_info_cols\n",
      "    limit this null check only to frames with smaller dimensions than\n",
      "    specified.\n",
      "    [default: 1690785] [currently: 1690785]\n",
      "\n",
      "display.max_rows : int\n",
      "    If max_rows is exceeded, switch to truncate view. Depending on\n",
      "    `large_repr`, objects are either centrally truncated or printed as\n",
      "    a summary view. 'None' value means unlimited.\n",
      "\n",
      "    In case python/IPython is running in a terminal and `large_repr`\n",
      "    equals 'truncate' this can be set to 0 and pandas will auto-detect\n",
      "    the height of the terminal and print a truncated object which fits\n",
      "    the screen height. The IPython notebook, IPython qtconsole, or\n",
      "    IDLE do not run in a terminal and hence it is not possible to do\n",
      "    correct auto-detection.\n",
      "    [default: 60] [currently: None]\n",
      "\n",
      "display.max_seq_items : int or None\n",
      "    when pretty-printing a long sequence, no more then `max_seq_items`\n",
      "    will be printed. If items are omitted, they will be denoted by the\n",
      "    addition of \"...\" to the resulting string.\n",
      "\n",
      "    If set to None, the number of items to be printed is unlimited.\n",
      "    [default: 100] [currently: 100]\n",
      "\n",
      "display.memory_usage : bool, string or None\n",
      "    This specifies if the memory usage of a DataFrame should be displayed when\n",
      "    df.info() is called. Valid values True,False,'deep'\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.multi_sparse : boolean\n",
      "    \"sparsify\" MultiIndex display (don't display repeated\n",
      "    elements in outer levels within groups)\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.notebook_repr_html : boolean\n",
      "    When True, IPython notebook will use html representation for\n",
      "    pandas objects (if it is available).\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.pprint_nest_depth : int\n",
      "    Controls the number of nested levels to process when pretty-printing\n",
      "    [default: 3] [currently: 3]\n",
      "\n",
      "display.precision : int\n",
      "    Floating point output precision (number of significant digits). This is\n",
      "    only a suggestion\n",
      "    [default: 6] [currently: 6]\n",
      "\n",
      "display.show_dimensions : boolean or 'truncate'\n",
      "    Whether to print out dimensions at the end of DataFrame repr.\n",
      "    If 'truncate' is specified, only print out the dimensions if the\n",
      "    frame is truncated (e.g. not display all rows and/or columns)\n",
      "    [default: truncate] [currently: truncate]\n",
      "\n",
      "display.unicode.ambiguous_as_wide : boolean\n",
      "    Whether to use the Unicode East Asian Width to calculate the display text\n",
      "    width.\n",
      "    Enabling this may affect to the performance (default: False)\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.unicode.east_asian_width : boolean\n",
      "    Whether to use the Unicode East Asian Width to calculate the display text\n",
      "    width.\n",
      "    Enabling this may affect to the performance (default: False)\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.width : int\n",
      "    Width of the display in characters. In case python/IPython is running in\n",
      "    a terminal this can be set to None and pandas will correctly auto-detect\n",
      "    the width.\n",
      "    Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a\n",
      "    terminal and hence it is not possible to correctly detect the width.\n",
      "    [default: 80] [currently: 1000]\n",
      "\n",
      "html.border : int\n",
      "    A ``border=value`` attribute is inserted in the ``<table>`` tag\n",
      "    for the DataFrame HTML repr.\n",
      "    [default: 1] [currently: 1]\n",
      "    (Deprecated, use `display.html.border` instead.)\n",
      "\n",
      "io.excel.xls.writer : string\n",
      "    The default Excel writer engine for 'xls' files. Available options:\n",
      "    auto, xlwt.\n",
      "    [default: auto] [currently: auto]\n",
      "\n",
      "io.excel.xlsm.writer : string\n",
      "    The default Excel writer engine for 'xlsm' files. Available options:\n",
      "    auto, openpyxl.\n",
      "    [default: auto] [currently: auto]\n",
      "\n",
      "io.excel.xlsx.writer : string\n",
      "    The default Excel writer engine for 'xlsx' files. Available options:\n",
      "    auto, openpyxl, xlsxwriter.\n",
      "    [default: auto] [currently: auto]\n",
      "\n",
      "io.hdf.default_format : format\n",
      "    default format writing format, if None, then\n",
      "    put will default to 'fixed' and append will default to 'table'\n",
      "    [default: None] [currently: None]\n",
      "\n",
      "io.hdf.dropna_table : boolean\n",
      "    drop ALL nan rows when appending to a table\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "io.parquet.engine : string\n",
      "    The default parquet reader/writer engine. Available options:\n",
      "    'auto', 'pyarrow', 'fastparquet', the default is 'auto'\n",
      "    [default: auto] [currently: auto]\n",
      "\n",
      "mode.chained_assignment : string\n",
      "    Raise an exception, warn, or no action if trying to use chained assignment,\n",
      "    The default is warn\n",
      "    [default: warn] [currently: warn]\n",
      "\n",
      "mode.sim_interactive : boolean\n",
      "    Whether to simulate interactive mode for purposes of testing\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "mode.use_inf_as_na : boolean\n",
      "    True means treat None, NaN, INF, -INF as NA (old way),\n",
      "    False means None and NaN are null, but INF, -INF are not NA\n",
      "    (new way).\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "mode.use_inf_as_null : boolean\n",
      "    use_inf_as_null had been deprecated and will be removed in a future\n",
      "    version. Use `use_inf_as_na` instead.\n",
      "    [default: False] [currently: False]\n",
      "    (Deprecated, use `mode.use_inf_as_na` instead.)\n",
      "\n",
      "plotting.matplotlib.register_converters : bool\n",
      "    Whether to register converters with matplotlib's units registry for\n",
      "    dates, times, datetimes, and Periods. Toggling to False will remove\n",
      "    the converters, restoring any converters that pandas overwrote.\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.describe_option()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gen_Report(exposed_tg_all = None, \n",
    "               exposed_dropped_tg_all = None,\n",
    "               invalid_tg_with_dup_schema = None, \n",
    "               invalid_tg_with_dup_target = None,\n",
    "               src_delta  = None,\n",
    "               tg_new  = None, \n",
    "               tg_existing = None,\n",
    "               invalid_files_set = None,\n",
    "               invalid_schema_files = None,\n",
    "               tg_data_files_dict = None, \n",
    "               tg_data_schema_dict = None, \n",
    "               tg_delta_drop_n_create = None, \n",
    "               tg_delta_create = None):\n",
    "\n",
    "    ''' Report '''\n",
    "    \n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "    '''  Exposed TG Report  '''\n",
    "\n",
    "    exposed_tg_report_data = [i.get_dict() for i in exposed_tg_all]\n",
    "    report_title = 'Exposed TG Report'\n",
    "    log_report(exposed_tg_report_data,  columns=['tg_name', 'key_cols', 'target_cols', 'location'], \n",
    "               sort_by='tg_name', report_title=report_title)\n",
    "\n",
    "\n",
    "    '''  Exposed Dropped TG Report '''\n",
    "\n",
    "    exposed_tg_dropped_report_data = [i.get_dict() for i in exposed_dropped_tg_all]\n",
    "    report_title = 'Exposed Dropped TG Report'\n",
    "    log_report(exposed_tg_dropped_report_data, columns=['tg_name', 'key_cols', 'target_cols', 'location'], \n",
    "               sort_by='tg_name', report_title=report_title)\n",
    "\n",
    "\n",
    "    \n",
    "    '''   Dropped TG Completely '''\n",
    "\n",
    "    tg_dropped_rep_gen =(extract_info(f['Key']) for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items())\n",
    "    tg_dropped_report_dict =  [{'Taxonomy_Grp':i['FileGrp'], 'File_Name':i['FileName'], 'Date':i['Date'], 'Schema': tg_data_schema_dict[i['FileGrp']] }\n",
    "                               for i in tg_dropped_rep_gen] \n",
    "    report_title = 'Dropped TG Completely'\n",
    "    log_report(list_of_row_dict=tg_dropped_report_dict, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Dropped TG to change schema '''\n",
    "\n",
    "    tg_drop_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), schema_new, extract_info(f['Key'])) \n",
    "                                 for tg in tg_delta_drop_n_create \n",
    "                                 for schema_new in src_delta['new_tg_schema_dict'].get(tg)\n",
    "                                 for fn, f in tg_data_files_dict.get(tg).items())\n",
    "    tg_drop_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2]} \n",
    "                                         for i in tg_drop_schema_change_rep]\n",
    "    report_title = 'Dropped TG to change schema'\n",
    "    log_report(list_of_row_dict=tg_drop_schema_change_report_dict, \n",
    "               columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema'], \n",
    "               sort_by = ['Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Created TG Absolute New '''\n",
    "\n",
    "    tg_newly_created_report_data = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                        for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Created TG Absolute New'\n",
    "    log_report(tg_newly_created_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Created TG to change schema(with new Schema) '''\n",
    "\n",
    "    tg_re_created_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), f['Schema'], extract_info(f['Key']), 'Re-delivered') \n",
    "                                       if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                       else (tg, 'NAN', f['Schema'], extract_info(f['Key']), 'New File')\n",
    "\n",
    "                                       for tg in tg_delta_drop_n_create \n",
    "                                       #for schema_new in src_delta['new_tg_schema_dict'].get(tg) \n",
    "\n",
    "                                       for fn, f in src_delta['new_tg_files_dict'].get(tg).items() \n",
    "                                       )\n",
    "\n",
    "    tg_recreated_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2], 'Desc': i[4]} \n",
    "                                              for i in tg_re_created_schema_change_rep]\n",
    "    report_title = 'Created TG to change schema(with new Schema)'\n",
    "    log_report(list_of_row_dict=tg_recreated_schema_change_report_dict, \n",
    "               columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema', 'Desc'], \n",
    "               sort_by = ['Grp','Date'], report_title=report_title)\n",
    "\n",
    "\n",
    "    '''   Retained TG with retained files, new files and dropped files '''\n",
    "\n",
    "    tg_retained_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'Retained' }\n",
    "\n",
    "                                if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                else {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'New File' }\n",
    "                                for tg in tg_existing \n",
    "                                for fn, f in src_delta['existing_tg_files_dict'].get(tg).items()]\n",
    "\n",
    "    tg_retained_dropped_files = [extract_info(tg_data_files_dict.get(tg).get(fn)['Key']) \n",
    "                                 for tg in tg_existing \n",
    "                                 for fn in set(tg_data_files_dict.get(tg).keys()).difference(\n",
    "                                     set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "\n",
    "    tg_retained_dropped_files_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': tg_data_schema_dict[f['FileGrp']], 'Desc' : 'Dropped' }\n",
    "                                              for f in tg_retained_dropped_files]\n",
    "\n",
    "    tg_retained_report_data.extend(tg_retained_dropped_files_report_data)\n",
    "    report_title = 'Retained TG with retained files, new files and dropped files'\n",
    "    log_report(tg_retained_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema', 'Desc'], \n",
    "               header_align='left', sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "    \n",
    "    '''  Invalid TG due to schema conflict/already used Report '''\n",
    "\n",
    "    invalid_tg_with_dup_schema_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                                      for i in  invalid_tg_with_dup_schema \n",
    "                                      for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Invalid TG due to schema conflict/already used Report'\n",
    "    log_report(invalid_tg_with_dup_schema_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''  Invalid TG due to Target Column conflict/already used Report '''\n",
    "\n",
    "    invalid_tg_with_dup_target_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                        for i in  invalid_tg_with_dup_target for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Invalid TG due to Target Column conflict/already used Report'\n",
    "    log_report(invalid_tg_with_dup_target_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    ''' Invalid files from retained grp due to schema or target mismatch with previously delivered files for same'''\n",
    "\n",
    "    partially_invalid_tg_set = tg_new.intersection(tg_existing)\n",
    "    partially_invalid_tg_report = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Grp_Schema': tg_data_schema_dict[i] } \n",
    "                                   for i in partially_invalid_tg_set  \n",
    "                                   for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Invalid files from retained grp due to schema or target mismatch'\n",
    "    log_report(partially_invalid_tg_report, columns=['Taxonomy_Grp','File_Name','Date', 'Schema','Grp_Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Invalid file not match with required file pattern '''\n",
    "\n",
    "    invalid_files_report_data = [{'File_Name' : filename_by_key(i)} for i in invalid_files_set]\n",
    "    report_title = 'Invalid file not match with required file pattern'\n",
    "    log_report(invalid_files_report_data,  columns=['File_Name'], \n",
    "               sort_by = ['File_Name'], report_title=report_title)\n",
    "\n",
    "\n",
    "    '''   Invalid file not match with required schema pattern '''\n",
    "\n",
    "    invalid_schema_files_rep_data = [{'File_Name' : filename_by_key(i[0]), 'Schema': i[1], 'Reason' : i[2]} for i in invalid_schema_files]\n",
    "    report_title = 'Invalid file not match with required schema pattern'\n",
    "    log_report(invalid_schema_files_rep_data,  columns=['File_Name', 'Schema','Reason'], \n",
    "               header_align='left', sort_by = ['File_Name'], report_title=report_title)\n",
    "\n",
    "\n",
    "    #''' End '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Exposed TG Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           tg_name key_cols   target_cols   location                                    \n",
      " **      0  tg0      [key_a0]   [target_a0]  s3://qubole-ford/taxonomy_cs/test1/data/tg0\n",
      " **      1  tg1      [key_a1]   [target_a1]  s3://qubole-ford/taxonomy_cs/test1/data/tg1\n",
      " **      2  tg2     [key_a21]  [target_a21]  s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      3  tg4      [key_a4]   [target_a4]  s3://qubole-ford/taxonomy_cs/test1/data/tg4\n",
      " **      4  tg5      [key_a5]   [target_a5]  s3://qubole-ford/taxonomy_cs/test1/data/tg5\n",
      " **      5  tg6      [key_a6]  [target_a61]  s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **      6  tg7      [key_a7]   [target_a7]  s3://qubole-ford/taxonomy_cs/test1/data/tg7\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Exposed Dropped TG Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           tg_name key_cols  target_cols  location                                    \n",
      " **      0  tg2     [key_a2]  [target_a2]  s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      1  tg3     [key_a3]  [target_a3]  s3://qubole-ford/taxonomy_cs/test1/data/tg3\n",
      " **      2  tg6     [key_a6]  [target_a6]  s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Dropped TG Completely\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema           \n",
      " **      0  tg3          tg3_2020-11-01_ford.csv  2020-11-01  key_a3,target_a3\n",
      " **      1  tg3          tg3_2020-11-02_ford.csv  2020-11-02  key_a3,target_a3\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Dropped TG to change schema\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema         \n",
      " **      0  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21\n",
      " **      2  tg2  tg2_2020-11-04_ford.csv  2020-11-04  key_a2,target_a2  key_a21,target_a21\n",
      " **      3  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61\n",
      " **      4  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Created TG Absolute New\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema           \n",
      " **      0  tg0          tg0_2020-11-02_ford.csv  2020-11-02  key_a0,target_a0\n",
      " **      1  tg4          tg4_2020-11-01_ford.csv  2020-11-01  key_a4,target_a4\n",
      " **      2  tg4          tg4_2020-11-02_ford.csv  2020-11-02  key_a4,target_a4\n",
      " **      3  tg4          tg4_2020-11-03_ford.csv  2020-11-03  key_a4,target_a4\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Created TG to change schema(with new Schema)\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema          Desc         \n",
      " **      0  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      2  tg2  tg2_2020-11-03_ford.csv  2020-11-03               NAN  key_a21,target_a21      New File\n",
      " **      3  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **      4  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Retained TG with retained files, new files and dropped files\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema            Desc     \n",
      " **      0  tg1          tg1_2020-11-01_ford.csv  2020-11-01  key_a1,target_a1  Retained\n",
      " **      1  tg1          tg1_2020-11-02_ford.csv  2020-11-02  key_a1,target_a1  New File\n",
      " **      2  tg5          tg5_2020-11-01_ford.csv  2020-11-01  key_a5,target_a5  Retained\n",
      " **      3  tg5          tg5_2020-11-02_ford.csv  2020-11-02  key_a5,target_a5  Retained\n",
      " **      4  tg5          tg5_2020-11-03_ford.csv  2020-11-03  key_a5,target_a5  New File\n",
      " **      5  tg5          tg5_2020-11-05_ford.csv  2020-11-05  key_a5,target_a5   Dropped\n",
      " **      6  tg7          tg7_2020-11-01_ford.csv  2020-11-01  key_a7,target_a7  Retained\n",
      " **      7  tg7          tg7_2020-11-02_ford.csv  2020-11-02  key_a7,target_a7  Retained\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid TG due to schema conflict/already used Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                 Date        Schema               \n",
      " **      0  tg11         tg11_2020-11-01_ford.csv  2020-11-01      key_a9,target_a9\n",
      " **      1  tg14         tg14_2020-11-01_ford.csv  2020-11-01      key_a7,target_a7\n",
      " **      2  tg14         tg14_2020-11-02_ford.csv  2020-11-02      key_a7,target_a7\n",
      " **      3  tg15         tg15_2020-11-01_ford.csv  2020-11-01  key_a151,target_a151\n",
      " **      4  tg15         tg15_2020-11-02_ford.csv  2020-11-02  key_a151,target_a151\n",
      " **      5  tg15         tg15_2020-11-03_ford.csv  2020-11-03  key_a151,target_a151\n",
      " **      6  tg15         tg15_2020-11-05_ford.csv  2020-11-05  key_a152,target_a152\n",
      " **      7   tg9          tg9_2020-11-01_ford.csv  2020-11-01      key_a9,target_a9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid TG due to Target Column conflict/already used Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                 Date        Schema                         \n",
      " **      0  tg10         tg10_2020-11-01_ford.csv  2020-11-01               key_a10,target_a9\n",
      " **      1  tg11         tg11_2020-11-01_ford.csv  2020-11-01                key_a9,target_a9\n",
      " **      2  tg14         tg14_2020-11-01_ford.csv  2020-11-01                key_a7,target_a7\n",
      " **      3  tg14         tg14_2020-11-02_ford.csv  2020-11-02                key_a7,target_a7\n",
      " **      4  tg16         tg16_2020-11-01_ford.csv  2020-11-01  key_a16,target_a16,target_a162\n",
      " **      5  tg16         tg16_2020-11-02_ford.csv  2020-11-02  key_a16,target_a16,target_a162\n",
      " **      6  tg16         tg16_2020-11-03_ford.csv  2020-11-03  key_a16,target_a16,target_a162\n",
      " **      7  tg17         tg17_2020-11-01_ford.csv  2020-11-01    key_a16,target_a16,target_a9\n",
      " **      8   tg8          tg8_2020-11-01_ford.csv  2020-11-01                key_a8,target_a7\n",
      " **      9   tg9          tg9_2020-11-01_ford.csv  2020-11-01                key_a9,target_a9\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid files from retained grp due to schema or target mismatch\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema              Grp_Schema       \n",
      " **      0  tg5          tg5_2020-11-04_ford.csv  2020-11-04  key_a51,target_a51  key_a5,target_a5\n",
      " **      1  tg7          tg7_2020-11-03_ford.csv  2020-11-03   key_a7,target_a71  key_a7,target_a7\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid file not match with required file pattern\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name              \n",
      " **      0  tg0_202-11-01_ford.csv\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid file not match with required schema pattern\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name                Schema            Reason                                                                            \n",
      " **      0  tg0_2020-11-03_ford.csv  key_a0, targe_a0  At least one Target column is required! \\nAll given columns should Key or Target!\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Gen_Report(exposed_tg_all = exposed_tg_all, \n",
    "               exposed_dropped_tg_all = exposed_dropped_tg_all,\n",
    "               invalid_tg_with_dup_schema = invalid_tg_with_dup_schema, \n",
    "               invalid_tg_with_dup_target = invalid_tg_with_dup_target,\n",
    "               src_delta  = src_delta,\n",
    "               tg_new  = tg_new, \n",
    "               tg_existing = tg_existing,\n",
    "               invalid_files_set = invalid_files_set,\n",
    "               invalid_schema_files = invalid_schema_files,\n",
    "               tg_data_files_dict = tg_data_files_dict, \n",
    "               tg_data_schema_dict = tg_data_schema_dict, \n",
    "               tg_delta_drop_n_create = tg_delta_drop_n_create, \n",
    "               tg_delta_create = tg_delta_create\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.__str__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\r\n",
      "Base and utility classes for pandas objects.\r\n",
      "\"\"\"\r\n",
      "import textwrap\r\n",
      "import warnings\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "import pandas._libs.lib as lib\r\n",
      "import pandas.compat as compat\r\n",
      "from pandas.compat import PYPY, OrderedDict, builtins, map, range\r\n",
      "from pandas.compat.numpy import function as nv\r\n",
      "from pandas.errors import AbstractMethodError\r\n",
      "from pandas.util._decorators import Appender, Substitution, cache_readonly\r\n",
      "from pandas.util._validators import validate_bool_kwarg\r\n",
      "\r\n",
      "from pandas.core.dtypes.common import (\r\n",
      "    is_datetime64_ns_dtype, is_datetime64tz_dtype, is_datetimelike,\r\n",
      "    is_extension_array_dtype, is_extension_type, is_list_like, is_object_dtype,\r\n",
      "    is_scalar, is_timedelta64_ns_dtype)\r\n",
      "from pandas.core.dtypes.generic import ABCDataFrame, ABCIndexClass, ABCSeries\r\n",
      "from pandas.core.dtypes.missing import isna\r\n",
      "\r\n",
      "from pandas.core import algorithms, common as com\r\n",
      "from pandas.core.accessor import DirNamesMixin\r\n",
      "import pandas.core.nanops as nanops\r\n",
      "\r\n",
      "_shared_docs = dict()\r\n",
      "_indexops_doc_kwargs = dict(klass='IndexOpsMixin', inplace='',\r\n",
      "                            unique='IndexOpsMixin', duplicated='IndexOpsMixin')\r\n",
      "\r\n",
      "\r\n",
      "class StringMixin(object):\r\n",
      "    \"\"\"implements string methods so long as object defines a `__unicode__`\r\n",
      "    method.\r\n",
      "\r\n",
      "    Handles Python2/3 compatibility transparently.\r\n",
      "    \"\"\"\r\n",
      "    # side note - this could be made into a metaclass if more than one\r\n",
      "    #             object needs\r\n",
      "\r\n",
      "    # ----------------------------------------------------------------------\r\n",
      "    # Formatting\r\n",
      "\r\n",
      "    def __unicode__(self):\r\n",
      "        raise AbstractMethodError(self)\r\n",
      "\r\n",
      "    def __str__(self):\r\n",
      "        \"\"\"\r\n",
      "        Return a string representation for a particular Object\r\n",
      "\r\n",
      "        Invoked by str(df) in both py2/py3.\r\n",
      "        Yields Bytestring in Py2, Unicode String in py3.\r\n",
      "        \"\"\"\r\n",
      "\r\n",
      "        if compat.PY3:\r\n",
      "            return self.__unicode__()\r\n",
      "        return self.__bytes__()\r\n",
      "\r\n",
      "    def __bytes__(self):\r\n",
      "        \"\"\"\r\n",
      "        Return a string representation for a particular object.\r\n",
      "\r\n",
      "        Invoked by bytes(obj) in py3 only.\r\n",
      "        Yields a bytestring in both py2/py3.\r\n",
      "        \"\"\"\r\n",
      "        from pandas.core.config import get_option\r\n",
      "\r\n",
      "        encoding = get_option(\"display.encoding\")\r\n",
      "        return self.__unicode__().encode(encoding, 'replace')\r\n",
      "\r\n",
      "    def __repr__(self):\r\n",
      "        \"\"\"\r\n",
      "        Return a string representation for a particular object.\r\n",
      "\r\n",
      "        Yields Bytestring in Py2, Unicode String in py3.\r\n",
      "        \"\"\"\r\n",
      "        return str(self)\r\n",
      "\r\n",
      "\r\n",
      "class PandasObject(StringMixin, DirNamesMixin):\r\n",
      "\r\n",
      "    \"\"\"baseclass for various pandas objects\"\"\"\r\n",
      "\r\n",
      "    @property\r\n",
      "    def _constructor(self):\r\n",
      "        \"\"\"class constructor (for this class it's just `__class__`\"\"\"\r\n",
      "        return self.__class__\r\n",
      "\r\n",
      "    def __unicode__(self):\r\n",
      "        \"\"\"\r\n",
      "        Return a string representation for a particular object.\r\n",
      "\r\n",
      "        Invoked by unicode(obj) in py2 only. Yields a Unicode String in both\r\n",
      "        py2/py3.\r\n",
      "        \"\"\"\r\n",
      "        # Should be overwritten by base classes\r\n",
      "        return object.__repr__(self)\r\n",
      "\r\n",
      "    def _reset_cache(self, key=None):\r\n",
      "        \"\"\"\r\n",
      "        Reset cached properties. If ``key`` is passed, only clears that key.\r\n",
      "        \"\"\"\r\n",
      "        if getattr(self, '_cache', None) is None:\r\n",
      "            return\r\n",
      "        if key is None:\r\n",
      "            self._cache.clear()\r\n",
      "        else:\r\n",
      "            self._cache.pop(key, None)\r\n",
      "\r\n",
      "    def __sizeof__(self):\r\n",
      "        \"\"\"\r\n",
      "        Generates the total memory usage for an object that returns\r\n",
      "        either a value or Series of values\r\n",
      "        \"\"\"\r\n",
      "        if hasattr(self, 'memory_usage'):\r\n",
      "            mem = self.memory_usage(deep=True)\r\n",
      "            if not is_scalar(mem):\r\n",
      "                mem = mem.sum()\r\n",
      "            return int(mem)\r\n",
      "\r\n",
      "        # no memory_usage attribute, so fall back to\r\n",
      "        # object's 'sizeof'\r\n",
      "        return super(PandasObject, self).__sizeof__()\r\n",
      "\r\n",
      "\r\n",
      "class NoNewAttributesMixin(object):\r\n",
      "    \"\"\"Mixin which prevents adding new attributes.\r\n",
      "\r\n",
      "    Prevents additional attributes via xxx.attribute = \"something\" after a\r\n",
      "    call to `self.__freeze()`. Mainly used to prevent the user from using\r\n",
      "    wrong attributes on a accessor (`Series.cat/.str/.dt`).\r\n",
      "\r\n",
      "    If you really want to add a new attribute at a later time, you need to use\r\n",
      "    `object.__setattr__(self, key, value)`.\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    def _freeze(self):\r\n",
      "        \"\"\"Prevents setting additional attributes\"\"\"\r\n",
      "        object.__setattr__(self, \"__frozen\", True)\r\n",
      "\r\n",
      "    # prevent adding any attribute via s.xxx.new_attribute = ...\r\n",
      "    def __setattr__(self, key, value):\r\n",
      "        # _cache is used by a decorator\r\n",
      "        # We need to check both 1.) cls.__dict__ and 2.) getattr(self, key)\r\n",
      "        # because\r\n",
      "        # 1.) getattr is false for attributes that raise errors\r\n",
      "        # 2.) cls.__dict__ doesn't traverse into base classes\r\n",
      "        if (getattr(self, \"__frozen\", False) and not\r\n",
      "                (key == \"_cache\" or\r\n",
      "                 key in type(self).__dict__ or\r\n",
      "                 getattr(self, key, None) is not None)):\r\n",
      "            raise AttributeError(\"You cannot add any new attribute '{key}'\".\r\n",
      "                                 format(key=key))\r\n",
      "        object.__setattr__(self, key, value)\r\n",
      "\r\n",
      "\r\n",
      "class GroupByError(Exception):\r\n",
      "    pass\r\n",
      "\r\n",
      "\r\n",
      "class DataError(GroupByError):\r\n",
      "    pass\r\n",
      "\r\n",
      "\r\n",
      "class SpecificationError(GroupByError):\r\n",
      "    pass\r\n",
      "\r\n",
      "\r\n",
      "class SelectionMixin(object):\r\n",
      "    \"\"\"\r\n",
      "    mixin implementing the selection & aggregation interface on a group-like\r\n",
      "    object sub-classes need to define: obj, exclusions\r\n",
      "    \"\"\"\r\n",
      "    _selection = None\r\n",
      "    _internal_names = ['_cache', '__setstate__']\r\n",
      "    _internal_names_set = set(_internal_names)\r\n",
      "\r\n",
      "    _builtin_table = OrderedDict((\r\n",
      "        (builtins.sum, np.sum),\r\n",
      "        (builtins.max, np.max),\r\n",
      "        (builtins.min, np.min),\r\n",
      "    ))\r\n",
      "\r\n",
      "    _cython_table = OrderedDict((\r\n",
      "        (builtins.sum, 'sum'),\r\n",
      "        (builtins.max, 'max'),\r\n",
      "        (builtins.min, 'min'),\r\n",
      "        (np.all, 'all'),\r\n",
      "        (np.any, 'any'),\r\n",
      "        (np.sum, 'sum'),\r\n",
      "        (np.nansum, 'sum'),\r\n",
      "        (np.mean, 'mean'),\r\n",
      "        (np.nanmean, 'mean'),\r\n",
      "        (np.prod, 'prod'),\r\n",
      "        (np.nanprod, 'prod'),\r\n",
      "        (np.std, 'std'),\r\n",
      "        (np.nanstd, 'std'),\r\n",
      "        (np.var, 'var'),\r\n",
      "        (np.nanvar, 'var'),\r\n",
      "        (np.median, 'median'),\r\n",
      "        (np.nanmedian, 'median'),\r\n",
      "        (np.max, 'max'),\r\n",
      "        (np.nanmax, 'max'),\r\n",
      "        (np.min, 'min'),\r\n",
      "        (np.nanmin, 'min'),\r\n",
      "        (np.cumprod, 'cumprod'),\r\n",
      "        (np.nancumprod, 'cumprod'),\r\n",
      "        (np.cumsum, 'cumsum'),\r\n",
      "        (np.nancumsum, 'cumsum'),\r\n",
      "    ))\r\n",
      "\r\n",
      "    @property\r\n",
      "    def _selection_name(self):\r\n",
      "        \"\"\"\r\n",
      "        return a name for myself; this would ideally be called\r\n",
      "        the 'name' property, but we cannot conflict with the\r\n",
      "        Series.name property which can be set\r\n",
      "        \"\"\"\r\n",
      "        if self._selection is None:\r\n",
      "            return None  # 'result'\r\n",
      "        else:\r\n",
      "            return self._selection\r\n",
      "\r\n",
      "    @property\r\n",
      "    def _selection_list(self):\r\n",
      "        if not isinstance(self._selection, (list, tuple, ABCSeries,\r\n",
      "                                            ABCIndexClass, np.ndarray)):\r\n",
      "            return [self._selection]\r\n",
      "        return self._selection\r\n",
      "\r\n",
      "    @cache_readonly\r\n",
      "    def _selected_obj(self):\r\n",
      "\r\n",
      "        if self._selection is None or isinstance(self.obj, ABCSeries):\r\n",
      "            return self.obj\r\n",
      "        else:\r\n",
      "            return self.obj[self._selection]\r\n",
      "\r\n",
      "    @cache_readonly\r\n",
      "    def ndim(self):\r\n",
      "        return self._selected_obj.ndim\r\n",
      "\r\n",
      "    @cache_readonly\r\n",
      "    def _obj_with_exclusions(self):\r\n",
      "        if self._selection is not None and isinstance(self.obj,\r\n",
      "                                                      ABCDataFrame):\r\n",
      "            return self.obj.reindex(columns=self._selection_list)\r\n",
      "\r\n",
      "        if len(self.exclusions) > 0:\r\n",
      "            return self.obj.drop(self.exclusions, axis=1)\r\n",
      "        else:\r\n",
      "            return self.obj\r\n",
      "\r\n",
      "    def __getitem__(self, key):\r\n",
      "        if self._selection is not None:\r\n",
      "            raise IndexError('Column(s) {selection} already selected'\r\n",
      "                             .format(selection=self._selection))\r\n",
      "\r\n",
      "        if isinstance(key, (list, tuple, ABCSeries, ABCIndexClass,\r\n",
      "                            np.ndarray)):\r\n",
      "            if len(self.obj.columns.intersection(key)) != len(key):\r\n",
      "                bad_keys = list(set(key).difference(self.obj.columns))\r\n",
      "                raise KeyError(\"Columns not found: {missing}\"\r\n",
      "                               .format(missing=str(bad_keys)[1:-1]))\r\n",
      "            return self._gotitem(list(key), ndim=2)\r\n",
      "\r\n",
      "        elif not getattr(self, 'as_index', False):\r\n",
      "            if key not in self.obj.columns:\r\n",
      "                raise KeyError(\"Column not found: {key}\".format(key=key))\r\n",
      "            return self._gotitem(key, ndim=2)\r\n",
      "\r\n",
      "        else:\r\n",
      "            if key not in self.obj:\r\n",
      "                raise KeyError(\"Column not found: {key}\".format(key=key))\r\n",
      "            return self._gotitem(key, ndim=1)\r\n",
      "\r\n",
      "    def _gotitem(self, key, ndim, subset=None):\r\n",
      "        \"\"\"\r\n",
      "        sub-classes to define\r\n",
      "        return a sliced object\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        key : string / list of selections\r\n",
      "        ndim : 1,2\r\n",
      "            requested ndim of result\r\n",
      "        subset : object, default None\r\n",
      "            subset to act on\r\n",
      "\r\n",
      "        \"\"\"\r\n",
      "        raise AbstractMethodError(self)\r\n",
      "\r\n",
      "    def aggregate(self, func, *args, **kwargs):\r\n",
      "        raise AbstractMethodError(self)\r\n",
      "\r\n",
      "    agg = aggregate\r\n",
      "\r\n",
      "    def _try_aggregate_string_function(self, arg, *args, **kwargs):\r\n",
      "        \"\"\"\r\n",
      "        if arg is a string, then try to operate on it:\r\n",
      "        - try to find a function (or attribute) on ourselves\r\n",
      "        - try to find a numpy function\r\n",
      "        - raise\r\n",
      "\r\n",
      "        \"\"\"\r\n",
      "        assert isinstance(arg, compat.string_types)\r\n",
      "\r\n",
      "        f = getattr(self, arg, None)\r\n",
      "        if f is not None:\r\n",
      "            if callable(f):\r\n",
      "                return f(*args, **kwargs)\r\n",
      "\r\n",
      "            # people may try to aggregate on a non-callable attribute\r\n",
      "            # but don't let them think they can pass args to it\r\n",
      "            assert len(args) == 0\r\n",
      "            assert len([kwarg for kwarg in kwargs\r\n",
      "                        if kwarg not in ['axis', '_level']]) == 0\r\n",
      "            return f\r\n",
      "\r\n",
      "        f = getattr(np, arg, None)\r\n",
      "        if f is not None:\r\n",
      "            return f(self, *args, **kwargs)\r\n",
      "\r\n",
      "        raise ValueError(\"{arg} is an unknown string function\".format(arg=arg))\r\n",
      "\r\n",
      "    def _aggregate(self, arg, *args, **kwargs):\r\n",
      "        \"\"\"\r\n",
      "        provide an implementation for the aggregators\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        arg : string, dict, function\r\n",
      "        *args : args to pass on to the function\r\n",
      "        **kwargs : kwargs to pass on to the function\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        tuple of result, how\r\n",
      "\r\n",
      "        Notes\r\n",
      "        -----\r\n",
      "        how can be a string describe the required post-processing, or\r\n",
      "        None if not required\r\n",
      "        \"\"\"\r\n",
      "        is_aggregator = lambda x: isinstance(x, (list, tuple, dict))\r\n",
      "        is_nested_renamer = False\r\n",
      "\r\n",
      "        _axis = kwargs.pop('_axis', None)\r\n",
      "        if _axis is None:\r\n",
      "            _axis = getattr(self, 'axis', 0)\r\n",
      "        _level = kwargs.pop('_level', None)\r\n",
      "\r\n",
      "        if isinstance(arg, compat.string_types):\r\n",
      "            return self._try_aggregate_string_function(arg, *args,\r\n",
      "                                                       **kwargs), None\r\n",
      "\r\n",
      "        if isinstance(arg, dict):\r\n",
      "\r\n",
      "            # aggregate based on the passed dict\r\n",
      "            if _axis != 0:  # pragma: no cover\r\n",
      "                raise ValueError('Can only pass dict with axis=0')\r\n",
      "\r\n",
      "            obj = self._selected_obj\r\n",
      "\r\n",
      "            def nested_renaming_depr(level=4):\r\n",
      "                # deprecation of nested renaming\r\n",
      "                # GH 15931\r\n",
      "                warnings.warn(\r\n",
      "                    (\"using a dict with renaming \"\r\n",
      "                     \"is deprecated and will be removed in a future \"\r\n",
      "                     \"version\"),\r\n",
      "                    FutureWarning, stacklevel=level)\r\n",
      "\r\n",
      "            # if we have a dict of any non-scalars\r\n",
      "            # eg. {'A' : ['mean']}, normalize all to\r\n",
      "            # be list-likes\r\n",
      "            if any(is_aggregator(x) for x in compat.itervalues(arg)):\r\n",
      "                new_arg = compat.OrderedDict()\r\n",
      "                for k, v in compat.iteritems(arg):\r\n",
      "                    if not isinstance(v, (tuple, list, dict)):\r\n",
      "                        new_arg[k] = [v]\r\n",
      "                    else:\r\n",
      "                        new_arg[k] = v\r\n",
      "\r\n",
      "                    # the keys must be in the columns\r\n",
      "                    # for ndim=2, or renamers for ndim=1\r\n",
      "\r\n",
      "                    # ok for now, but deprecated\r\n",
      "                    # {'A': { 'ra': 'mean' }}\r\n",
      "                    # {'A': { 'ra': ['mean'] }}\r\n",
      "                    # {'ra': ['mean']}\r\n",
      "\r\n",
      "                    # not ok\r\n",
      "                    # {'ra' : { 'A' : 'mean' }}\r\n",
      "                    if isinstance(v, dict):\r\n",
      "                        is_nested_renamer = True\r\n",
      "\r\n",
      "                        if k not in obj.columns:\r\n",
      "                            msg = ('cannot perform renaming for {key} with a '\r\n",
      "                                   'nested dictionary').format(key=k)\r\n",
      "                            raise SpecificationError(msg)\r\n",
      "                        nested_renaming_depr(4 + (_level or 0))\r\n",
      "\r\n",
      "                    elif isinstance(obj, ABCSeries):\r\n",
      "                        nested_renaming_depr()\r\n",
      "                    elif (isinstance(obj, ABCDataFrame) and\r\n",
      "                          k not in obj.columns):\r\n",
      "                        raise KeyError(\r\n",
      "                            \"Column '{col}' does not exist!\".format(col=k))\r\n",
      "\r\n",
      "                arg = new_arg\r\n",
      "\r\n",
      "            else:\r\n",
      "                # deprecation of renaming keys\r\n",
      "                # GH 15931\r\n",
      "                keys = list(compat.iterkeys(arg))\r\n",
      "                if (isinstance(obj, ABCDataFrame) and\r\n",
      "                        len(obj.columns.intersection(keys)) != len(keys)):\r\n",
      "                    nested_renaming_depr()\r\n",
      "\r\n",
      "            from pandas.core.reshape.concat import concat\r\n",
      "\r\n",
      "            def _agg_1dim(name, how, subset=None):\r\n",
      "                \"\"\"\r\n",
      "                aggregate a 1-dim with how\r\n",
      "                \"\"\"\r\n",
      "                colg = self._gotitem(name, ndim=1, subset=subset)\r\n",
      "                if colg.ndim != 1:\r\n",
      "                    raise SpecificationError(\"nested dictionary is ambiguous \"\r\n",
      "                                             \"in aggregation\")\r\n",
      "                return colg.aggregate(how, _level=(_level or 0) + 1)\r\n",
      "\r\n",
      "            def _agg_2dim(name, how):\r\n",
      "                \"\"\"\r\n",
      "                aggregate a 2-dim with how\r\n",
      "                \"\"\"\r\n",
      "                colg = self._gotitem(self._selection, ndim=2,\r\n",
      "                                     subset=obj)\r\n",
      "                return colg.aggregate(how, _level=None)\r\n",
      "\r\n",
      "            def _agg(arg, func):\r\n",
      "                \"\"\"\r\n",
      "                run the aggregations over the arg with func\r\n",
      "                return an OrderedDict\r\n",
      "                \"\"\"\r\n",
      "                result = compat.OrderedDict()\r\n",
      "                for fname, agg_how in compat.iteritems(arg):\r\n",
      "                    result[fname] = func(fname, agg_how)\r\n",
      "                return result\r\n",
      "\r\n",
      "            # set the final keys\r\n",
      "            keys = list(compat.iterkeys(arg))\r\n",
      "            result = compat.OrderedDict()\r\n",
      "\r\n",
      "            # nested renamer\r\n",
      "            if is_nested_renamer:\r\n",
      "                result = list(_agg(arg, _agg_1dim).values())\r\n",
      "\r\n",
      "                if all(isinstance(r, dict) for r in result):\r\n",
      "\r\n",
      "                    result, results = compat.OrderedDict(), result\r\n",
      "                    for r in results:\r\n",
      "                        result.update(r)\r\n",
      "                    keys = list(compat.iterkeys(result))\r\n",
      "\r\n",
      "                else:\r\n",
      "\r\n",
      "                    if self._selection is not None:\r\n",
      "                        keys = None\r\n",
      "\r\n",
      "            # some selection on the object\r\n",
      "            elif self._selection is not None:\r\n",
      "\r\n",
      "                sl = set(self._selection_list)\r\n",
      "\r\n",
      "                # we are a Series like object,\r\n",
      "                # but may have multiple aggregations\r\n",
      "                if len(sl) == 1:\r\n",
      "\r\n",
      "                    result = _agg(arg, lambda fname,\r\n",
      "                                  agg_how: _agg_1dim(self._selection, agg_how))\r\n",
      "\r\n",
      "                # we are selecting the same set as we are aggregating\r\n",
      "                elif not len(sl - set(keys)):\r\n",
      "\r\n",
      "                    result = _agg(arg, _agg_1dim)\r\n",
      "\r\n",
      "                # we are a DataFrame, with possibly multiple aggregations\r\n",
      "                else:\r\n",
      "\r\n",
      "                    result = _agg(arg, _agg_2dim)\r\n",
      "\r\n",
      "            # no selection\r\n",
      "            else:\r\n",
      "\r\n",
      "                try:\r\n",
      "                    result = _agg(arg, _agg_1dim)\r\n",
      "                except SpecificationError:\r\n",
      "\r\n",
      "                    # we are aggregating expecting all 1d-returns\r\n",
      "                    # but we have 2d\r\n",
      "                    result = _agg(arg, _agg_2dim)\r\n",
      "\r\n",
      "            # combine results\r\n",
      "\r\n",
      "            def is_any_series():\r\n",
      "                # return a boolean if we have *any* nested series\r\n",
      "                return any(isinstance(r, ABCSeries)\r\n",
      "                           for r in compat.itervalues(result))\r\n",
      "\r\n",
      "            def is_any_frame():\r\n",
      "                # return a boolean if we have *any* nested series\r\n",
      "                return any(isinstance(r, ABCDataFrame)\r\n",
      "                           for r in compat.itervalues(result))\r\n",
      "\r\n",
      "            if isinstance(result, list):\r\n",
      "                return concat(result, keys=keys, axis=1, sort=True), True\r\n",
      "\r\n",
      "            elif is_any_frame():\r\n",
      "                # we have a dict of DataFrames\r\n",
      "                # return a MI DataFrame\r\n",
      "\r\n",
      "                return concat([result[k] for k in keys],\r\n",
      "                              keys=keys, axis=1), True\r\n",
      "\r\n",
      "            elif isinstance(self, ABCSeries) and is_any_series():\r\n",
      "\r\n",
      "                # we have a dict of Series\r\n",
      "                # return a MI Series\r\n",
      "                try:\r\n",
      "                    result = concat(result)\r\n",
      "                except TypeError:\r\n",
      "                    # we want to give a nice error here if\r\n",
      "                    # we have non-same sized objects, so\r\n",
      "                    # we don't automatically broadcast\r\n",
      "\r\n",
      "                    raise ValueError(\"cannot perform both aggregation \"\r\n",
      "                                     \"and transformation operations \"\r\n",
      "                                     \"simultaneously\")\r\n",
      "\r\n",
      "                return result, True\r\n",
      "\r\n",
      "            # fall thru\r\n",
      "            from pandas import DataFrame, Series\r\n",
      "            try:\r\n",
      "                result = DataFrame(result)\r\n",
      "            except ValueError:\r\n",
      "\r\n",
      "                # we have a dict of scalars\r\n",
      "                result = Series(result,\r\n",
      "                                name=getattr(self, 'name', None))\r\n",
      "\r\n",
      "            return result, True\r\n",
      "        elif is_list_like(arg) and arg not in compat.string_types:\r\n",
      "            # we require a list, but not an 'str'\r\n",
      "            return self._aggregate_multiple_funcs(arg,\r\n",
      "                                                  _level=_level,\r\n",
      "                                                  _axis=_axis), None\r\n",
      "        else:\r\n",
      "            result = None\r\n",
      "\r\n",
      "        f = self._is_cython_func(arg)\r\n",
      "        if f and not args and not kwargs:\r\n",
      "            return getattr(self, f)(), None\r\n",
      "\r\n",
      "        # caller can react\r\n",
      "        return result, True\r\n",
      "\r\n",
      "    def _aggregate_multiple_funcs(self, arg, _level, _axis):\r\n",
      "        from pandas.core.reshape.concat import concat\r\n",
      "\r\n",
      "        if _axis != 0:\r\n",
      "            raise NotImplementedError(\"axis other than 0 is not supported\")\r\n",
      "\r\n",
      "        if self._selected_obj.ndim == 1:\r\n",
      "            obj = self._selected_obj\r\n",
      "        else:\r\n",
      "            obj = self._obj_with_exclusions\r\n",
      "\r\n",
      "        results = []\r\n",
      "        keys = []\r\n",
      "\r\n",
      "        # degenerate case\r\n",
      "        if obj.ndim == 1:\r\n",
      "            for a in arg:\r\n",
      "                try:\r\n",
      "                    colg = self._gotitem(obj.name, ndim=1, subset=obj)\r\n",
      "                    results.append(colg.aggregate(a))\r\n",
      "\r\n",
      "                    # make sure we find a good name\r\n",
      "                    name = com.get_callable_name(a) or a\r\n",
      "                    keys.append(name)\r\n",
      "                except (TypeError, DataError):\r\n",
      "                    pass\r\n",
      "                except SpecificationError:\r\n",
      "                    raise\r\n",
      "\r\n",
      "        # multiples\r\n",
      "        else:\r\n",
      "            for index, col in enumerate(obj):\r\n",
      "                try:\r\n",
      "                    colg = self._gotitem(col, ndim=1,\r\n",
      "                                         subset=obj.iloc[:, index])\r\n",
      "                    results.append(colg.aggregate(arg))\r\n",
      "                    keys.append(col)\r\n",
      "                except (TypeError, DataError):\r\n",
      "                    pass\r\n",
      "                except ValueError:\r\n",
      "                    # cannot aggregate\r\n",
      "                    continue\r\n",
      "                except SpecificationError:\r\n",
      "                    raise\r\n",
      "\r\n",
      "        # if we are empty\r\n",
      "        if not len(results):\r\n",
      "            raise ValueError(\"no results\")\r\n",
      "\r\n",
      "        try:\r\n",
      "            return concat(results, keys=keys, axis=1, sort=False)\r\n",
      "        except TypeError:\r\n",
      "\r\n",
      "            # we are concatting non-NDFrame objects,\r\n",
      "            # e.g. a list of scalars\r\n",
      "\r\n",
      "            from pandas.core.dtypes.cast import is_nested_object\r\n",
      "            from pandas import Series\r\n",
      "            result = Series(results, index=keys, name=self.name)\r\n",
      "            if is_nested_object(result):\r\n",
      "                raise ValueError(\"cannot combine transform and \"\r\n",
      "                                 \"aggregation operations\")\r\n",
      "            return result\r\n",
      "\r\n",
      "    def _shallow_copy(self, obj=None, obj_type=None, **kwargs):\r\n",
      "        \"\"\"\r\n",
      "        return a new object with the replacement attributes\r\n",
      "        \"\"\"\r\n",
      "        if obj is None:\r\n",
      "            obj = self._selected_obj.copy()\r\n",
      "        if obj_type is None:\r\n",
      "            obj_type = self._constructor\r\n",
      "        if isinstance(obj, obj_type):\r\n",
      "            obj = obj.obj\r\n",
      "        for attr in self._attributes:\r\n",
      "            if attr not in kwargs:\r\n",
      "                kwargs[attr] = getattr(self, attr)\r\n",
      "        return obj_type(obj, **kwargs)\r\n",
      "\r\n",
      "    def _is_cython_func(self, arg):\r\n",
      "        \"\"\"\r\n",
      "        if we define an internal function for this argument, return it\r\n",
      "        \"\"\"\r\n",
      "        return self._cython_table.get(arg)\r\n",
      "\r\n",
      "    def _is_builtin_func(self, arg):\r\n",
      "        \"\"\"\r\n",
      "        if we define an builtin function for this argument, return it,\r\n",
      "        otherwise return the arg\r\n",
      "        \"\"\"\r\n",
      "        return self._builtin_table.get(arg, arg)\r\n",
      "\r\n",
      "\r\n",
      "class IndexOpsMixin(object):\r\n",
      "    \"\"\" common ops mixin to support a unified interface / docs for Series /\r\n",
      "    Index\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    # ndarray compatibility\r\n",
      "    __array_priority__ = 1000\r\n",
      "\r\n",
      "    def transpose(self, *args, **kwargs):\r\n",
      "        \"\"\"\r\n",
      "        Return the transpose, which is by definition self.\r\n",
      "        \"\"\"\r\n",
      "        nv.validate_transpose(args, kwargs)\r\n",
      "        return self\r\n",
      "\r\n",
      "    T = property(transpose, doc=\"Return the transpose, which is by \"\r\n",
      "                                \"definition self.\")\r\n",
      "\r\n",
      "    @property\r\n",
      "    def _is_homogeneous_type(self):\r\n",
      "        \"\"\"\r\n",
      "        Whether the object has a single dtype.\r\n",
      "\r\n",
      "        By definition, Series and Index are always considered homogeneous.\r\n",
      "        A MultiIndex may or may not be homogeneous, depending on the\r\n",
      "        dtypes of the levels.\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        DataFrame._is_homogeneous_type\r\n",
      "        MultiIndex._is_homogeneous_type\r\n",
      "        \"\"\"\r\n",
      "        return True\r\n",
      "\r\n",
      "    @property\r\n",
      "    def shape(self):\r\n",
      "        \"\"\"\r\n",
      "        Return a tuple of the shape of the underlying data.\r\n",
      "        \"\"\"\r\n",
      "        return self._values.shape\r\n",
      "\r\n",
      "    @property\r\n",
      "    def ndim(self):\r\n",
      "        \"\"\"\r\n",
      "        Number of dimensions of the underlying data, by definition 1.\r\n",
      "        \"\"\"\r\n",
      "        return 1\r\n",
      "\r\n",
      "    def item(self):\r\n",
      "        \"\"\"\r\n",
      "        Return the first element of the underlying data as a python scalar.\r\n",
      "        \"\"\"\r\n",
      "        try:\r\n",
      "            return self.values.item()\r\n",
      "        except IndexError:\r\n",
      "            # copy numpy's message here because Py26 raises an IndexError\r\n",
      "            raise ValueError('can only convert an array of size 1 to a '\r\n",
      "                             'Python scalar')\r\n",
      "\r\n",
      "    @property\r\n",
      "    def data(self):\r\n",
      "        \"\"\"\r\n",
      "        Return the data pointer of the underlying data.\r\n",
      "        \"\"\"\r\n",
      "        warnings.warn(\"{obj}.data is deprecated and will be removed \"\r\n",
      "                      \"in a future version\".format(obj=type(self).__name__),\r\n",
      "                      FutureWarning, stacklevel=2)\r\n",
      "        return self.values.data\r\n",
      "\r\n",
      "    @property\r\n",
      "    def itemsize(self):\r\n",
      "        \"\"\"\r\n",
      "        Return the size of the dtype of the item of the underlying data.\r\n",
      "        \"\"\"\r\n",
      "        warnings.warn(\"{obj}.itemsize is deprecated and will be removed \"\r\n",
      "                      \"in a future version\".format(obj=type(self).__name__),\r\n",
      "                      FutureWarning, stacklevel=2)\r\n",
      "        return self._ndarray_values.itemsize\r\n",
      "\r\n",
      "    @property\r\n",
      "    def nbytes(self):\r\n",
      "        \"\"\"\r\n",
      "        Return the number of bytes in the underlying data.\r\n",
      "        \"\"\"\r\n",
      "        return self._values.nbytes\r\n",
      "\r\n",
      "    @property\r\n",
      "    def strides(self):\r\n",
      "        \"\"\"\r\n",
      "        Return the strides of the underlying data.\r\n",
      "        \"\"\"\r\n",
      "        warnings.warn(\"{obj}.strides is deprecated and will be removed \"\r\n",
      "                      \"in a future version\".format(obj=type(self).__name__),\r\n",
      "                      FutureWarning, stacklevel=2)\r\n",
      "        return self._ndarray_values.strides\r\n",
      "\r\n",
      "    @property\r\n",
      "    def size(self):\r\n",
      "        \"\"\"\r\n",
      "        Return the number of elements in the underlying data.\r\n",
      "        \"\"\"\r\n",
      "        return self._values.size\r\n",
      "\r\n",
      "    @property\r\n",
      "    def flags(self):\r\n",
      "        \"\"\"\r\n",
      "        Return the ndarray.flags for the underlying data.\r\n",
      "        \"\"\"\r\n",
      "        warnings.warn(\"{obj}.flags is deprecated and will be removed \"\r\n",
      "                      \"in a future version\".format(obj=type(self).__name__),\r\n",
      "                      FutureWarning, stacklevel=2)\r\n",
      "        return self.values.flags\r\n",
      "\r\n",
      "    @property\r\n",
      "    def base(self):\r\n",
      "        \"\"\"\r\n",
      "        Return the base object if the memory of the underlying data is shared.\r\n",
      "        \"\"\"\r\n",
      "        warnings.warn(\"{obj}.base is deprecated and will be removed \"\r\n",
      "                      \"in a future version\".format(obj=type(self).__name__),\r\n",
      "                      FutureWarning, stacklevel=2)\r\n",
      "        return self.values.base\r\n",
      "\r\n",
      "    @property\r\n",
      "    def array(self):\r\n",
      "        # type: () -> ExtensionArray\r\n",
      "        \"\"\"\r\n",
      "        The ExtensionArray of the data backing this Series or Index.\r\n",
      "\r\n",
      "        .. versionadded:: 0.24.0\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        array : ExtensionArray\r\n",
      "            An ExtensionArray of the values stored within. For extension\r\n",
      "            types, this is the actual array. For NumPy native types, this\r\n",
      "            is a thin (no copy) wrapper around :class:`numpy.ndarray`.\r\n",
      "\r\n",
      "            ``.array`` differs ``.values`` which may require converting the\r\n",
      "            data to a different form.\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        Index.to_numpy : Similar method that always returns a NumPy array.\r\n",
      "        Series.to_numpy : Similar method that always returns a NumPy array.\r\n",
      "\r\n",
      "        Notes\r\n",
      "        -----\r\n",
      "        This table lays out the different array types for each extension\r\n",
      "        dtype within pandas.\r\n",
      "\r\n",
      "        ================== =============================\r\n",
      "        dtype              array type\r\n",
      "        ================== =============================\r\n",
      "        category           Categorical\r\n",
      "        period             PeriodArray\r\n",
      "        interval           IntervalArray\r\n",
      "        IntegerNA          IntegerArray\r\n",
      "        datetime64[ns, tz] DatetimeArray\r\n",
      "        ================== =============================\r\n",
      "\r\n",
      "        For any 3rd-party extension types, the array type will be an\r\n",
      "        ExtensionArray.\r\n",
      "\r\n",
      "        For all remaining dtypes ``.array`` will be a\r\n",
      "        :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\r\n",
      "        stored within. If you absolutely need a NumPy array (possibly with\r\n",
      "        copying / coercing data), then use :meth:`Series.to_numpy` instead.\r\n",
      "\r\n",
      "        Examples\r\n",
      "        --------\r\n",
      "\r\n",
      "        For regular NumPy types like int, and float, a PandasArray\r\n",
      "        is returned.\r\n",
      "\r\n",
      "        >>> pd.Series([1, 2, 3]).array\r\n",
      "        <PandasArray>\r\n",
      "        [1, 2, 3]\r\n",
      "        Length: 3, dtype: int64\r\n",
      "\r\n",
      "        For extension types, like Categorical, the actual ExtensionArray\r\n",
      "        is returned\r\n",
      "\r\n",
      "        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n",
      "        >>> ser.array\r\n",
      "        [a, b, a]\r\n",
      "        Categories (2, object): [a, b]\r\n",
      "        \"\"\"\r\n",
      "        result = self._values\r\n",
      "\r\n",
      "        if is_datetime64_ns_dtype(result.dtype):\r\n",
      "            from pandas.arrays import DatetimeArray\r\n",
      "            result = DatetimeArray(result)\r\n",
      "        elif is_timedelta64_ns_dtype(result.dtype):\r\n",
      "            from pandas.arrays import TimedeltaArray\r\n",
      "            result = TimedeltaArray(result)\r\n",
      "\r\n",
      "        elif not is_extension_array_dtype(result.dtype):\r\n",
      "            from pandas.core.arrays.numpy_ import PandasArray\r\n",
      "            result = PandasArray(result)\r\n",
      "\r\n",
      "        return result\r\n",
      "\r\n",
      "    def to_numpy(self, dtype=None, copy=False):\r\n",
      "        \"\"\"\r\n",
      "        A NumPy ndarray representing the values in this Series or Index.\r\n",
      "\r\n",
      "        .. versionadded:: 0.24.0\r\n",
      "\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        dtype : str or numpy.dtype, optional\r\n",
      "            The dtype to pass to :meth:`numpy.asarray`\r\n",
      "        copy : bool, default False\r\n",
      "            Whether to ensure that the returned value is a not a view on\r\n",
      "            another array. Note that ``copy=False`` does not *ensure* that\r\n",
      "            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\r\n",
      "            a copy is made, even if not strictly necessary.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        numpy.ndarray\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        Series.array : Get the actual data stored within.\r\n",
      "        Index.array : Get the actual data stored within.\r\n",
      "        DataFrame.to_numpy : Similar method for DataFrame.\r\n",
      "\r\n",
      "        Notes\r\n",
      "        -----\r\n",
      "        The returned array will be the same up to equality (values equal\r\n",
      "        in `self` will be equal in the returned array; likewise for values\r\n",
      "        that are not equal). When `self` contains an ExtensionArray, the\r\n",
      "        dtype may be different. For example, for a category-dtype Series,\r\n",
      "        ``to_numpy()`` will return a NumPy array and the categorical dtype\r\n",
      "        will be lost.\r\n",
      "\r\n",
      "        For NumPy dtypes, this will be a reference to the actual data stored\r\n",
      "        in this Series or Index (assuming ``copy=False``). Modifying the result\r\n",
      "        in place will modify the data stored in the Series or Index (not that\r\n",
      "        we recommend doing that).\r\n",
      "\r\n",
      "        For extension types, ``to_numpy()`` *may* require copying data and\r\n",
      "        coercing the result to a NumPy type (possibly object), which may be\r\n",
      "        expensive. When you need a no-copy reference to the underlying data,\r\n",
      "        :attr:`Series.array` should be used instead.\r\n",
      "\r\n",
      "        This table lays out the different dtypes and default return types of\r\n",
      "        ``to_numpy()`` for various dtypes within pandas.\r\n",
      "\r\n",
      "        ================== ================================\r\n",
      "        dtype              array type\r\n",
      "        ================== ================================\r\n",
      "        category[T]        ndarray[T] (same dtype as input)\r\n",
      "        period             ndarray[object] (Periods)\r\n",
      "        interval           ndarray[object] (Intervals)\r\n",
      "        IntegerNA          ndarray[object]\r\n",
      "        datetime64[ns]     datetime64[ns]\r\n",
      "        datetime64[ns, tz] ndarray[object] (Timestamps)\r\n",
      "        ================== ================================\r\n",
      "\r\n",
      "        Examples\r\n",
      "        --------\r\n",
      "        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\r\n",
      "        >>> ser.to_numpy()\r\n",
      "        array(['a', 'b', 'a'], dtype=object)\r\n",
      "\r\n",
      "        Specify the `dtype` to control how datetime-aware data is represented.\r\n",
      "        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\r\n",
      "        objects, each with the correct ``tz``.\r\n",
      "\r\n",
      "        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\r\n",
      "        >>> ser.to_numpy(dtype=object)\r\n",
      "        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET', freq='D'),\r\n",
      "               Timestamp('2000-01-02 00:00:00+0100', tz='CET', freq='D')],\r\n",
      "              dtype=object)\r\n",
      "\r\n",
      "        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\r\n",
      "        datetime64 values. The values are converted to UTC and the timezone\r\n",
      "        info is dropped.\r\n",
      "\r\n",
      "        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\r\n",
      "        ... # doctest: +ELLIPSIS\r\n",
      "        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\r\n",
      "              dtype='datetime64[ns]')\r\n",
      "        \"\"\"\r\n",
      "        if is_datetime64tz_dtype(self.dtype) and dtype is None:\r\n",
      "            # note: this is going to change very soon.\r\n",
      "            # I have a WIP PR making this unnecessary, but it's\r\n",
      "            # a bit out of scope for the DatetimeArray PR.\r\n",
      "            dtype = \"object\"\r\n",
      "\r\n",
      "        result = np.asarray(self._values, dtype=dtype)\r\n",
      "        # TODO(GH-24345): Avoid potential double copy\r\n",
      "        if copy:\r\n",
      "            result = result.copy()\r\n",
      "        return result\r\n",
      "\r\n",
      "    @property\r\n",
      "    def _ndarray_values(self):\r\n",
      "        # type: () -> np.ndarray\r\n",
      "        \"\"\"\r\n",
      "        The data as an ndarray, possibly losing information.\r\n",
      "\r\n",
      "        The expectation is that this is cheap to compute, and is primarily\r\n",
      "        used for interacting with our indexers.\r\n",
      "\r\n",
      "        - categorical -> codes\r\n",
      "        \"\"\"\r\n",
      "        if is_extension_array_dtype(self):\r\n",
      "            return self.array._ndarray_values\r\n",
      "        return self.values\r\n",
      "\r\n",
      "    @property\r\n",
      "    def empty(self):\r\n",
      "        return not self.size\r\n",
      "\r\n",
      "    def max(self, axis=None, skipna=True):\r\n",
      "        \"\"\"\r\n",
      "        Return the maximum value of the Index.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        axis : int, optional\r\n",
      "            For compatibility with NumPy. Only 0 or None are allowed.\r\n",
      "        skipna : bool, default True\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        scalar\r\n",
      "            Maximum value.\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        Index.min : Return the minimum value in an Index.\r\n",
      "        Series.max : Return the maximum value in a Series.\r\n",
      "        DataFrame.max : Return the maximum values in a DataFrame.\r\n",
      "\r\n",
      "        Examples\r\n",
      "        --------\r\n",
      "        >>> idx = pd.Index([3, 2, 1])\r\n",
      "        >>> idx.max()\r\n",
      "        3\r\n",
      "\r\n",
      "        >>> idx = pd.Index(['c', 'b', 'a'])\r\n",
      "        >>> idx.max()\r\n",
      "        'c'\r\n",
      "\r\n",
      "        For a MultiIndex, the maximum is determined lexicographically.\r\n",
      "\r\n",
      "        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\r\n",
      "        >>> idx.max()\r\n",
      "        ('b', 2)\r\n",
      "        \"\"\"\r\n",
      "        nv.validate_minmax_axis(axis)\r\n",
      "        return nanops.nanmax(self._values, skipna=skipna)\r\n",
      "\r\n",
      "    def argmax(self, axis=None, skipna=True):\r\n",
      "        \"\"\"\r\n",
      "        Return a ndarray of the maximum argument indexer.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        axis : {None}\r\n",
      "            Dummy argument for consistency with Series\r\n",
      "        skipna : bool, default True\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        numpy.ndarray.argmax\r\n",
      "        \"\"\"\r\n",
      "        nv.validate_minmax_axis(axis)\r\n",
      "        return nanops.nanargmax(self._values, skipna=skipna)\r\n",
      "\r\n",
      "    def min(self, axis=None, skipna=True):\r\n",
      "        \"\"\"\r\n",
      "        Return the minimum value of the Index.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        axis : {None}\r\n",
      "            Dummy argument for consistency with Series\r\n",
      "        skipna : bool, default True\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        scalar\r\n",
      "            Minimum value.\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        Index.max : Return the maximum value of the object.\r\n",
      "        Series.min : Return the minimum value in a Series.\r\n",
      "        DataFrame.min : Return the minimum values in a DataFrame.\r\n",
      "\r\n",
      "        Examples\r\n",
      "        --------\r\n",
      "        >>> idx = pd.Index([3, 2, 1])\r\n",
      "        >>> idx.min()\r\n",
      "        1\r\n",
      "\r\n",
      "        >>> idx = pd.Index(['c', 'b', 'a'])\r\n",
      "        >>> idx.min()\r\n",
      "        'a'\r\n",
      "\r\n",
      "        For a MultiIndex, the minimum is determined lexicographically.\r\n",
      "\r\n",
      "        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\r\n",
      "        >>> idx.min()\r\n",
      "        ('a', 1)\r\n",
      "        \"\"\"\r\n",
      "        nv.validate_minmax_axis(axis)\r\n",
      "        return nanops.nanmin(self._values, skipna=skipna)\r\n",
      "\r\n",
      "    def argmin(self, axis=None, skipna=True):\r\n",
      "        \"\"\"\r\n",
      "        Return a ndarray of the minimum argument indexer.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        axis : {None}\r\n",
      "            Dummy argument for consistency with Series\r\n",
      "        skipna : bool, default True\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        numpy.ndarray.argmin\r\n",
      "        \"\"\"\r\n",
      "        nv.validate_minmax_axis(axis)\r\n",
      "        return nanops.nanargmin(self._values, skipna=skipna)\r\n",
      "\r\n",
      "    def tolist(self):\r\n",
      "        \"\"\"\r\n",
      "        Return a list of the values.\r\n",
      "\r\n",
      "        These are each a scalar type, which is a Python scalar\r\n",
      "        (for str, int, float) or a pandas scalar\r\n",
      "        (for Timestamp/Timedelta/Interval/Period)\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        numpy.ndarray.tolist\r\n",
      "        \"\"\"\r\n",
      "        if is_datetimelike(self._values):\r\n",
      "            return [com.maybe_box_datetimelike(x) for x in self._values]\r\n",
      "        elif is_extension_array_dtype(self._values):\r\n",
      "            return list(self._values)\r\n",
      "        else:\r\n",
      "            return self._values.tolist()\r\n",
      "\r\n",
      "    to_list = tolist\r\n",
      "\r\n",
      "    def __iter__(self):\r\n",
      "        \"\"\"\r\n",
      "        Return an iterator of the values.\r\n",
      "\r\n",
      "        These are each a scalar type, which is a Python scalar\r\n",
      "        (for str, int, float) or a pandas scalar\r\n",
      "        (for Timestamp/Timedelta/Interval/Period)\r\n",
      "        \"\"\"\r\n",
      "        # We are explicity making element iterators.\r\n",
      "        if is_datetimelike(self._values):\r\n",
      "            return map(com.maybe_box_datetimelike, self._values)\r\n",
      "        elif is_extension_array_dtype(self._values):\r\n",
      "            return iter(self._values)\r\n",
      "        else:\r\n",
      "            return map(self._values.item, range(self._values.size))\r\n",
      "\r\n",
      "    @cache_readonly\r\n",
      "    def hasnans(self):\r\n",
      "        \"\"\"\r\n",
      "        Return if I have any nans; enables various perf speedups.\r\n",
      "        \"\"\"\r\n",
      "        return bool(isna(self).any())\r\n",
      "\r\n",
      "    def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\r\n",
      "                filter_type=None, **kwds):\r\n",
      "        \"\"\" perform the reduction type operation if we can \"\"\"\r\n",
      "        func = getattr(self, name, None)\r\n",
      "        if func is None:\r\n",
      "            raise TypeError(\"{klass} cannot perform the operation {op}\".format(\r\n",
      "                            klass=self.__class__.__name__, op=name))\r\n",
      "        return func(skipna=skipna, **kwds)\r\n",
      "\r\n",
      "    def _map_values(self, mapper, na_action=None):\r\n",
      "        \"\"\"\r\n",
      "        An internal function that maps values using the input\r\n",
      "        correspondence (which can be a dict, Series, or function).\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        mapper : function, dict, or Series\r\n",
      "            The input correspondence object\r\n",
      "        na_action : {None, 'ignore'}\r\n",
      "            If 'ignore', propagate NA values, without passing them to the\r\n",
      "            mapping function\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        applied : Union[Index, MultiIndex], inferred\r\n",
      "            The output of the mapping function applied to the index.\r\n",
      "            If the function returns a tuple with more than one element\r\n",
      "            a MultiIndex will be returned.\r\n",
      "\r\n",
      "        \"\"\"\r\n",
      "\r\n",
      "        # we can fastpath dict/Series to an efficient map\r\n",
      "        # as we know that we are not going to have to yield\r\n",
      "        # python types\r\n",
      "        if isinstance(mapper, dict):\r\n",
      "            if hasattr(mapper, '__missing__'):\r\n",
      "                # If a dictionary subclass defines a default value method,\r\n",
      "                # convert mapper to a lookup function (GH #15999).\r\n",
      "                dict_with_default = mapper\r\n",
      "                mapper = lambda x: dict_with_default[x]\r\n",
      "            else:\r\n",
      "                # Dictionary does not have a default. Thus it's safe to\r\n",
      "                # convert to an Series for efficiency.\r\n",
      "                # we specify the keys here to handle the\r\n",
      "                # possibility that they are tuples\r\n",
      "                from pandas import Series\r\n",
      "                mapper = Series(mapper)\r\n",
      "\r\n",
      "        if isinstance(mapper, ABCSeries):\r\n",
      "            # Since values were input this means we came from either\r\n",
      "            # a dict or a series and mapper should be an index\r\n",
      "            if is_extension_type(self.dtype):\r\n",
      "                values = self._values\r\n",
      "            else:\r\n",
      "                values = self.values\r\n",
      "\r\n",
      "            indexer = mapper.index.get_indexer(values)\r\n",
      "            new_values = algorithms.take_1d(mapper._values, indexer)\r\n",
      "\r\n",
      "            return new_values\r\n",
      "\r\n",
      "        # we must convert to python types\r\n",
      "        if is_extension_type(self.dtype):\r\n",
      "            values = self._values\r\n",
      "            if na_action is not None:\r\n",
      "                raise NotImplementedError\r\n",
      "            map_f = lambda values, f: values.map(f)\r\n",
      "        else:\r\n",
      "            values = self.astype(object)\r\n",
      "            values = getattr(values, 'values', values)\r\n",
      "            if na_action == 'ignore':\r\n",
      "                def map_f(values, f):\r\n",
      "                    return lib.map_infer_mask(values, f,\r\n",
      "                                              isna(values).view(np.uint8))\r\n",
      "            else:\r\n",
      "                map_f = lib.map_infer\r\n",
      "\r\n",
      "        # mapper is a function\r\n",
      "        new_values = map_f(values, mapper)\r\n",
      "\r\n",
      "        return new_values\r\n",
      "\r\n",
      "    def value_counts(self, normalize=False, sort=True, ascending=False,\r\n",
      "                     bins=None, dropna=True):\r\n",
      "        \"\"\"\r\n",
      "        Return a Series containing counts of unique values.\r\n",
      "\r\n",
      "        The resulting object will be in descending order so that the\r\n",
      "        first element is the most frequently-occurring element.\r\n",
      "        Excludes NA values by default.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        normalize : boolean, default False\r\n",
      "            If True then the object returned will contain the relative\r\n",
      "            frequencies of the unique values.\r\n",
      "        sort : boolean, default True\r\n",
      "            Sort by values.\r\n",
      "        ascending : boolean, default False\r\n",
      "            Sort in ascending order.\r\n",
      "        bins : integer, optional\r\n",
      "            Rather than count values, group them into half-open bins,\r\n",
      "            a convenience for ``pd.cut``, only works with numeric data.\r\n",
      "        dropna : boolean, default True\r\n",
      "            Don't include counts of NaN.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        counts : Series\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        Series.count: Number of non-NA elements in a Series.\r\n",
      "        DataFrame.count: Number of non-NA elements in a DataFrame.\r\n",
      "\r\n",
      "        Examples\r\n",
      "        --------\r\n",
      "        >>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\r\n",
      "        >>> index.value_counts()\r\n",
      "        3.0    2\r\n",
      "        4.0    1\r\n",
      "        2.0    1\r\n",
      "        1.0    1\r\n",
      "        dtype: int64\r\n",
      "\r\n",
      "        With `normalize` set to `True`, returns the relative frequency by\r\n",
      "        dividing all values by the sum of values.\r\n",
      "\r\n",
      "        >>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\r\n",
      "        >>> s.value_counts(normalize=True)\r\n",
      "        3.0    0.4\r\n",
      "        4.0    0.2\r\n",
      "        2.0    0.2\r\n",
      "        1.0    0.2\r\n",
      "        dtype: float64\r\n",
      "\r\n",
      "        **bins**\r\n",
      "\r\n",
      "        Bins can be useful for going from a continuous variable to a\r\n",
      "        categorical variable; instead of counting unique\r\n",
      "        apparitions of values, divide the index in the specified\r\n",
      "        number of half-open bins.\r\n",
      "\r\n",
      "        >>> s.value_counts(bins=3)\r\n",
      "        (2.0, 3.0]      2\r\n",
      "        (0.996, 2.0]    2\r\n",
      "        (3.0, 4.0]      1\r\n",
      "        dtype: int64\r\n",
      "\r\n",
      "        **dropna**\r\n",
      "\r\n",
      "        With `dropna` set to `False` we can also see NaN index values.\r\n",
      "\r\n",
      "        >>> s.value_counts(dropna=False)\r\n",
      "        3.0    2\r\n",
      "        NaN    1\r\n",
      "        4.0    1\r\n",
      "        2.0    1\r\n",
      "        1.0    1\r\n",
      "        dtype: int64\r\n",
      "        \"\"\"\r\n",
      "        from pandas.core.algorithms import value_counts\r\n",
      "        result = value_counts(self, sort=sort, ascending=ascending,\r\n",
      "                              normalize=normalize, bins=bins, dropna=dropna)\r\n",
      "        return result\r\n",
      "\r\n",
      "    def unique(self):\r\n",
      "        values = self._values\r\n",
      "\r\n",
      "        if hasattr(values, 'unique'):\r\n",
      "\r\n",
      "            result = values.unique()\r\n",
      "        else:\r\n",
      "            from pandas.core.algorithms import unique1d\r\n",
      "            result = unique1d(values)\r\n",
      "\r\n",
      "        return result\r\n",
      "\r\n",
      "    def nunique(self, dropna=True):\r\n",
      "        \"\"\"\r\n",
      "        Return number of unique elements in the object.\r\n",
      "\r\n",
      "        Excludes NA values by default.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        dropna : boolean, default True\r\n",
      "            Don't include NaN in the count.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        nunique : int\r\n",
      "        \"\"\"\r\n",
      "        uniqs = self.unique()\r\n",
      "        n = len(uniqs)\r\n",
      "        if dropna and isna(uniqs).any():\r\n",
      "            n -= 1\r\n",
      "        return n\r\n",
      "\r\n",
      "    @property\r\n",
      "    def is_unique(self):\r\n",
      "        \"\"\"\r\n",
      "        Return boolean if values in the object are unique.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        is_unique : boolean\r\n",
      "        \"\"\"\r\n",
      "        return self.nunique() == len(self)\r\n",
      "\r\n",
      "    @property\r\n",
      "    def is_monotonic(self):\r\n",
      "        \"\"\"\r\n",
      "        Return boolean if values in the object are\r\n",
      "        monotonic_increasing.\r\n",
      "\r\n",
      "        .. versionadded:: 0.19.0\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        is_monotonic : boolean\r\n",
      "        \"\"\"\r\n",
      "        from pandas import Index\r\n",
      "        return Index(self).is_monotonic\r\n",
      "\r\n",
      "    is_monotonic_increasing = is_monotonic\r\n",
      "\r\n",
      "    @property\r\n",
      "    def is_monotonic_decreasing(self):\r\n",
      "        \"\"\"\r\n",
      "        Return boolean if values in the object are\r\n",
      "        monotonic_decreasing.\r\n",
      "\r\n",
      "        .. versionadded:: 0.19.0\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        is_monotonic_decreasing : boolean\r\n",
      "        \"\"\"\r\n",
      "        from pandas import Index\r\n",
      "        return Index(self).is_monotonic_decreasing\r\n",
      "\r\n",
      "    def memory_usage(self, deep=False):\r\n",
      "        \"\"\"\r\n",
      "        Memory usage of the values\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        deep : bool\r\n",
      "            Introspect the data deeply, interrogate\r\n",
      "            `object` dtypes for system-level memory consumption\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        bytes used\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        numpy.ndarray.nbytes\r\n",
      "\r\n",
      "        Notes\r\n",
      "        -----\r\n",
      "        Memory usage does not include memory consumed by elements that\r\n",
      "        are not components of the array if deep=False or if used on PyPy\r\n",
      "        \"\"\"\r\n",
      "        if hasattr(self.array, 'memory_usage'):\r\n",
      "            return self.array.memory_usage(deep=deep)\r\n",
      "\r\n",
      "        v = self.array.nbytes\r\n",
      "        if deep and is_object_dtype(self) and not PYPY:\r\n",
      "            v += lib.memory_usage_of_objects(self.array)\r\n",
      "        return v\r\n",
      "\r\n",
      "    @Substitution(\r\n",
      "        values='', order='', size_hint='',\r\n",
      "        sort=textwrap.dedent(\"\"\"\\\r\n",
      "            sort : boolean, default False\r\n",
      "                Sort `uniques` and shuffle `labels` to maintain the\r\n",
      "                relationship.\r\n",
      "            \"\"\"))\r\n",
      "    @Appender(algorithms._shared_docs['factorize'])\r\n",
      "    def factorize(self, sort=False, na_sentinel=-1):\r\n",
      "        return algorithms.factorize(self, sort=sort, na_sentinel=na_sentinel)\r\n",
      "\r\n",
      "    _shared_docs['searchsorted'] = (\r\n",
      "        \"\"\"\r\n",
      "        Find indices where elements should be inserted to maintain order.\r\n",
      "\r\n",
      "        Find the indices into a sorted %(klass)s `self` such that, if the\r\n",
      "        corresponding elements in `value` were inserted before the indices,\r\n",
      "        the order of `self` would be preserved.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        value : array_like\r\n",
      "            Values to insert into `self`.\r\n",
      "        side : {'left', 'right'}, optional\r\n",
      "            If 'left', the index of the first suitable location found is given.\r\n",
      "            If 'right', return the last such index.  If there is no suitable\r\n",
      "            index, return either 0 or N (where N is the length of `self`).\r\n",
      "        sorter : 1-D array_like, optional\r\n",
      "            Optional array of integer indices that sort `self` into ascending\r\n",
      "            order. They are typically the result of ``np.argsort``.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        int or array of int\r\n",
      "            A scalar or array of insertion points with the\r\n",
      "            same shape as `value`.\r\n",
      "\r\n",
      "            .. versionchanged :: 0.24.0\r\n",
      "                If `value` is a scalar, an int is now always returned.\r\n",
      "                Previously, scalar inputs returned an 1-item array for\r\n",
      "                :class:`Series` and :class:`Categorical`.\r\n",
      "\r\n",
      "        See Also\r\n",
      "        --------\r\n",
      "        numpy.searchsorted\r\n",
      "\r\n",
      "        Notes\r\n",
      "        -----\r\n",
      "        Binary search is used to find the required insertion points.\r\n",
      "\r\n",
      "        Examples\r\n",
      "        --------\r\n",
      "\r\n",
      "        >>> x = pd.Series([1, 2, 3])\r\n",
      "        >>> x\r\n",
      "        0    1\r\n",
      "        1    2\r\n",
      "        2    3\r\n",
      "        dtype: int64\r\n",
      "\r\n",
      "        >>> x.searchsorted(4)\r\n",
      "        3\r\n",
      "\r\n",
      "        >>> x.searchsorted([0, 4])\r\n",
      "        array([0, 3])\r\n",
      "\r\n",
      "        >>> x.searchsorted([1, 3], side='left')\r\n",
      "        array([0, 2])\r\n",
      "\r\n",
      "        >>> x.searchsorted([1, 3], side='right')\r\n",
      "        array([1, 3])\r\n",
      "\r\n",
      "        >>> x = pd.Categorical(['apple', 'bread', 'bread',\r\n",
      "                                'cheese', 'milk'], ordered=True)\r\n",
      "        [apple, bread, bread, cheese, milk]\r\n",
      "        Categories (4, object): [apple < bread < cheese < milk]\r\n",
      "\r\n",
      "        >>> x.searchsorted('bread')\r\n",
      "        1\r\n",
      "\r\n",
      "        >>> x.searchsorted(['bread'], side='right')\r\n",
      "        array([3])\r\n",
      "        \"\"\")\r\n",
      "\r\n",
      "    @Substitution(klass='IndexOpsMixin')\r\n",
      "    @Appender(_shared_docs['searchsorted'])\r\n",
      "    def searchsorted(self, value, side='left', sorter=None):\r\n",
      "        # needs coercion on the key (DatetimeIndex does already)\r\n",
      "        return self._values.searchsorted(value, side=side, sorter=sorter)\r\n",
      "\r\n",
      "    def drop_duplicates(self, keep='first', inplace=False):\r\n",
      "        inplace = validate_bool_kwarg(inplace, 'inplace')\r\n",
      "        if isinstance(self, ABCIndexClass):\r\n",
      "            if self.is_unique:\r\n",
      "                return self._shallow_copy()\r\n",
      "\r\n",
      "        duplicated = self.duplicated(keep=keep)\r\n",
      "        result = self[np.logical_not(duplicated)]\r\n",
      "        if inplace:\r\n",
      "            return self._update_inplace(result)\r\n",
      "        else:\r\n",
      "            return result\r\n",
      "\r\n",
      "    def duplicated(self, keep='first'):\r\n",
      "        from pandas.core.algorithms import duplicated\r\n",
      "        if isinstance(self, ABCIndexClass):\r\n",
      "            if self.is_unique:\r\n",
      "                return np.zeros(len(self), dtype=np.bool)\r\n",
      "            return duplicated(self, keep=keep)\r\n",
      "        else:\r\n",
      "            return self._constructor(duplicated(self, keep=keep),\r\n",
      "                                     index=self.index).__finalize__(self)\r\n",
      "\r\n",
      "    # ----------------------------------------------------------------------\r\n",
      "    # abstracts\r\n",
      "\r\n",
      "    def _update_inplace(self, result, **kwargs):\r\n",
      "        raise AbstractMethodError(self)\r\n"
     ]
    }
   ],
   "source": [
    "#!cat ~/raj_fb/anaconda3/envs/aws_util/lib/python3.7/site-packages/pandas/core/base.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_util",
   "language": "python",
   "name": "aws_util"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
