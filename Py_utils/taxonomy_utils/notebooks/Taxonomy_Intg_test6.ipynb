{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, inspect, re\n",
    "sys.path.append(\"/home/vbhargava/feature_test0/msaction_backend/common/BU3.0_core/util/Py_utils/taxonomy_utils\")\n",
    "import time, logging\n",
    "import pandas as pd \n",
    "numeric_level = getattr(logging, 'INFO', None)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "logging.basicConfig(level=numeric_level,\n",
    "                        format='%(asctime)s %(levelname)s %(name)s: %(message)s',\n",
    "                        handlers=[stdout_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.s3_ops import S3_OPs\n",
    "from libs.s3_stream import S3Stream\n",
    "from libs.configs import Config\n",
    "from libs.nio_executor import NIO\n",
    "from libs import utils\n",
    "from libs import xml_writer \n",
    "from libs import decorator\n",
    "from collections import defaultdict\n",
    "from model.models import Taxonomy_Grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '/home/vbhargava/feature_test0/msaction_backend/customers/raj_ford_test/common/config/inputs/platform_config.xml'\n",
    "lmt_src = 's3://qubole-ford/taxonomy_cs/test1/src/'\n",
    "lmt_data = 's3://qubole-ford/taxonomy_cs/test1/data/'\n",
    "config_input_loc = '/home/vbhargava/feature_test0/temp/taxo_config_xmls/'\n",
    "config_file_name = 'test.xml'\n",
    "dn_version = '12.1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = Config.get_qubole_config(config)\n",
    "ACCESS_KEY=config_data['access_key']\n",
    "SECRET_KEY=config_data['secret_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TG_EXTRACT_REGEX = '^.*?/([a-zA-Z]+\\-?[0-9]*)/$' \n",
    "FILE_EXTRACT_REGEX = '^.*/([a-zA-Z0-9.\\-_]{0,255}.csv)$' #'^.*/([a-zA-Z0-9.\\-_]{0,255}.csv)$'\n",
    "TARGET_EXTRACT_REGEX ='^.*,?(target_[A-Za-z0-9_-]+).*$'\n",
    "VALID_FILE_KEY_REGEX = '^(.*/([a-zA-Z]+\\-?[0-9]*)?/)?(([a-zA-Z]+\\-?[0-9]*?)_([0-9]{4}-[0-9]{2}-[0-9]{2}?)_([a-zA-Z0-9.\\-_]+?).csv?)$'\n",
    "KEY_REGEX = '^[Kk]ey_[A-Za-z0-9_]{2,30}$'\n",
    "TARGET_REGEX = '^[Tt]arget_[A-Za-z0-9_]{2,30}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_ops = S3_OPs(ACCESS_KEY, SECRET_KEY)\n",
    "\n",
    "def filename_by_key(key):\n",
    "    return get_val_by_regex(key, FILE_EXTRACT_REGEX, error_msg=\"Not vaild key for taxonomy data csv file\")\n",
    "\n",
    "def find_by_data_tg(key, regex):\n",
    "    return get_val_by_regex(key, regex, error_msg=\"Not vaild taxonomy data dir\")\n",
    "\n",
    "        \n",
    "def get_val_by_regex(key, regex, error_msg=\"can't be extract a val.\"):\n",
    "    matched = re.findall(regex, key)\n",
    "    if len(matched) > 0:\n",
    "        return matched[0]\n",
    "    else:\n",
    "        raise Exception(error_msg)\n",
    "        \n",
    "def get_data_n_schema(tg, data_files_loc):\n",
    "    data_file_lock_detail = s3_ops.get_bucket_name(data_files_loc)\n",
    "    files = s3_ops.list_complete(data_file_lock_detail['bucket'], data_file_lock_detail['key'])\n",
    "    res = {}\n",
    "    if len(files)>0:\n",
    "        s3_stream = S3Stream(ACCESS_KEY, SECRET_KEY)\n",
    "        schema = s3_stream.get_header(s3_ops.get_full_s3_path(data_file_lock_detail['bucket'],files[0]['Key']))\n",
    "        #res[tg]={'schema':schema, 'files': files}\n",
    "        res['schema'] = {tg:schema}\n",
    "        res['files'] = {tg:files}\n",
    "    return res\n",
    "\n",
    "def extract_schema(schema):\n",
    "    return schema.replace(\" \",\"\").lower()\n",
    "\n",
    "def validate_schema(schema):\n",
    "    if schema=='': \n",
    "        return {'IsValid' : False, 'schema': schema, 'message' : \"Schema shouldn't be empty\"}\n",
    "    tokens = schema.split(',')\n",
    "    if len(tokens) < 2:\n",
    "         return {'IsValid' : False, 'schema': schema, 'message' : \"Schema should have at least 2 columns\"}\n",
    "    \n",
    "    key_cnt = 0\n",
    "    target_cnt = 0\n",
    "    invalid_headers = []\n",
    "    columns = defaultdict(list)\n",
    "    res = {}\n",
    "    target_col = None\n",
    "    key_cols_set = set()\n",
    "    for t in tokens:\n",
    "        t = t.strip()\n",
    "        if re.match(TARGET_REGEX, t):\n",
    "            target_cnt = target_cnt + 1\n",
    "            target_col = t\n",
    "        elif re.match(KEY_REGEX, t):\n",
    "            key_cnt = key_cnt + 1\n",
    "            key_cols_set.add(t)\n",
    "        else:\n",
    "            invalid_headers.append(t)\n",
    "        columns[t.lower()].append(1)\n",
    "\n",
    "    error_msgs=[]\n",
    "    if target_cnt != 1 :\n",
    "        error_msgs.append(\"Exact one Target column is required!\")\n",
    "    if key_cnt < 1 :\n",
    "        error_msgs.append(\"At least one Key column is required!\")\n",
    "    if len(invalid_headers) > 0 :\n",
    "        error_msgs.append(\"All given columns should Key or Target!\")\n",
    "    for k, v in columns.items():\n",
    "\n",
    "        if len(v) > 1:\n",
    "            print(\"--\")\n",
    "            error_msgs.append(\"Same name: {} should not represent more than one column in schema! cols names are case insensitive. \".format(k))\n",
    "\n",
    "    if len(error_msgs) > 0:\n",
    "        return {'IsValid' : False, 'schema': schema, 'errors' : \" \\n\".join(error_msgs)}\n",
    "    #print(str(key_cnt)+\":\"+str(target_cnt)+\":\"+str(invalid_headers)+\":\"+str(columns))\n",
    "    return {'IsValid' : True, 'Schema': schema.replace(\" \",\"\").lower(), \n",
    "            'TargetCol' : target_col, 'KeyColsSet' : key_cols_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator.elapsed_time(func_name='extract_data_detail')\n",
    "def extract_data_detail(lmt_src, lmt_data, access_key, secret_key):\n",
    "#     Valid data Taxonomy Grps\n",
    "    \n",
    "    #\n",
    "    lmt_data_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "    lmt_data_loc_bucket = lmt_data_loc_detail['bucket']\n",
    "    lmt_data_loc_key = lmt_data_loc_detail['key']\n",
    "    valid_tg_list_res = s3_ops.list_subdirs(lmt_data_loc_detail['bucket'],lmt_data_loc_detail['key'],)\n",
    "    \n",
    "    valid_tgrp_loc_list = [ [find_by_data_tg(item['Prefix'], TG_EXTRACT_REGEX), \n",
    "                         '{}{}'.format(lmt_data, find_by_data_tg(item['Prefix'], TG_EXTRACT_REGEX))] \n",
    "                       for item in valid_tg_list_res]\n",
    "    \n",
    "    collected = NIO.decorated_run_io(task=get_data_n_schema, task_n_args_list=valid_tgrp_loc_list, max_workers=25,)\n",
    "#     return collected\n",
    "    tg_data_schema_dict = {k:extract_schema(v)  for item in collected for k, v in item['result']['schema'].items()}\n",
    "    tg_data_files_dict = {k:{filename_by_key(u['Key']):u for u in v } for item in collected for k, v in item['result']['files'].items()}\n",
    "    target_data_tg_dict = {re.findall(TARGET_EXTRACT_REGEX,V)[0]: K for K, V in tg_data_schema_dict.items()}\n",
    "    \n",
    "    return tg_data_schema_dict, tg_data_files_dict,target_data_tg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_file(key:str='', regex = VALID_FILE_KEY_REGEX):\n",
    "    if re.match(regex, key) is None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_info(key:str='', regex = VALID_FILE_KEY_REGEX):\n",
    "    matched = re.findall(regex, key)\n",
    "    return {\n",
    "            'KeyDirPath' : matched[0][0],\n",
    "            'ParentDir' : matched[0][1],\n",
    "            'FileName' : matched[0][2],\n",
    "            'FileGrp' :  matched[0][3],\n",
    "            'Date' :  matched[0][4],\n",
    "            'ClientName' : matched[0][5]\n",
    "           }\n",
    "def extract_info_with_bucket(key:str='', bucket = ''):\n",
    "    res = extract_info(key)\n",
    "    res.update({'Bucket' : bucket})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_tg(collected, tg_files_dict_type='new_tg_files_dict'):\n",
    "    collect = defaultdict(dict)\n",
    "    tg_f_gen = (item['result'][tg_files_dict_type] for item in collected if len(item['result'][tg_files_dict_type]) > 0)\n",
    "    tg_f_gen2 = (collect[tg].update({filename: file_dict})  for item in tg_f_gen for tg, file_detail_dict in item.items() for filename, file_dict in file_detail_dict.items())\n",
    "    [ i for i in tg_f_gen2]\n",
    "    tg = dict(collect)\n",
    "    return tg\n",
    "\n",
    "def grouped_flag_dict(collected, flag_dict_type='schema_tg_dict'):\n",
    "    f_gen = (item['result'][flag_dict_type] for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    collect = defaultdict(set)\n",
    "    f_gen2 = (collect[K].add(V)  for item in f_gen for K, V in item.items())\n",
    "    [ i for i in f_gen2]\n",
    "    res = dict(collect)\n",
    "    return res\n",
    "\n",
    "def grouped_set_of_flags_dict(collected, flag_dict_type='schema_tg_dict'):\n",
    "    f_gen = (item['result'][flag_dict_type] for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    collect = defaultdict(set)\n",
    "    f_gen2 = (collect[K].update(V)  for item in f_gen for K, V in item.items())\n",
    "    [ i for i in f_gen2]\n",
    "    res = dict(collect)\n",
    "    return res\n",
    "\n",
    "def grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'):\n",
    "    res_set=set()\n",
    "    f_gen = (res_set.update(item['result'][flag_dict_type]) for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    [ i for i in f_gen]\n",
    "    return res_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def file_process_task(src_file_details):\n",
    "    \n",
    "    invalid_schema_files = set()\n",
    "\n",
    "    target_already_exist_files = set()\n",
    "    \n",
    "    ''' {'key_evt_advertiser_key,target_evt_advertiser_name': {'tg1', 'tg2', ...}}'''\n",
    "    schema_tg_dict = {}\n",
    "    \n",
    "    ''' {'target_evt_advertiser_name': {'tg1', 'tg2', ...}}'''\n",
    "    target_tg_dict = {}\n",
    "\n",
    "    ''' {'tg': {'key_evt_advertiser_key,target_evt_advertiser_name', '',...}}'''\n",
    "    new_tg_schema_dict = {}\n",
    "    ''' {'tg': {'AdvertiserReporting_2020-06-01_ford.csv': {file detailed obj dict} }  }'''\n",
    "    new_tg_files_dict = {}\n",
    "    ''' {'tg': {'AdvertiserReporting_2020-06-01_ford.csv': {file detailed obj dict} }  }'''\n",
    "    existing_tg_files_dict = {}\n",
    "\n",
    "\n",
    "    # tg_data_schema_dict = \n",
    "    # tg_data_files_dict = \n",
    "    # target_data_tg_dict = \n",
    "\n",
    "#     src_file_details = valid_file_arg[0]\n",
    "    src_file_loc = s3_ops.get_full_s3_path(src_file_details['Bucket'], src_file_details['Key'])\n",
    "\n",
    "    s3_stream = S3Stream(ACCESS_KEY, SECRET_KEY)\n",
    "    schema =  s3_stream.get_header(src_file_loc)\n",
    "    #schema = 'key_evt_advertiser_key, targe_evt_advertiser_name'\n",
    "    validate_res = validate_schema(schema)\n",
    "    if validate_res['IsValid']:\n",
    "        \n",
    "        \n",
    "        \n",
    "        tg = src_file_details['FileGrp']\n",
    "        file_name = src_file_details['FileName']\n",
    "        \n",
    "        if tg_data_schema_dict.get(tg) is None or tg_data_schema_dict.get(tg) != validate_res['Schema']:\n",
    "                \n",
    "#             data_tg_for_target = target_data_tg_dict.get(validate_res['TargetCol'])\n",
    "#             if  data_tg_for_target is not None:# and data_tg_for_target != tg:\n",
    "#                 target_already_exist_files.add((src_file_loc, data_tg_for_target))\n",
    "#             else:\n",
    "            new_tg_schema_dict[tg] = validate_res['Schema']\n",
    "            new_tg_files_dict[tg] = {file_name: src_file_details}\n",
    "        else:\n",
    "            existing_tg_files_dict[tg] = {file_name: src_file_details}\n",
    "        \n",
    "        src_file_details['Schema'] = validate_res['Schema']\n",
    "        schema_tg_dict[validate_res['Schema']] = tg\n",
    "        target_tg_dict[validate_res['TargetCol']] = tg\n",
    "\n",
    "    else:\n",
    "        invalid_schema_files.add((src_file_loc, schema, validate_res['errors']))\n",
    "\n",
    "    return {'invalid_schema_files': invalid_schema_files,\n",
    "#             'target_already_exist_files':target_already_exist_files,\n",
    "            'schema_tg_dict': schema_tg_dict,\n",
    "            'target_tg_dict':target_tg_dict,\n",
    "            'new_tg_schema_dict': new_tg_schema_dict,\n",
    "            'new_tg_files_dict' : new_tg_files_dict,\n",
    "            'existing_tg_files_dict' : existing_tg_files_dict\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "def src_list_page_process_task(list_page):\n",
    "    \n",
    "    lmt_src_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    lmt_src_loc_bucket = lmt_src_loc_detail['bucket']\n",
    "    lmt_src_loc_key = lmt_src_loc_detail['key']\n",
    "    \n",
    "    invalid_files_set = { s3_ops.get_full_s3_path(lmt_src_loc_detail['bucket'], item['Key']) for item in list_page if  not is_valid_file(key=item['Key'])}\n",
    "    valid_file_set = [[utils.dict_append(extract_info_with_bucket(item['Key'], lmt_src_loc_detail['bucket']),item)] for item in list_page if  is_valid_file(key=item['Key']) ]\n",
    "    collected = NIO.decorated_run_io(task=file_process_task, task_n_args_list=valid_file_set, max_workers=25,)\n",
    "#     return collected\n",
    "    return {'invalid_files_set' : invalid_files_set,\n",
    "            'invalid_schema_files': grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'),\n",
    "#             'target_already_exist_files' : grouped_set_of_flags(collected, flag_dict_type='target_already_exist_files'),\n",
    "            'schema_tg_dict': grouped_flag_dict(collected, flag_dict_type='schema_tg_dict'),\n",
    "            'target_tg_dict': grouped_flag_dict(collected, flag_dict_type='target_tg_dict'),\n",
    "            'new_tg_schema_dict': grouped_flag_dict(collected, flag_dict_type='new_tg_schema_dict'),\n",
    "            'new_tg_files_dict' : grouped_tg(collected, 'new_tg_files_dict'),\n",
    "            'existing_tg_files_dict' : grouped_tg(collected, 'existing_tg_files_dict')\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_src_detail(maxKeysPerReq=3):\n",
    "    lmt_src_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    lmt_src_loc_bucket = lmt_src_loc_detail['bucket']\n",
    "    lmt_src_loc_key = lmt_src_loc_detail['key']\n",
    "    page_generator = s3_ops.list_gen(lmt_src_loc_bucket, lmt_src_loc_key, maxKeysPerReq=maxKeysPerReq, )\n",
    "    page_args_generator = ([page] for page in page_generator)\n",
    "    #list_page = [i for i in page_generator][0]\n",
    "    collected = NIO.decorated_run_with_args_generator(task=src_list_page_process_task, args_generator=page_args_generator, is_kernal_thread=True,)\n",
    "    \n",
    "    return {'invalid_files_set' : grouped_set_of_flags(collected, flag_dict_type='invalid_files_set'),\n",
    "            'invalid_schema_files': grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'),\n",
    "#             'target_already_exist_files' : grouped_set_of_flags(collected, flag_dict_type='target_already_exist_files'),\n",
    "            'schema_tg_dict': grouped_set_of_flags_dict(collected, flag_dict_type='schema_tg_dict'),\n",
    "            'target_tg_dict': grouped_set_of_flags_dict(collected, flag_dict_type='target_tg_dict'),\n",
    "            'new_tg_schema_dict': grouped_set_of_flags_dict(collected, flag_dict_type='new_tg_schema_dict'),\n",
    "            'new_tg_files_dict' : grouped_tg(collected, 'new_tg_files_dict'),\n",
    "            'existing_tg_files_dict' : grouped_tg(collected, 'existing_tg_files_dict')\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' E.g. '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def s3_copy_into_data_loc_task(tg, file_name, src_file, src_size, dry_run=True):\n",
    "#     data_file_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "    src_file_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    src_s3 = 's3://{}/{}'.format(src_file_loc_detail['bucket'], src_file)\n",
    "    dest_s3 = '{}{}/{}'.format(lmt_data,tg, file_name)\n",
    "    if dry_run:\n",
    "        print(\"[dry_run]: S3 copy from {} to {}\".format(src_s3, dest_s3))\n",
    "    else:\n",
    "        pass\n",
    "        #s3_ops.copy(src=src_s3, dest = dest_s3, src_size=src_size)\n",
    "    return 'Copied Successfully! by task'\n",
    "\n",
    "\n",
    "def s3_remove_at_data_loc_task(file,  dry_run=True):\n",
    "    data_file_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "#     src_file_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "#     src_s3 = 's3://{}/{}'.format(lmt_src, src_file)\n",
    "    \n",
    "    if dry_run:\n",
    "        file_loc = 's3://{}/{}'.format(data_file_loc_detail['bucket'], file)\n",
    "        print(\"[dry_run]: S3 delete from {} \".format(file_loc))\n",
    "    else:\n",
    "        pass\n",
    "        #s3_ops.delete_file(data_file_loc_detail['bucket'], file)\n",
    "    return 'Deleted Successfully! by task'\n",
    "\n",
    "''' E.g. '''\n",
    "# s3_copy_into_data_loc_task('tg5', 'tg5_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 48 )\n",
    "# s3_remove_at_data_loc_task('taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_data_detail **Start Time = 2020-11-02 19:44:01.603370\n",
      "\n",
      "2020-11-02 19:44:01,661:84249 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:01,662:84249 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,663:84249 ThreadPoolExecutor-0_0 (task-0): passed args :['tg1', 's3://qubole-ford/taxonomy_cs/test1/data/tg1']\n",
      "\n",
      "2020-11-02 19:44:01,664:84249 ThreadPoolExecutor-0_1 (task-1): passed args :['tg2', 's3://qubole-ford/taxonomy_cs/test1/data/tg2']\n",
      "\n",
      "2020-11-02 19:44:01,664:84249 ThreadPoolExecutor-0_0 (task-0): running\n",
      "\n",
      "2020-11-02 19:44:01,665:84249 ThreadPoolExecutor-0_2 (task-2): passed args :['tg3', 's3://qubole-ford/taxonomy_cs/test1/data/tg3']\n",
      "\n",
      "2020-11-02 19:44:01,665:84249 ThreadPoolExecutor-0_3 (task-3): passed args :['tg5', 's3://qubole-ford/taxonomy_cs/test1/data/tg5']\n",
      "\n",
      "2020-11-02 19:44:01,665:84249 ThreadPoolExecutor-0_4 (task-4): passed args :['tg6', 's3://qubole-ford/taxonomy_cs/test1/data/tg6']\n",
      "\n",
      "2020-11-02 19:44:01,666:84249 ThreadPoolExecutor-0_1 (task-1): running\n",
      "\n",
      "2020-11-02 19:44:01,666:84249 ThreadPoolExecutor-0_5 (task-5): passed args :['tg7', 's3://qubole-ford/taxonomy_cs/test1/data/tg7']\n",
      "\n",
      "2020-11-02 19:44:01,667:84249 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,670:84249 ThreadPoolExecutor-0_2 (task-2): running\n",
      "\n",
      "2020-11-02 19:44:01,671:84249 ThreadPoolExecutor-0_3 (task-3): running\n",
      "\n",
      "2020-11-02 19:44:01,672:84249 ThreadPoolExecutor-0_4 (task-4): running\n",
      "\n",
      "2020-11-02 19:44:01,674:84249 ThreadPoolExecutor-0_5 (task-5): running\n",
      "\n",
      "2020-11-02 19:44:01,761:84249 ThreadPoolExecutor-0_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:01,798:84249 ThreadPoolExecutor-0_5 (task-5): done\n",
      "\n",
      "2020-11-02 19:44:01,801:84249 ThreadPoolExecutor-0_4 (task-4): done\n",
      "\n",
      "2020-11-02 19:44:01,810:84249 ThreadPoolExecutor-0_2 (task-2): done\n",
      "\n",
      "2020-11-02 19:44:01,818:84249 ThreadPoolExecutor-0_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:01,819:84249 ThreadPoolExecutor-0_3 (task-3): done\n",
      "\n",
      "2020-11-02 19:44:01,821:84249 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "extract_data_detail **End Time = 2020-11-02 19:44:01.823368\n",
      "extract_data_detail **Elapsed Time = 0:00:00.219998\n"
     ]
    }
   ],
   "source": [
    "''' Get Existing State of System'''\n",
    "tg_data = extract_data_detail(lmt_src, lmt_data, ACCESS_KEY, SECRET_KEY)\n",
    "tg_data_schema_dict = tg_data[0]\n",
    "tg_data_files_dict = tg_data[1]\n",
    "target_data_tg_dict = tg_data[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 19:44:01,832   process-id:84249 run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:01,832   process-id:84249 run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,899   process-id:84285   (task-0): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg0_202-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"535b60451f6d20c2826b045438a50fb9\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"7a5d08cbb4c718d16851d1f2b57ffc50\"', 'Size': 27, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 21, 21, 36, tzinfo=tzlocal()), 'ETag': '\"b19c288a2ef5e2ec6739cac3674391a6\"', 'Size': 28, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 19:44:01,901   process-id:84285   (task-0): running\n",
      "\n",
      "2020-11-02 19:44:01,904:84285 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:01,905:84285 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,907:84285 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg0_2020-11-02_ford.csv', 'FileGrp': 'tg0', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"7a5d08cbb4c718d16851d1f2b57ffc50\"', 'Size': 27, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,908:84285 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg0_2020-11-03_ford.csv', 'FileGrp': 'tg0', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 21, 21, 36, tzinfo=tzlocal()), 'ETag': '\"b19c288a2ef5e2ec6739cac3674391a6\"', 'Size': 28, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,908:84285 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,908:84285 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 19:44:01,910:84285 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 19:44:01,926   process-id:84286   (task-1): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg10_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"1aa284fe1180b5d4d776e26ff8a03358\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 19:44:01,928   process-id:84286   (task-1): running\n",
      "\n",
      "2020-11-02 19:44:01,932:84286 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:01,933:84286 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,935:84286 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg10_2020-11-01_ford.csv', 'FileGrp': 'tg10', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg10_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"1aa284fe1180b5d4d776e26ff8a03358\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,936:84286 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg11_2020-11-01_ford.csv', 'FileGrp': 'tg11', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,937:84286 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg1_2020-11-01_ford.csv', 'FileGrp': 'tg1', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,937:84286 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,937:84286 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 19:44:01,938:84286 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 19:44:01,947   process-id:84287   (task-2): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 19:44:01,950   process-id:84287   (task-2): running\n",
      "\n",
      "2020-11-02 19:44:01,953:84287 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:01,954:84287 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,939:84286 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 19:44:01,955:84287 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg1_2020-11-02_ford.csv', 'FileGrp': 'tg1', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,956:84287 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-01_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,956:84287 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-02_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,957:84287 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,957:84287 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 19:44:01,961:84285 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:01,958:84287 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 19:44:01,966   process-id:84288   (task-3): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 19:44:01,968:84285 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:01,970   process-id:84288   (task-3): running\n",
      "\n",
      "2020-11-02 19:44:01,971:84285 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 19:44:01,974:84288 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:01,976:84288 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,977:84288 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-03_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,973   process-id:84285   (task-0): done\n",
      "\n",
      "2020-11-02 19:44:01,959:84287 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 19:44:01,978:84288 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-01_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,978:84288 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-02_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,979:84288 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,979:84288 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 19:44:01,987   process-id:84289   (task-4): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 19:44:01,990   process-id:84289   (task-4): running\n",
      "\n",
      "2020-11-02 19:44:01,981:84288 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 19:44:01,993:84289 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:01,994:84289 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:01,996:84289 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-03_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,998:84289 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-01_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:01,998:84289 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 19:44:02,003:84286 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 19:44:01,983:84288 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 19:44:01,998:84289 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-02_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:02,006:84286 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:02,012   process-id:84290   (task-5): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-04_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"f87ad6041aa111ac6b6d0776be1c774f\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 19:44:02,015:84286 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:02,015   process-id:84290   (task-5): running\n",
      "\n",
      "2020-11-02 19:44:02,017:84286 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 19:44:01,999:84289 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,019:84290 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:02,019   process-id:84286   (task-1): done\n",
      "\n",
      "2020-11-02 19:44:01,999:84289 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 19:44:02,020:84290 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,022:84290 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-03_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:02,023:84290 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-04_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-04', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-04_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"f87ad6041aa111ac6b6d0776be1c774f\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:02,016:84289 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 19:44:02,023:84290 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg6_2020-11-01_ford.csv', 'FileGrp': 'tg6', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:02,023:84290 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 19:44:02,031:84287 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:02,023:84290 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,025:84290 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 19:44:02,039:84287 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:02,026:84290 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 19:44:02,046   process-id:84291   (task-6): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 19:44:02,049   process-id:84291   (task-6): running\n",
      "\n",
      "2020-11-02 19:44:02,053:84291 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:02,054:84291 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,054:84288 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:02,055:84291 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg6_2020-11-02_ford.csv', 'FileGrp': 'tg6', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:02,057:84291 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-01_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:02,057:84291 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 19:44:02,057:84291 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-02_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:02,068   process-id:84249 run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,064:84288 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:02,069:84287 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 19:44:02,071:84287 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 19:44:02,058:84291 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,073   process-id:84287   (task-2): done\n",
      "\n",
      "2020-11-02 19:44:02,058:84291 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 19:44:02,075:84288 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 19:44:02,074:84289 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 19:44:02,077:84289 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:02,077:84288 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 19:44:02,080   process-id:84288   (task-3): done\n",
      "\n",
      "2020-11-02 19:44:02,069   process-id:84292   (task-7): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg7_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"d54b283d90621ca6b19c85f4c96d4b8f\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"8d575874cb97b2d601ae8542aaf11431\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-11-02 19:44:02,084   process-id:84292   (task-7): running\n",
      "\n",
      "2020-11-02 19:44:02,071:84291 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 19:44:02,087:84292 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:02,088:84292 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,090:84292 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-03_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"d54b283d90621ca6b19c85f4c96d4b8f\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:02,091:84292 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg8_2020-11-01_ford.csv', 'FileGrp': 'tg8', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"8d575874cb97b2d601ae8542aaf11431\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:02,091:84292 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg9_2020-11-01_ford.csv', 'FileGrp': 'tg9', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-11-02 19:44:02,091:84292 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,091:84292 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-11-02 19:44:02,100:84290 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 19:44:02,093:84292 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-11-02 19:44:02,108:84290 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:02,113:84291 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:02,094:84292 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-11-02 19:44:02,122:84291 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:02,125:84290 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:02,127:84290 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 19:44:02,128   process-id:84290   (task-5): done\n",
      "\n",
      "2020-11-02 19:44:02,131:84291 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 19:44:02,132:84291 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 19:44:02,134   process-id:84291   (task-6): done\n",
      "\n",
      "2020-11-02 19:44:02,149:84292 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:02,151:84289 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:02,153:84289 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 19:44:02,155   process-id:84289   (task-4): done\n",
      "\n",
      "2020-11-02 19:44:02,173:84292 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:02,248:84292 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 19:44:02,250:84292 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 19:44:02,251   process-id:84292   (task-7): done\n",
      "\n",
      "2020-11-02 19:44:02,253   process-id:84249 run_blocking_tasks: exiting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_delta = extract_src_detail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_target_splitter(schema = ''):\n",
    "    tokens = schema.split(',')\n",
    "    key_cols = []\n",
    "    for t in tokens:\n",
    "        t = t.strip()\n",
    "        if re.match(TARGET_REGEX, t):\n",
    "            target_col = t\n",
    "        elif re.match(KEY_REGEX, t):\n",
    "            key_cols.append(t)\n",
    "        else:\n",
    "            raise Exception(\"Not a valid schema\")\n",
    "    return [{'target': target_col, 'key_cols' : key_cols}]\n",
    "\n",
    "\n",
    "@decorator.box_logged\n",
    "def log_report(list_of_row_dict=[], columns:list=[], header_align = 'left', sort_by= None, ascending = True, report_title='', ):\n",
    "    pd.set_option(\"display.colheader_justify\", header_align)\n",
    "    df = pd.DataFrame(list_of_row_dict, columns=columns) \n",
    "    if sort_by is not None:\n",
    "        df = df.sort_values(by=sort_by, ascending=ascending)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=['index'])\n",
    "    #df = df.set_index(' **      ' + df.index.astype(str) )\n",
    "    df = df.rename(' **      {}'.format)\n",
    "#     df1 = df.reindex(columns=['Taxonomy_Grp','File','Date', 'Schema'])\n",
    "    #df[df.columns[new_order]]\n",
    "    #df = df.transpose()\n",
    "    if report_title != '': \n",
    "        report_titled(report_title)\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"\")\n",
    "    logging.info(str(df))\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "@decorator.box_titled\n",
    "def report_titled(title:str=''):\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"    \"+title)\n",
    "    logging.info(\"\")\n",
    "    \n",
    "    \n",
    "# class Taxonomy_Grp:\n",
    "    \n",
    "#     def __init__(self, tg_name, key_cols=[], target_col='', data_location=''):\n",
    "#         self.tg_name = tg_name\n",
    "#         self.key_cols = key_cols\n",
    "#         self.target_col = target_col\n",
    "#         self.location =os.path.join(data_location, tg_name)\n",
    "        \n",
    "#     def get_dict(self):\n",
    "#         if self.target_col == '':\n",
    "#             return {'tg_name': self.tg_name}\n",
    "        \n",
    "#         return {'tg_name': self.tg_name, \n",
    "#                 'key_cols': self.key_cols, \n",
    "#                 'target_col': self.target_col, \n",
    "#                 'location': self.location}\n",
    "\n",
    "#     def __str__(self):\n",
    "#         if self.target_col == '':\n",
    "#             return 'tg_name: {}'.format(self.tg_name)\n",
    "#         return 'tg_name: {}, key_cols: {}, target_col: {}, location: {}'.format(self.tg_name, self.key_cols, self.target_col,self.location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 19:44:02,288:84249 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:02,289:84249 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,290:84249 ThreadPoolExecutor-1_0 (task-0): passed args :['taxonomy_cs/test1/data/tg3/tg3_2020-11-01_ford.csv']\n",
      "\n",
      "2020-11-02 19:44:02,290:84249 ThreadPoolExecutor-1_1 (task-1): passed args :['taxonomy_cs/test1/data/tg3/tg3_2020-11-02_ford.csv']\n",
      "\n",
      "2020-11-02 19:44:02,291:84249 ThreadPoolExecutor-1_2 (task-2): passed args :['taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv']\n",
      "\n",
      "2020-11-02 19:44:02,292:84249 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg3/tg3_2020-11-01_ford.csv \n",
      "2020-11-02 19:44:02,292:84249 ThreadPoolExecutor-1_3 (task-3): passed args :['taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv']\n",
      "\n",
      "2020-11-02 19:44:02,292:84249 ThreadPoolExecutor-1_4 (task-4): passed args :['taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv']\n",
      "\n",
      "2020-11-02 19:44:02,294:84249 ThreadPoolExecutor-1_5 (task-5): passed args :['taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv']\n",
      "\n",
      "2020-11-02 19:44:02,294:84249 ThreadPoolExecutor-1_6 (task-6): passed args :['taxonomy_cs/test1/data/tg5/tg5_2020-11-05_ford.csv']\n",
      "\n",
      "2020-11-02 19:44:02,294:84249 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,295:84249 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg3/tg3_2020-11-02_ford.csv \n",
      "2020-11-02 19:44:02,296:84249 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv \n",
      "2020-11-02 19:44:02,297:84249 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:02,298:84249 ThreadPoolExecutor-1_3 (task-3): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv \n",
      "2020-11-02 19:44:02,299:84249 ThreadPoolExecutor-1_4 (task-4): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv \n",
      "2020-11-02 19:44:02,300:84249 ThreadPoolExecutor-1_5 (task-5): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv \n",
      "2020-11-02 19:44:02,301:84249 ThreadPoolExecutor-1_6 (task-6): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-05_ford.csv \n",
      "2020-11-02 19:44:02,305:84249 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:02,306:84249 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-11-02 19:44:02,309:84249 ThreadPoolExecutor-1_3 (task-3): done\n",
      "\n",
      "2020-11-02 19:44:02,310:84249 ThreadPoolExecutor-1_4 (task-4): done\n",
      "\n",
      "2020-11-02 19:44:02,310:84249 ThreadPoolExecutor-1_5 (task-5): done\n",
      "\n",
      "2020-11-02 19:44:02,311:84249 ThreadPoolExecutor-1_6 (task-6): done\n",
      "\n",
      "2020-11-02 19:44:02,321:84249 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 19:44:02,323:84249 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-11-02 19:44:02,324:84249 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,325:84249 ThreadPoolExecutor-2_0 (task-0): passed args :['tg6', 'tg6_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 49]\n",
      "\n",
      "2020-11-02 19:44:02,326:84249 ThreadPoolExecutor-2_1 (task-1): passed args :['tg6', 'tg6_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 49]\n",
      "\n",
      "2020-11-02 19:44:02,326:84249 ThreadPoolExecutor-2_0 (task-0): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv\n",
      "2020-11-02 19:44:02,327:84249 ThreadPoolExecutor-2_2 (task-2): passed args :['tg2', 'tg2_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 50]\n",
      "\n",
      "2020-11-02 19:44:02,328:84249 ThreadPoolExecutor-2_3 (task-3): passed args :['tg2', 'tg2_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 50]\n",
      "\n",
      "2020-11-02 19:44:02,328:84249 ThreadPoolExecutor-2_1 (task-1): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv\n",
      "2020-11-02 19:44:02,328:84249 ThreadPoolExecutor-2_4 (task-4): passed args :['tg2', 'tg2_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 50]\n",
      "\n",
      "2020-11-02 19:44:02,329:84249 ThreadPoolExecutor-2_5 (task-5): passed args :['tg0', 'tg0_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 27]\n",
      "\n",
      "2020-11-02 19:44:02,330:84249 ThreadPoolExecutor-2_0 (task-0): done\n",
      "\n",
      "2020-11-02 19:44:02,331:84249 ThreadPoolExecutor-2_6 (task-6): passed args :['tg4', 'tg4_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-02 19:44:02,331:84249 ThreadPoolExecutor-2_2 (task-2): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-03_ford.csv\n",
      "2020-11-02 19:44:02,331:84249 ThreadPoolExecutor-2_7 (task-7): passed args :['tg4', 'tg4_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-02 19:44:02,333:84249 ThreadPoolExecutor-2_8 (task-8): passed args :['tg4', 'tg4_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 48]\n",
      "\n",
      "2020-11-02 19:44:02,334:84249 ThreadPoolExecutor-2_9 (task-9): passed args :['tg1', 'tg1_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-02 19:44:02,334:84249 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-11-02 19:44:02,334:84249 ThreadPoolExecutor-2_3 (task-3): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv\n",
      "2020-11-02 19:44:02,335:84249 ThreadPoolExecutor-2_1 (task-1): done\n",
      "\n",
      "2020-11-02 19:44:02,336:84249 ThreadPoolExecutor-2_4 (task-4): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv\n",
      "2020-11-02 19:44:02,337:84249 ThreadPoolExecutor-2_5 (task-5): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg0/tg0_2020-11-02_ford.csv\n",
      "2020-11-02 19:44:02,338:84249 ThreadPoolExecutor-2_0 (task-10): passed args :['tg1', 'tg1_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-02 19:44:02,339:84249 ThreadPoolExecutor-2_6 (task-6): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-01_ford.csv\n",
      "2020-11-02 19:44:02,340:84249 ThreadPoolExecutor-2_2 (task-2): done\n",
      "\n",
      "2020-11-02 19:44:02,341:84249 ThreadPoolExecutor-2_7 (task-7): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-02_ford.csv\n",
      "2020-11-02 19:44:02,342:84249 ThreadPoolExecutor-2_8 (task-8): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-03_ford.csv\n",
      "2020-11-02 19:44:02,343:84249 ThreadPoolExecutor-2_9 (task-9): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg1/tg1_2020-11-01_ford.csv\n",
      "2020-11-02 19:44:02,344:84249 ThreadPoolExecutor-2_3 (task-3): done\n",
      "\n",
      "2020-11-02 19:44:02,345:84249 ThreadPoolExecutor-2_1 (task-11): passed args :['tg7', 'tg7_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-02 19:44:02,347:84249 ThreadPoolExecutor-2_4 (task-4): done\n",
      "\n",
      "2020-11-02 19:44:02,348:84249 ThreadPoolExecutor-2_5 (task-5): done\n",
      "\n",
      "2020-11-02 19:44:02,348:84249 ThreadPoolExecutor-2_0 (task-10): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg1/tg1_2020-11-02_ford.csv\n",
      "2020-11-02 19:44:02,349:84249 ThreadPoolExecutor-2_6 (task-6): done\n",
      "\n",
      "2020-11-02 19:44:02,350:84249 ThreadPoolExecutor-2_2 (task-12): passed args :['tg7', 'tg7_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 48]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 19:44:02,351:84249 ThreadPoolExecutor-2_7 (task-7): done\n",
      "\n",
      "2020-11-02 19:44:02,352:84249 ThreadPoolExecutor-2_8 (task-8): done\n",
      "\n",
      "2020-11-02 19:44:02,353:84249 ThreadPoolExecutor-2_9 (task-9): done\n",
      "\n",
      "2020-11-02 19:44:02,354:84249 ThreadPoolExecutor-2_3 (task-13): passed args :['tg5', 'tg5_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-11-02 19:44:02,355:84249 ThreadPoolExecutor-2_1 (task-11): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg7/tg7_2020-11-01_ford.csv\n",
      "2020-11-02 19:44:02,356:84249 ThreadPoolExecutor-2_4 (task-14): passed args :['tg5', 'tg5_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-11-02 19:44:02,357:84249 ThreadPoolExecutor-2_5 (task-15): passed args :['tg5', 'tg5_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 48]\n",
      "\n",
      "2020-11-02 19:44:02,358:84249 ThreadPoolExecutor-2_0 (task-10): done\n",
      "\n",
      "2020-11-02 19:44:02,360:84249 ThreadPoolExecutor-2_2 (task-12): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg7/tg7_2020-11-02_ford.csv\n",
      "2020-11-02 19:44:02,374:84249 ThreadPoolExecutor-2_3 (task-13): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-01_ford.csv\n",
      "2020-11-02 19:44:02,375:84249 ThreadPoolExecutor-2_1 (task-11): done\n",
      "\n",
      "2020-11-02 19:44:02,377:84249 ThreadPoolExecutor-2_4 (task-14): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-02_ford.csv\n",
      "2020-11-02 19:44:02,378:84249 ThreadPoolExecutor-2_5 (task-15): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-03_ford.csv\n",
      "2020-11-02 19:44:02,380:84249 ThreadPoolExecutor-2_2 (task-12): done\n",
      "\n",
      "2020-11-02 19:44:02,381:84249 ThreadPoolExecutor-2_3 (task-13): done\n",
      "\n",
      "2020-11-02 19:44:02,383:84249 ThreadPoolExecutor-2_4 (task-14): done\n",
      "\n",
      "2020-11-02 19:44:02,384:84249 ThreadPoolExecutor-2_5 (task-15): done\n",
      "\n",
      "2020-11-02 19:44:02,390:84249 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-11-02 19:44:02,399 INFO libs.xml_writer: \n",
      "<configroot version=\"12.1\">\n",
      "\t<set>\n",
      "\t\t<name>CS_TAXONOMY_LMT_SCHEMA_SET</name>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg6</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a6</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a61</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg6/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg2</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a21</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a21</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg2/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg0</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a0</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a0</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg0/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg4</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a4</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a4</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg4/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg1</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a1</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a1</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg1/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg5</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a5</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a5</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg5/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg7</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a7</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a7</attr>\n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg7/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "    </set>\n",
      "    <set>\n",
      "\t\t<name>DROP_CS_TAXONOMY_LMT_SCHEMA_SET</name>\n",
      "        \n",
      "\t\t <val> tg3</val>\n",
      "        \n",
      "\t\t <val> tg2</val>\n",
      "        \n",
      "\t\t <val> tg6</val>\n",
      "        \n",
      "    </set>\n",
      "    \n",
      "</configroot>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-02 19:44:02,402 INFO libs.xml_writer: test.xml has been generated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Extract Info needed to expose configs and show in logs and reports'''\n",
    "\n",
    "invalid_files_set = src_delta['invalid_files_set']\n",
    "invalid_schema_files = src_delta['invalid_schema_files']\n",
    "\n",
    "tg_data = { k for k in tg_data_files_dict.keys()}\n",
    "tg_existing = { k for k in src_delta['existing_tg_files_dict'].keys()}\n",
    "tg_new ={ k for k in src_delta['new_tg_files_dict'].keys()}\n",
    "tg_all = tg_new.union(tg_existing)\n",
    "\n",
    "many_tg4schema_check_gen = (v for k, v in src_delta['schema_tg_dict'].items() if len(v) > 1)\n",
    "many_tg4target_check_gen = (v for k, v in src_delta['target_tg_dict'].items() if len(v) > 1)\n",
    "newTg4schema = {k for k, v in src_delta['new_tg_schema_dict'].items() if len(v) > 1}\n",
    "\n",
    "\n",
    "tg4schema = set()\n",
    "[tg4schema.update(i) for i in many_tg4schema_check_gen]\n",
    "tg4target = set()\n",
    "[tg4target.update(i) for i in many_tg4target_check_gen]\n",
    "\n",
    "\n",
    "invalid_tg_with_dup_schema = (tg4schema.union(newTg4schema)).difference(tg_existing)\n",
    "\n",
    "invalid_tg_with_dup_target = tg4target.difference(tg_existing)\n",
    "\n",
    "invalid_tg_all = invalid_tg_with_dup_schema.union(invalid_tg_with_dup_target)\n",
    "\n",
    "tg_delta = tg_new.difference(invalid_tg_all)\n",
    "\n",
    "tg_delta_create = tg_delta.difference(tg_data)\n",
    "\n",
    "tg_delta_drop_n_create = (tg_delta.intersection(tg_data)).difference(tg_existing)\n",
    "\n",
    "tg_dropped = tg_data.difference(tg_all)\n",
    "\n",
    "tg_dropped_all = tg_dropped.union(tg_delta_drop_n_create)\n",
    "tg_create_all = tg_delta_create.union(tg_delta_drop_n_create)\n",
    "\n",
    "''' File Sync'''\n",
    "files_to_be_dropped = [[f['Key']] for i in  tg_dropped_all \n",
    "                       for fn, f in tg_data_files_dict.get(i).items()]\n",
    "files_not_retained_existing_tg =[[tg_data_files_dict.get(tg).get(fn)['Key']]\n",
    "                                 for tg in tg_existing \n",
    "                                 for fn in set(tg_data_files_dict.get(tg).keys()).difference(set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "file_drop_args = []\n",
    "file_drop_args.extend(files_to_be_dropped )\n",
    "file_drop_args.extend(files_not_retained_existing_tg )\n",
    "\n",
    "\n",
    "files_to_be_created = [[i, f['FileName'], f['Key'], f['Size']] \n",
    "                       for i in  tg_create_all \n",
    "                       for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "files_to_be_copied = [[k, f_dict['FileName'], f_dict['Key'], f_dict['Size']] \n",
    "                      for k, v in src_delta['existing_tg_files_dict'].items() \n",
    "                      for f, f_dict in v.items()] #['Key']]\n",
    "file_copy_args = []\n",
    "file_copy_args.extend(files_to_be_created )\n",
    "file_copy_args.extend(files_to_be_copied )\n",
    "\n",
    "collected = NIO.decorated_run_io(task=s3_remove_at_data_loc_task, task_n_args_list=file_drop_args, \n",
    "                                 is_kernal_thread=False,)\n",
    "\n",
    "collected = NIO.decorated_run_io(task=s3_copy_into_data_loc_task, task_n_args_list=file_copy_args, \n",
    "                                 is_kernal_thread=False,)\n",
    "\n",
    "\n",
    "\n",
    "''' Expose details to generate configs'''\n",
    "tg_create_all_n_schema = {tg: schema \n",
    "                          for tg in tg_create_all \n",
    "                          for schema in src_delta['new_tg_schema_dict'].get(tg)}\n",
    "tg_retain_all_n_schema = {tg: tg_data_schema_dict.get(tg) \n",
    "                          for tg in tg_existing}\n",
    "tg_dropped_all_n_schema = {tg: tg_data_schema_dict.get(tg) \n",
    "                           for tg in tg_dropped_all }\n",
    "\n",
    "\n",
    "'''New and Drop_n_create(With new attributes like schema) Taxonomy Grps'''\n",
    "exposed_tg_all = [Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target'], lmt_data) \n",
    "                  for tg ,schema in tg_create_all_n_schema.items() \n",
    "                  for schema_dict in key_target_splitter(schema)]\n",
    "'''Retaining Taxonomy Grps with either NO CHANGES or Create and Drop some files in a retained group'''\n",
    "exposed_tg_all.extend([Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target'], lmt_data) \n",
    "                       for tg ,schema in tg_retain_all_n_schema.items() \n",
    "                       for schema_dict in key_target_splitter(schema)])\n",
    "\n",
    "\n",
    "'''Dropped and Drop_n_create(With old attributes like schema) Taxonomy Grps'''\n",
    "exposed_dropped_tg_all = [Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target'], lmt_data) \n",
    "                          for tg ,schema in tg_dropped_all_n_schema.items() \n",
    "                          for schema_dict in key_target_splitter(schema)]\n",
    "\n",
    "\n",
    "''' Generating output config xml'''\n",
    "xml_writer.generate_output_config(exposed_tg_all, exposed_dropped_tg_all, dn_version, config_input_loc, config_file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ''' Report '''\n",
    "# # invalid files Schema delails\n",
    "# # To Be\n",
    "\n",
    "# # TG level\n",
    "# tg_create_n_schema = [(i, src_delta['new_tg_schema_dict'].get(i)) for i in tg_delta_create]\n",
    "# tg_drop_create_n_schema = [(tg, tg_data_schema_dict.get(tg), schema_new) for tg in tg_delta_drop_n_create for schema_new in src_delta['new_tg_schema_dict'].get(tg)]\n",
    "# tg_drop_n_schema = [(i, tg_data_schema_dict.get(i)) for i in tg_dropped]\n",
    "# tg_retain_n_schema = [(i, tg_data_schema_dict.get(i)) for i in tg_existing]\n",
    "\n",
    "# # File Level\n",
    "# files_to_be_dropped = [ f['Key'] for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items()]\n",
    "# files_to_be_dropped_schema_change = [ f['Key'] for i in  tg_delta_drop_n_create for fn, f in tg_data_files_dict.get(i).items()]\n",
    "# files_to_be_created = {f['Key'] :f['Schema'] for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()}\n",
    "# files_to_be_created_schema_change = {f['Key'] :f['Schema'] for i in  tg_delta_drop_n_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           tg_name key_cols   target_col  location                                    \n",
      " **      0  tg0      [key_a0]   target_a0  s3://qubole-ford/taxonomy_cs/test1/data/tg0\n",
      " **      1  tg1      [key_a1]   target_a1  s3://qubole-ford/taxonomy_cs/test1/data/tg1\n",
      " **      2  tg2     [key_a21]  target_a21  s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      3  tg4      [key_a4]   target_a4  s3://qubole-ford/taxonomy_cs/test1/data/tg4\n",
      " **      4  tg5      [key_a5]   target_a5  s3://qubole-ford/taxonomy_cs/test1/data/tg5\n",
      " **      5  tg6      [key_a6]  target_a61  s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **      6  tg7      [key_a7]   target_a7  s3://qubole-ford/taxonomy_cs/test1/data/tg7\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           tg_name key_cols  target_col location                                    \n",
      " **      0  tg2     [key_a2]  target_a2  s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      1  tg3     [key_a3]  target_a3  s3://qubole-ford/taxonomy_cs/test1/data/tg3\n",
      " **      2  tg6     [key_a6]  target_a6  s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                 Date        Schema           \n",
      " **      0  tg11         tg11_2020-11-01_ford.csv  2020-11-01  key_a9,target_a9\n",
      " **      1   tg9          tg9_2020-11-01_ford.csv  2020-11-01  key_a9,target_a9\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                 Date        Schema            \n",
      " **      0  tg11         tg11_2020-11-01_ford.csv  2020-11-01   key_a9,target_a9\n",
      " **      1   tg9          tg9_2020-11-01_ford.csv  2020-11-01   key_a9,target_a9\n",
      " **      2   tg8          tg8_2020-11-01_ford.csv  2020-11-01   key_a8,target_a7\n",
      " **      3  tg10         tg10_2020-11-01_ford.csv  2020-11-01  key_a10,target_a9\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema              Grp_Schema       \n",
      " **      0  tg5          tg5_2020-11-04_ford.csv  2020-11-04  key_a51,target_a51  key_a5,target_a5\n",
      " **      1  tg7          tg7_2020-11-03_ford.csv  2020-11-03   key_a7,target_a71  key_a7,target_a7\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name                                                     \n",
      " **      0  s3://qubole-ford/taxonomy_cs/test1/src/tg0_202-11-01_ford.csv\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name                                                       Schema            Reason                                                                         \n",
      " **      0  s3://qubole-ford/taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv  key_a0, targe_a0  Exact one Target column is required! \\nAll given columns should Key or Target!\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema           \n",
      " **      0  tg3          tg3_2020-11-01_ford.csv  2020-11-01  key_a3,target_a3\n",
      " **      1  tg3          tg3_2020-11-02_ford.csv  2020-11-02  key_a3,target_a3\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema         \n",
      " **      0  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21\n",
      " **      2  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61\n",
      " **      3  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema           \n",
      " **      0  tg0          tg0_2020-11-02_ford.csv  2020-11-02  key_a0,target_a0\n",
      " **      1  tg4          tg4_2020-11-01_ford.csv  2020-11-01  key_a4,target_a4\n",
      " **      2  tg4          tg4_2020-11-02_ford.csv  2020-11-02  key_a4,target_a4\n",
      " **      3  tg4          tg4_2020-11-03_ford.csv  2020-11-03  key_a4,target_a4\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema          Desc         \n",
      " **      0  tg2  tg2_2020-11-03_ford.csv  2020-11-03               NAN  key_a21,target_a21      New File\n",
      " **      1  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      2  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      3  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **      4  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema            Desc     \n",
      " **      0  tg1          tg1_2020-11-01_ford.csv  2020-11-01  key_a1,target_a1  Retained\n",
      " **      1  tg1          tg1_2020-11-02_ford.csv  2020-11-02  key_a1,target_a1  New File\n",
      " **      2  tg5          tg5_2020-11-01_ford.csv  2020-11-01  key_a5,target_a5  Retained\n",
      " **      3  tg5          tg5_2020-11-02_ford.csv  2020-11-02  key_a5,target_a5  Retained\n",
      " **      4  tg5          tg5_2020-11-03_ford.csv  2020-11-03  key_a5,target_a5  New File\n",
      " **      5  tg5          tg5_2020-11-05_ford.csv  2020-11-05  key_a5,target_a5   Dropped\n",
      " **      6  tg7          tg7_2020-11-01_ford.csv  2020-11-01  key_a7,target_a7  Retained\n",
      " **      7  tg7          tg7_2020-11-02_ford.csv  2020-11-02  key_a7,target_a7  Retained\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ''' Report '''\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "'''  Exposed TG Report  '''\n",
    "\n",
    "exposed_tg_report_data = [i.get_dict() for i in exposed_tg_all]\n",
    "log_report(exposed_tg_report_data,  columns=['tg_name', 'key_cols','target_col', 'location'], sort_by='tg_name')\n",
    "\n",
    "\n",
    "'''  Exposed Dropped TG Report '''\n",
    "\n",
    "exposed_tg_dropped_report_data = [i.get_dict() for i in exposed_dropped_tg_all]\n",
    "log_report(exposed_tg_dropped_report_data, columns=['tg_name', 'key_cols','target_col', 'location'], sort_by='tg_name')\n",
    "\n",
    "\n",
    "'''  Invalid TG due to schema conflict/already used Report '''\n",
    "\n",
    "invalid_tg_with_dup_schema_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                    for i in  invalid_tg_with_dup_schema for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(invalid_tg_with_dup_schema_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''  Invalid TG due to Target Column conflict/already used Report '''\n",
    "\n",
    "invalid_tg_with_dup_target_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                    for i in  invalid_tg_with_dup_target for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(invalid_tg_with_dup_target_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''  Invalid files from retained grp due to schema or target mismatch with previously delivered files for same'''\n",
    "\n",
    "partially_invalid_tg_set = tg_new.intersection(tg_existing)\n",
    "partially_invalid_tg_report = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Grp_Schema': tg_data_schema_dict[i] } \n",
    "                               for i in partially_invalid_tg_set  \n",
    "                               for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(partially_invalid_tg_report, columns=['Taxonomy_Grp','File_Name','Date', 'Schema','Grp_Schema']) \n",
    "\n",
    "\n",
    "'''   Invalid file not match with required file pattern '''\n",
    "\n",
    "invalid_files_report_data = [{'File_Name' : i} for i in invalid_files_set]\n",
    "log_report(invalid_files_report_data,  columns=['File_Name'])\n",
    "\n",
    "\n",
    "'''   Invalid file not match with required schema pattern '''\n",
    "\n",
    "invalid_schema_files_rep_data = [{'File_Name' : i[0], 'Schema': i[1], 'Reason' : i[2]} for i in invalid_schema_files]\n",
    "log_report(invalid_schema_files_rep_data,  columns=['File_Name', 'Schema','Reason'], header_align='left')\n",
    "\n",
    "\n",
    "'''   Dropped TG Completely '''\n",
    "\n",
    "tg_dropped_rep_gen =(extract_info(f['Key']) for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items())\n",
    "tg_dropped_report_dict =  [{'Taxonomy_Grp':i['FileGrp'], 'File_Name':i['FileName'], 'Date':i['Date'], 'Schema': tg_data_schema_dict[i['FileGrp']] }\n",
    "                           for i in tg_dropped_rep_gen] \n",
    "log_report(list_of_row_dict=tg_dropped_report_dict,columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''   Dropped TG to change schema '''\n",
    "\n",
    "tg_drop_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), schema_new, extract_info(f['Key'])) \n",
    "                             for tg in tg_delta_drop_n_create \n",
    "                             for schema_new in src_delta['new_tg_schema_dict'].get(tg)\n",
    "                             for fn, f in tg_data_files_dict.get(tg).items())\n",
    "tg_drop_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2]} \n",
    "                                     for i in tg_drop_schema_change_rep]\n",
    "log_report(list_of_row_dict=tg_drop_schema_change_report_dict,columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema']) \n",
    "\n",
    "\n",
    "'''   Created TG Absolute New '''\n",
    "\n",
    "tg_newly_created_report_data = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                    for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(tg_newly_created_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''   Created TG to change schema(with new Schema) '''\n",
    "\n",
    "tg_re_created_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), f['Schema'], extract_info(f['Key']), 'Re-delivered') \n",
    "                                   if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                   else (tg, 'NAN', f['Schema'], extract_info(f['Key']), 'New File')\n",
    "                                    \n",
    "                                   for tg in tg_delta_drop_n_create \n",
    "                                   #for schema_new in src_delta['new_tg_schema_dict'].get(tg) \n",
    "                                   \n",
    "                                   for fn, f in src_delta['new_tg_files_dict'].get(tg).items() \n",
    "                                   )\n",
    "\n",
    "tg_recreated_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2], 'Desc': i[4]} \n",
    "                                          for i in tg_re_created_schema_change_rep]\n",
    "\n",
    "log_report(list_of_row_dict=tg_recreated_schema_change_report_dict,columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema', 'Desc'])\n",
    "\n",
    "\n",
    "'''   Retained TG with retained files, new files and dropped files '''\n",
    "\n",
    "tg_retained_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'Retained' }\n",
    "                           \n",
    "                            if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                            else {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'New File' }\n",
    "                            for tg in tg_existing \n",
    "                            for fn, f in src_delta['existing_tg_files_dict'].get(tg).items()]\n",
    "\n",
    "tg_retained_dropped_files = [extract_info(tg_data_files_dict.get(tg).get(fn)['Key']) \n",
    "                             for tg in tg_existing \n",
    "                             for fn in set(tg_data_files_dict.get(tg).keys()).difference(set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "                             \n",
    "tg_retained_dropped_files_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': tg_data_schema_dict[f['FileGrp']], 'Desc' : 'Dropped' }\n",
    "                                          for f in tg_retained_dropped_files]\n",
    "\n",
    "tg_retained_report_data.extend(tg_retained_dropped_files_report_data)\n",
    "log_report(tg_retained_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema', 'Desc'], sort_by = ['Taxonomy_Grp','Date']) \n",
    "\n",
    "#''' End '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gen_Report(exposed_tg_all = None, \n",
    "               exposed_dropped_tg_all = None,\n",
    "               invalid_tg_with_dup_schema = None, \n",
    "               invalid_tg_with_dup_target = None,\n",
    "               src_delta  = None,\n",
    "               tg_new  = None, \n",
    "               tg_existing = None,\n",
    "               invalid_files_set = None,\n",
    "               invalid_schema_files = None,\n",
    "               tg_data_files_dict = None, \n",
    "               tg_data_schema_dict = None, \n",
    "               tg_delta_drop_n_create = None, \n",
    "               tg_delta_create = None):\n",
    "\n",
    "    ''' Report '''\n",
    "    \n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "    '''  Exposed TG Report  '''\n",
    "\n",
    "    exposed_tg_report_data = [i.get_dict() for i in exposed_tg_all]\n",
    "    report_title = 'Exposed TG Report'\n",
    "    log_report(exposed_tg_report_data,  columns=['tg_name', 'key_cols','target_col', 'location'], \n",
    "               sort_by='tg_name', report_title=report_title)\n",
    "\n",
    "\n",
    "    '''  Exposed Dropped TG Report '''\n",
    "\n",
    "    exposed_tg_dropped_report_data = [i.get_dict() for i in exposed_dropped_tg_all]\n",
    "    report_title = 'Exposed Dropped TG Report'\n",
    "    log_report(exposed_tg_dropped_report_data, columns=['tg_name', 'key_cols','target_col', 'location'], \n",
    "               sort_by='tg_name', report_title=report_title)\n",
    "\n",
    "\n",
    "    \n",
    "    '''   Dropped TG Completely '''\n",
    "\n",
    "    tg_dropped_rep_gen =(extract_info(f['Key']) for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items())\n",
    "    tg_dropped_report_dict =  [{'Taxonomy_Grp':i['FileGrp'], 'File_Name':i['FileName'], 'Date':i['Date'], 'Schema': tg_data_schema_dict[i['FileGrp']] }\n",
    "                               for i in tg_dropped_rep_gen] \n",
    "    report_title = 'Dropped TG Completely'\n",
    "    log_report(list_of_row_dict=tg_dropped_report_dict, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Dropped TG to change schema '''\n",
    "\n",
    "    tg_drop_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), schema_new, extract_info(f['Key'])) \n",
    "                                 for tg in tg_delta_drop_n_create \n",
    "                                 for schema_new in src_delta['new_tg_schema_dict'].get(tg)\n",
    "                                 for fn, f in tg_data_files_dict.get(tg).items())\n",
    "    tg_drop_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2]} \n",
    "                                         for i in tg_drop_schema_change_rep]\n",
    "    report_title = 'Dropped TG to change schema'\n",
    "    log_report(list_of_row_dict=tg_drop_schema_change_report_dict, \n",
    "               columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema'], \n",
    "               sort_by = ['Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Created TG Absolute New '''\n",
    "\n",
    "    tg_newly_created_report_data = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                        for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Created TG Absolute New'\n",
    "    log_report(tg_newly_created_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Created TG to change schema(with new Schema) '''\n",
    "\n",
    "    tg_re_created_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), f['Schema'], extract_info(f['Key']), 'Re-delivered') \n",
    "                                       if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                       else (tg, 'NAN', f['Schema'], extract_info(f['Key']), 'New File')\n",
    "\n",
    "                                       for tg in tg_delta_drop_n_create \n",
    "                                       #for schema_new in src_delta['new_tg_schema_dict'].get(tg) \n",
    "\n",
    "                                       for fn, f in src_delta['new_tg_files_dict'].get(tg).items() \n",
    "                                       )\n",
    "\n",
    "    tg_recreated_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2], 'Desc': i[4]} \n",
    "                                              for i in tg_re_created_schema_change_rep]\n",
    "    report_title = 'Created TG to change schema(with new Schema)'\n",
    "    log_report(list_of_row_dict=tg_recreated_schema_change_report_dict, \n",
    "               columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema', 'Desc'], \n",
    "               sort_by = ['Grp','Date'], report_title=report_title)\n",
    "\n",
    "\n",
    "    '''   Retained TG with retained files, new files and dropped files '''\n",
    "\n",
    "    tg_retained_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'Retained' }\n",
    "\n",
    "                                if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                else {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'New File' }\n",
    "                                for tg in tg_existing \n",
    "                                for fn, f in src_delta['existing_tg_files_dict'].get(tg).items()]\n",
    "\n",
    "    tg_retained_dropped_files = [extract_info(tg_data_files_dict.get(tg).get(fn)['Key']) \n",
    "                                 for tg in tg_existing \n",
    "                                 for fn in set(tg_data_files_dict.get(tg).keys()).difference(\n",
    "                                     set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "\n",
    "    tg_retained_dropped_files_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': tg_data_schema_dict[f['FileGrp']], 'Desc' : 'Dropped' }\n",
    "                                              for f in tg_retained_dropped_files]\n",
    "\n",
    "    tg_retained_report_data.extend(tg_retained_dropped_files_report_data)\n",
    "    report_title = 'Retained TG with retained files, new files and dropped files'\n",
    "    log_report(tg_retained_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema', 'Desc'], \n",
    "               header_align='left', sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "    \n",
    "    '''  Invalid TG due to schema conflict/already used Report '''\n",
    "\n",
    "    invalid_tg_with_dup_schema_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                                      for i in  invalid_tg_with_dup_schema \n",
    "                                      for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Invalid TG due to schema conflict/already used Report'\n",
    "    log_report(invalid_tg_with_dup_schema_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''  Invalid TG due to Target Column conflict/already used Report '''\n",
    "\n",
    "    invalid_tg_with_dup_target_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                        for i in  invalid_tg_with_dup_target for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Invalid TG due to Target Column conflict/already used Report'\n",
    "    log_report(invalid_tg_with_dup_target_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    ''' Invalid files from retained grp due to schema or target mismatch with previously delivered files for same'''\n",
    "\n",
    "    partially_invalid_tg_set = tg_new.intersection(tg_existing)\n",
    "    partially_invalid_tg_report = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Grp_Schema': tg_data_schema_dict[i] } \n",
    "                                   for i in partially_invalid_tg_set  \n",
    "                                   for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Invalid files from retained grp due to schema or target mismatch'\n",
    "    log_report(partially_invalid_tg_report, columns=['Taxonomy_Grp','File_Name','Date', 'Schema','Grp_Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Invalid file not match with required file pattern '''\n",
    "\n",
    "    invalid_files_report_data = [{'File_Name' : filename_by_key(i)} for i in invalid_files_set]\n",
    "    report_title = 'Invalid file not match with required file pattern'\n",
    "    log_report(invalid_files_report_data,  columns=['File_Name'], \n",
    "               sort_by = ['File_Name'], report_title=report_title)\n",
    "\n",
    "\n",
    "    '''   Invalid file not match with required schema pattern '''\n",
    "\n",
    "    invalid_schema_files_rep_data = [{'File_Name' : filename_by_key(i[0]), 'Schema': i[1], 'Reason' : i[2]} for i in invalid_schema_files]\n",
    "    report_title = 'Invalid file not match with required schema pattern'\n",
    "    log_report(invalid_schema_files_rep_data,  columns=['File_Name', 'Schema','Reason'], \n",
    "               header_align='left', sort_by = ['File_Name'], report_title=report_title)\n",
    "\n",
    "\n",
    "    #''' End '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Exposed TG Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           tg_name key_cols   target_col  location                                    \n",
      " **      0  tg0      [key_a0]   target_a0  s3://qubole-ford/taxonomy_cs/test1/data/tg0\n",
      " **      1  tg1      [key_a1]   target_a1  s3://qubole-ford/taxonomy_cs/test1/data/tg1\n",
      " **      2  tg2     [key_a21]  target_a21  s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      3  tg4      [key_a4]   target_a4  s3://qubole-ford/taxonomy_cs/test1/data/tg4\n",
      " **      4  tg5      [key_a5]   target_a5  s3://qubole-ford/taxonomy_cs/test1/data/tg5\n",
      " **      5  tg6      [key_a6]  target_a61  s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **      6  tg7      [key_a7]   target_a7  s3://qubole-ford/taxonomy_cs/test1/data/tg7\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Exposed Dropped TG Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           tg_name key_cols  target_col location                                    \n",
      " **      0  tg2     [key_a2]  target_a2  s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      1  tg3     [key_a3]  target_a3  s3://qubole-ford/taxonomy_cs/test1/data/tg3\n",
      " **      2  tg6     [key_a6]  target_a6  s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Dropped TG Completely\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema           \n",
      " **      0  tg3          tg3_2020-11-01_ford.csv  2020-11-01  key_a3,target_a3\n",
      " **      1  tg3          tg3_2020-11-02_ford.csv  2020-11-02  key_a3,target_a3\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Dropped TG to change schema\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema         \n",
      " **      0  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21\n",
      " **      2  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61\n",
      " **      3  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Created TG Absolute New\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema           \n",
      " **      0  tg0          tg0_2020-11-02_ford.csv  2020-11-02  key_a0,target_a0\n",
      " **      1  tg4          tg4_2020-11-01_ford.csv  2020-11-01  key_a4,target_a4\n",
      " **      2  tg4          tg4_2020-11-02_ford.csv  2020-11-02  key_a4,target_a4\n",
      " **      3  tg4          tg4_2020-11-03_ford.csv  2020-11-03  key_a4,target_a4\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Created TG to change schema(with new Schema)\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema          Desc         \n",
      " **      0  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      2  tg2  tg2_2020-11-03_ford.csv  2020-11-03               NAN  key_a21,target_a21      New File\n",
      " **      3  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **      4  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Retained TG with retained files, new files and dropped files\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema            Desc     \n",
      " **      0  tg1          tg1_2020-11-01_ford.csv  2020-11-01  key_a1,target_a1  Retained\n",
      " **      1  tg1          tg1_2020-11-02_ford.csv  2020-11-02  key_a1,target_a1  New File\n",
      " **      2  tg5          tg5_2020-11-01_ford.csv  2020-11-01  key_a5,target_a5  Retained\n",
      " **      3  tg5          tg5_2020-11-02_ford.csv  2020-11-02  key_a5,target_a5  Retained\n",
      " **      4  tg5          tg5_2020-11-03_ford.csv  2020-11-03  key_a5,target_a5  New File\n",
      " **      5  tg5          tg5_2020-11-05_ford.csv  2020-11-05  key_a5,target_a5   Dropped\n",
      " **      6  tg7          tg7_2020-11-01_ford.csv  2020-11-01  key_a7,target_a7  Retained\n",
      " **      7  tg7          tg7_2020-11-02_ford.csv  2020-11-02  key_a7,target_a7  Retained\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid TG due to schema conflict/already used Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                 Date        Schema           \n",
      " **      0  tg11         tg11_2020-11-01_ford.csv  2020-11-01  key_a9,target_a9\n",
      " **      1   tg9          tg9_2020-11-01_ford.csv  2020-11-01  key_a9,target_a9\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid TG due to Target Column conflict/already used Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                 Date        Schema            \n",
      " **      0  tg10         tg10_2020-11-01_ford.csv  2020-11-01  key_a10,target_a9\n",
      " **      1  tg11         tg11_2020-11-01_ford.csv  2020-11-01   key_a9,target_a9\n",
      " **      2   tg8          tg8_2020-11-01_ford.csv  2020-11-01   key_a8,target_a7\n",
      " **      3   tg9          tg9_2020-11-01_ford.csv  2020-11-01   key_a9,target_a9\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid files from retained grp due to schema or target mismatch\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema              Grp_Schema       \n",
      " **      0  tg5          tg5_2020-11-04_ford.csv  2020-11-04  key_a51,target_a51  key_a5,target_a5\n",
      " **      1  tg7          tg7_2020-11-03_ford.csv  2020-11-03   key_a7,target_a71  key_a7,target_a7\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid file not match with required file pattern\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name              \n",
      " **      0  tg0_202-11-01_ford.csv\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid file not match with required schema pattern\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name                Schema            Reason                                                                         \n",
      " **      0  tg0_2020-11-03_ford.csv  key_a0, targe_a0  Exact one Target column is required! \\nAll given columns should Key or Target!\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Gen_Report(exposed_tg_all = exposed_tg_all, \n",
    "               exposed_dropped_tg_all = exposed_dropped_tg_all,\n",
    "               invalid_tg_with_dup_schema = invalid_tg_with_dup_schema, \n",
    "               invalid_tg_with_dup_target = invalid_tg_with_dup_target,\n",
    "               src_delta  = src_delta,\n",
    "               tg_new  = tg_new, \n",
    "               tg_existing = tg_existing,\n",
    "               invalid_files_set = invalid_files_set,\n",
    "               invalid_schema_files = invalid_schema_files,\n",
    "               tg_data_files_dict = tg_data_files_dict, \n",
    "               tg_data_schema_dict = tg_data_schema_dict, \n",
    "               tg_delta_drop_n_create = tg_delta_drop_n_create, \n",
    "               tg_delta_create = tg_delta_create\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_util",
   "language": "python",
   "name": "aws_util"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
