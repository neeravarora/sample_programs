{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, inspect, re\n",
    "import functools\n",
    "sys.path.append(\"/home/vbhargava/feature_test0/msaction_backend/common/BU3.0_core/util/Py_utils/taxonomy_utils\")\n",
    "import time, logging\n",
    "import pandas as pd \n",
    "numeric_level = getattr(logging, 'INFO', None)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "logging.basicConfig(level=numeric_level,\n",
    "                        format='%(asctime)s %(levelname)s %(name)s: %(message)s',\n",
    "                        handlers=[stdout_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.s3_ops import S3_OPs\n",
    "from libs.s3_stream import S3Stream\n",
    "from libs.configs import Config\n",
    "from libs.nio_executor import NIO\n",
    "from libs import utils\n",
    "from libs import xml_writer \n",
    "from libs import decorator\n",
    "from collections import defaultdict\n",
    "#from model.models import Taxonomy_Grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Taxonomy_Grp:\n",
    "    \n",
    "    def __init__(self, tg_name, key_cols=[], target_cols=[], data_location=''):\n",
    "        self.tg_name = tg_name\n",
    "        self.key_cols = key_cols\n",
    "        self.target_cols = target_cols\n",
    "        self.location =os.path.join(data_location, tg_name)\n",
    "        \n",
    "    def get_dict(self):\n",
    "        if len(self.target_cols) == 0:\n",
    "            return {'tg_name': self.tg_name}\n",
    "        \n",
    "        return {'tg_name': self.tg_name, \n",
    "                'key_cols': self.key_cols, \n",
    "                'target_cols': self.target_cols, \n",
    "                'location': self.location}\n",
    "\n",
    "    def __str__(self):\n",
    "        if len(self.target_cols) == 0:\n",
    "            return 'tg_name: {}'.format(self.tg_name)\n",
    "        return 'tg_name: {}, key_cols: {}, target_cols: {}, location: {}'.format(self.tg_name, self.key_cols, self.target_cols,self.location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '/home/vbhargava/feature_test0/msaction_backend/customers/raj_ford_test/common/config/inputs/platform_config.xml'\n",
    "lmt_src = 's3://qubole-ford/taxonomy_cs/test1/src/'\n",
    "lmt_data = 's3://qubole-ford/taxonomy_cs/test1/data/'\n",
    "config_input_loc = '/home/vbhargava/feature_test0/temp/taxo_config_xmls/'\n",
    "config_file_name = 'test.xml'\n",
    "dn_version = '12.1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = Config.get_qubole_config(config)\n",
    "ACCESS_KEY=config_data['access_key']\n",
    "SECRET_KEY=config_data['secret_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TG_EXTRACT_REGEX = '^.*?/([a-zA-Z]+\\-?[0-9]*)/$' \n",
    "FILE_EXTRACT_REGEX = '^.*/([a-zA-Z0-9.\\-_]{0,255}.csv)$' #'^.*/([a-zA-Z0-9.\\-_]{0,255}.csv)$'\n",
    "TARGET_EXTRACT_REGEX ='^.*,?(target_[A-Za-z0-9_-]+).*$'\n",
    "TARGET_EXTRACT_2_REGEX ='(target_[A-Za-z0-9_-]+)'\n",
    "VALID_FILE_KEY_REGEX = '^(.*/([a-zA-Z]+\\-?[0-9]*)?/)?(([a-zA-Z]+\\-?[0-9]*?)_([0-9]{4}-[0-9]{2}-[0-9]{2}?)_([a-zA-Z0-9.\\-_]+?).csv?)$'\n",
    "KEY_REGEX = '^[Kk]ey_[A-Za-z0-9_]{2,30}$'\n",
    "TARGET_REGEX = '^[Tt]arget_[A-Za-z0-9_]{2,30}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_ops = S3_OPs(ACCESS_KEY, SECRET_KEY)\n",
    "\n",
    "def filename_by_key(key):\n",
    "    return get_val_by_regex(key, FILE_EXTRACT_REGEX, error_msg=\"Not vaild key for taxonomy data csv file\")\n",
    "\n",
    "def find_by_data_tg(key, regex):\n",
    "    return get_val_by_regex(key, regex, error_msg=\"Not vaild taxonomy data dir\")\n",
    "\n",
    "        \n",
    "def get_val_by_regex(key, regex, error_msg=\"can't be extract a val.\"):\n",
    "    matched = re.findall(regex, key)\n",
    "    if len(matched) > 0:\n",
    "        return matched[0]\n",
    "    else:\n",
    "        raise Exception(error_msg)\n",
    "        \n",
    "def get_data_n_schema(tg, data_files_loc):\n",
    "    data_file_lock_detail = s3_ops.get_bucket_name(data_files_loc)\n",
    "    files = s3_ops.list_complete(data_file_lock_detail['bucket'], data_file_lock_detail['key'])\n",
    "    res = {}\n",
    "    if len(files)>0:\n",
    "        s3_stream = S3Stream(ACCESS_KEY, SECRET_KEY)\n",
    "        schema = s3_stream.get_header(s3_ops.get_full_s3_path(data_file_lock_detail['bucket'],files[0]['Key']))\n",
    "        #res[tg]={'schema':schema, 'files': files}\n",
    "        res['schema'] = {tg:schema}\n",
    "        res['files'] = {tg:files}\n",
    "    return res\n",
    "\n",
    "def extract_schema(schema):\n",
    "    return schema.replace(\" \",\"\").lower()\n",
    "\n",
    "def validate_schema(schema):\n",
    "    if schema=='': \n",
    "        return {'IsValid' : False, 'schema': schema, 'message' : \"Schema shouldn't be empty\"}\n",
    "    tokens = schema.split(',')\n",
    "    if len(tokens) < 2:\n",
    "         return {'IsValid' : False, 'schema': schema, 'message' : \"Schema should have at least 2 columns\"}\n",
    "    \n",
    "    key_cnt = 0\n",
    "    target_cnt = 0\n",
    "    invalid_headers = []\n",
    "    columns = defaultdict(list)\n",
    "    res = {}\n",
    "    target_cols_set = set()\n",
    "    key_cols_set = set()\n",
    "    for t in tokens:\n",
    "        t = t.strip()\n",
    "        if re.match(TARGET_REGEX, t):\n",
    "            target_cnt = target_cnt + 1\n",
    "            target_cols_set.add(t)\n",
    "        elif re.match(KEY_REGEX, t):\n",
    "            key_cnt = key_cnt + 1\n",
    "            key_cols_set.add(t)\n",
    "        else:\n",
    "            invalid_headers.append(t)\n",
    "        columns[t.lower()].append(1)\n",
    "\n",
    "    error_msgs=[]\n",
    "    if target_cnt < 1 :\n",
    "        error_msgs.append(\"At least one Target column is required!\")\n",
    "    if key_cnt < 1 :\n",
    "        error_msgs.append(\"At least one Key column is required!\")\n",
    "    if len(invalid_headers) > 0 :\n",
    "        error_msgs.append(\"All given columns should Key or Target!\")\n",
    "    for k, v in columns.items():\n",
    "\n",
    "        if len(v) > 1:\n",
    "            print(\"--\")\n",
    "            error_msgs.append(\"Same name: {} should not represent more than one column in schema! cols names are case insensitive. \".format(k))\n",
    "\n",
    "    if len(error_msgs) > 0:\n",
    "        return {'IsValid' : False, 'schema': schema, 'errors' : \" \\n\".join(error_msgs)}\n",
    "    #print(str(key_cnt)+\":\"+str(target_cnt)+\":\"+str(invalid_headers)+\":\"+str(columns))\n",
    "    return {'IsValid' : True, 'Schema': schema.replace(\" \",\"\").lower(), \n",
    "            'TargetColsSet' : target_cols_set, 'KeyColsSet' : key_cols_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator.elapsed_time(func_name='extract_data_detail')\n",
    "def extract_data_detail(lmt_src, lmt_data, access_key, secret_key):\n",
    "#     Valid data Taxonomy Grps\n",
    "    \n",
    "    #\n",
    "    lmt_data_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "    lmt_data_loc_bucket = lmt_data_loc_detail['bucket']\n",
    "    lmt_data_loc_key = lmt_data_loc_detail['key']\n",
    "    valid_tg_list_res = s3_ops.list_subdirs(lmt_data_loc_detail['bucket'],lmt_data_loc_detail['key'],)\n",
    "    \n",
    "    valid_tgrp_loc_list = [ [find_by_data_tg(item['Prefix'], TG_EXTRACT_REGEX), \n",
    "                         '{}{}/'.format(lmt_data, find_by_data_tg(item['Prefix'], TG_EXTRACT_REGEX))] \n",
    "                       for item in valid_tg_list_res]\n",
    "    \n",
    "    collected = NIO.decorated_run_io(task=get_data_n_schema, task_n_args_list=valid_tgrp_loc_list, max_workers=25,)\n",
    "#     return collected\n",
    "    tg_data_schema_dict = {k:extract_schema(v)  for item in collected for k, v in item['result']['schema'].items()}\n",
    "    tg_data_files_dict = {k:{filename_by_key(u['Key']):u for u in v } for item in collected for k, v in item['result']['files'].items()}\n",
    "    #target_data_tg_dict = {re.findall(TARGET_EXTRACT_REGEX,V)[0]: K for K, V in tg_data_schema_dict.items()}\n",
    "    target_data_tg_dict = {target : K for K, V in tg_data_schema_dict.items() \n",
    "                           for target in re.findall(TARGET_EXTRACT_2_REGEX,V)}\n",
    "    \n",
    "    return tg_data_schema_dict, tg_data_files_dict,target_data_tg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_file(key:str='', regex = VALID_FILE_KEY_REGEX):\n",
    "    if re.match(regex, key) is None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_info(key:str='', regex = VALID_FILE_KEY_REGEX):\n",
    "    matched = re.findall(regex, key)\n",
    "    return {\n",
    "            'KeyDirPath' : matched[0][0],\n",
    "            'ParentDir' : matched[0][1],\n",
    "            'FileName' : matched[0][2],\n",
    "            'FileGrp' :  matched[0][3],\n",
    "            'Date' :  matched[0][4],\n",
    "            'ClientName' : matched[0][5]\n",
    "           }\n",
    "def extract_info_with_bucket(key:str='', bucket = ''):\n",
    "    res = extract_info(key)\n",
    "    res.update({'Bucket' : bucket})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_tg(collected, tg_files_dict_type='new_tg_files_dict'):\n",
    "    collect = defaultdict(dict)\n",
    "    tg_f_gen = (item['result'][tg_files_dict_type] for item in collected if len(item['result'][tg_files_dict_type]) > 0)\n",
    "    tg_f_gen2 = (collect[tg].update({filename: file_dict})  for item in tg_f_gen for tg, file_detail_dict in item.items() for filename, file_dict in file_detail_dict.items())\n",
    "    [ i for i in tg_f_gen2]\n",
    "    tg = dict(collect)\n",
    "    return tg\n",
    "\n",
    "def grouped_flag_dict(collected, flag_dict_type='schema_tg_dict'):\n",
    "    f_gen = (item['result'][flag_dict_type] for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    collect = defaultdict(set)\n",
    "    f_gen2 = (collect[K].add(V)  for item in f_gen for K, V in item.items())\n",
    "    [ i for i in f_gen2]\n",
    "    res = dict(collect)\n",
    "    return res\n",
    "\n",
    "def grouped_set_of_flags_dict(collected, flag_dict_type='schema_tg_dict'):\n",
    "    f_gen = (item['result'][flag_dict_type] for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    collect = defaultdict(set)\n",
    "    f_gen2 = (collect[K].update(V)  for item in f_gen for K, V in item.items())\n",
    "    [ i for i in f_gen2]\n",
    "    res = dict(collect)\n",
    "    return res\n",
    "\n",
    "def grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'):\n",
    "    res_set=set()\n",
    "    f_gen = (res_set.update(item['result'][flag_dict_type]) for item in collected if len(item['result'][flag_dict_type]) > 0)\n",
    "    [ i for i in f_gen]\n",
    "    return res_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def file_process_task(src_file_details):\n",
    "    \n",
    "    invalid_schema_files = set()\n",
    "\n",
    "    target_already_exist_files = set()\n",
    "    \n",
    "    ''' {'key_evt_advertiser_key,target_evt_advertiser_name': {'tg1', 'tg2', ...}}'''\n",
    "    schema_tg_dict = {}\n",
    "    \n",
    "    ''' {'target_evt_advertiser_name': {'tg1', 'tg2', ...}}'''\n",
    "    target_tg_dict = {}\n",
    "\n",
    "    ''' {'tg': {'key_evt_advertiser_key,target_evt_advertiser_name', '',...}}'''\n",
    "    new_tg_schema_dict = {}\n",
    "    ''' {'tg': {'AdvertiserReporting_2020-06-01_ford.csv': {file detailed obj dict} }  }'''\n",
    "    new_tg_files_dict = {}\n",
    "    ''' {'tg': {'AdvertiserReporting_2020-06-01_ford.csv': {file detailed obj dict} }  }'''\n",
    "    existing_tg_files_dict = {}\n",
    "\n",
    "\n",
    "    # tg_data_schema_dict = \n",
    "    # tg_data_files_dict = \n",
    "    # target_data_tg_dict = \n",
    "\n",
    "#     src_file_details = valid_file_arg[0]\n",
    "    src_file_loc = s3_ops.get_full_s3_path(src_file_details['Bucket'], src_file_details['Key'])\n",
    "\n",
    "    s3_stream = S3Stream(ACCESS_KEY, SECRET_KEY)\n",
    "    schema =  s3_stream.get_header(src_file_loc)\n",
    "    #schema = 'key_evt_advertiser_key, targe_evt_advertiser_name'\n",
    "    validate_res = validate_schema(schema)\n",
    "    if validate_res['IsValid']:\n",
    "        \n",
    "        \n",
    "        \n",
    "        tg = src_file_details['FileGrp']\n",
    "        file_name = src_file_details['FileName']\n",
    "        \n",
    "        if tg_data_schema_dict.get(tg) is None or tg_data_schema_dict.get(tg) != validate_res['Schema']:\n",
    "                \n",
    "#             data_tg_for_target = target_data_tg_dict.get(validate_res['TargetCol'])\n",
    "#             if  data_tg_for_target is not None:# and data_tg_for_target != tg:\n",
    "#                 target_already_exist_files.add((src_file_loc, data_tg_for_target))\n",
    "#             else:\n",
    "            new_tg_schema_dict[tg] = validate_res['Schema']\n",
    "            new_tg_files_dict[tg] = {file_name: src_file_details}\n",
    "        else:\n",
    "            existing_tg_files_dict[tg] = {file_name: src_file_details}\n",
    "        \n",
    "        src_file_details['Schema'] = validate_res['Schema']\n",
    "        schema_tg_dict[validate_res['Schema']] = tg\n",
    "        \n",
    "        #target_tg_dict[validate_res['TargetCol']] = tg\n",
    "        target_tg_dict = {target:tg for target in validate_res['TargetColsSet']}\n",
    "\n",
    "    else:\n",
    "        invalid_schema_files.add((src_file_loc, schema, validate_res['errors']))\n",
    "\n",
    "    return {'invalid_schema_files': invalid_schema_files,\n",
    "#             'target_already_exist_files':target_already_exist_files,\n",
    "            'schema_tg_dict': schema_tg_dict,\n",
    "            'target_tg_dict':target_tg_dict,\n",
    "            'new_tg_schema_dict': new_tg_schema_dict,\n",
    "            'new_tg_files_dict' : new_tg_files_dict,\n",
    "            'existing_tg_files_dict' : existing_tg_files_dict\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "def src_list_page_process_task(list_page):\n",
    "    \n",
    "    lmt_src_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    lmt_src_loc_bucket = lmt_src_loc_detail['bucket']\n",
    "    lmt_src_loc_key = lmt_src_loc_detail['key']\n",
    "    \n",
    "    invalid_files_set = { s3_ops.get_full_s3_path(lmt_src_loc_detail['bucket'], item['Key']) for item in list_page if  not is_valid_file(key=item['Key'])}\n",
    "    valid_file_set = [[utils.dict_append(extract_info_with_bucket(item['Key'], lmt_src_loc_detail['bucket']),item)] for item in list_page if  is_valid_file(key=item['Key']) ]\n",
    "    collected = NIO.decorated_run_io(task=file_process_task, task_n_args_list=valid_file_set, max_workers=25,)\n",
    "#     return collected\n",
    "    return {'invalid_files_set' : invalid_files_set,\n",
    "            'invalid_schema_files': grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'),\n",
    "#             'target_already_exist_files' : grouped_set_of_flags(collected, flag_dict_type='target_already_exist_files'),\n",
    "            'schema_tg_dict': grouped_flag_dict(collected, flag_dict_type='schema_tg_dict'),\n",
    "            'target_tg_dict': grouped_flag_dict(collected, flag_dict_type='target_tg_dict'),\n",
    "            'new_tg_schema_dict': grouped_flag_dict(collected, flag_dict_type='new_tg_schema_dict'),\n",
    "            'new_tg_files_dict' : grouped_tg(collected, 'new_tg_files_dict'),\n",
    "            'existing_tg_files_dict' : grouped_tg(collected, 'existing_tg_files_dict')\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_src_detail(maxKeysPerReq=3):\n",
    "    lmt_src_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    lmt_src_loc_bucket = lmt_src_loc_detail['bucket']\n",
    "    lmt_src_loc_key = lmt_src_loc_detail['key']\n",
    "    page_generator = s3_ops.list_gen(lmt_src_loc_bucket, lmt_src_loc_key, maxKeysPerReq=maxKeysPerReq, )\n",
    "    page_args_generator = ([page] for page in page_generator)\n",
    "    #list_page = [i for i in page_generator][0]\n",
    "    collected = NIO.decorated_run_with_args_generator(task=src_list_page_process_task, args_generator=page_args_generator, is_kernal_thread=True,)\n",
    "    \n",
    "    return {'invalid_files_set' : grouped_set_of_flags(collected, flag_dict_type='invalid_files_set'),\n",
    "            'invalid_schema_files': grouped_set_of_flags(collected, flag_dict_type='invalid_schema_files'),\n",
    "#             'target_already_exist_files' : grouped_set_of_flags(collected, flag_dict_type='target_already_exist_files'),\n",
    "            'schema_tg_dict': grouped_set_of_flags_dict(collected, flag_dict_type='schema_tg_dict'),\n",
    "            'target_tg_dict': grouped_set_of_flags_dict(collected, flag_dict_type='target_tg_dict'),\n",
    "            'new_tg_schema_dict': grouped_set_of_flags_dict(collected, flag_dict_type='new_tg_schema_dict'),\n",
    "            'new_tg_files_dict' : grouped_tg(collected, 'new_tg_files_dict'),\n",
    "            'existing_tg_files_dict' : grouped_tg(collected, 'existing_tg_files_dict')\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' E.g. '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def s3_copy_into_data_loc_task(tg, file_name, src_file, src_size, dry_run=True):\n",
    "#     data_file_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "    src_file_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "    src_s3 = 's3://{}/{}'.format(src_file_loc_detail['bucket'], src_file)\n",
    "    dest_s3 = '{}{}/{}'.format(lmt_data,tg, file_name)\n",
    "    if dry_run:\n",
    "        print(\"[dry_run]: S3 copy from {} to {}\".format(src_s3, dest_s3))\n",
    "    else:\n",
    "        pass\n",
    "        #s3_ops.copy(src=src_s3, dest = dest_s3, src_size=src_size)\n",
    "    return 'Copied Successfully! by task'\n",
    "\n",
    "\n",
    "def s3_remove_at_data_loc_task(file,  dry_run=True):\n",
    "    data_file_loc_detail = s3_ops.get_bucket_name(lmt_data)\n",
    "#     src_file_loc_detail = s3_ops.get_bucket_name(lmt_src)\n",
    "#     src_s3 = 's3://{}/{}'.format(lmt_src, src_file)\n",
    "    \n",
    "    if dry_run:\n",
    "        file_loc = 's3://{}/{}'.format(data_file_loc_detail['bucket'], file)\n",
    "        print(\"[dry_run]: S3 delete from {} \".format(file_loc))\n",
    "    else:\n",
    "        pass\n",
    "        #s3_ops.delete_file(data_file_loc_detail['bucket'], file)\n",
    "    return 'Deleted Successfully! by task'\n",
    "\n",
    "''' E.g. '''\n",
    "# s3_copy_into_data_loc_task('tg5', 'tg5_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 48 )\n",
    "# s3_remove_at_data_loc_task('taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_data_detail **Start Time = 2020-12-15 04:49:16.944679\n",
      "\n",
      "2020-12-15 04:49:16,994:102410 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:16,995:102410 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:16,996:102410 ThreadPoolExecutor-0_0 (task-0): passed args :['tg1', 's3://qubole-ford/taxonomy_cs/test1/data/tg1/']\n",
      "\n",
      "2020-12-15 04:49:16,997:102410 ThreadPoolExecutor-0_1 (task-1): passed args :['tg15', 's3://qubole-ford/taxonomy_cs/test1/data/tg15/']\n",
      "\n",
      "2020-12-15 04:49:16,997:102410 ThreadPoolExecutor-0_2 (task-2): passed args :['tg2', 's3://qubole-ford/taxonomy_cs/test1/data/tg2/']\n",
      "\n",
      "2020-12-15 04:49:16,998:102410 ThreadPoolExecutor-0_3 (task-3): passed args :['tg3', 's3://qubole-ford/taxonomy_cs/test1/data/tg3/']\n",
      "\n",
      "2020-12-15 04:49:16,999:102410 ThreadPoolExecutor-0_4 (task-4): passed args :['tg5', 's3://qubole-ford/taxonomy_cs/test1/data/tg5/']\n",
      "\n",
      "2020-12-15 04:49:16,999:102410 ThreadPoolExecutor-0_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:16,999:102410 ThreadPoolExecutor-0_5 (task-5): passed args :['tg6', 's3://qubole-ford/taxonomy_cs/test1/data/tg6/']\n",
      "\n",
      "2020-12-15 04:49:17,000:102410 ThreadPoolExecutor-0_6 (task-6): passed args :['tg7', 's3://qubole-ford/taxonomy_cs/test1/data/tg7/']\n",
      "\n",
      "2020-12-15 04:49:17,000:102410 ThreadPoolExecutor-0_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,000:102410 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,001:102410 ThreadPoolExecutor-0_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,002:102410 ThreadPoolExecutor-0_3 (task-3): running\n",
      "\n",
      "2020-12-15 04:49:17,002:102410 ThreadPoolExecutor-0_4 (task-4): running\n",
      "\n",
      "2020-12-15 04:49:17,006:102410 ThreadPoolExecutor-0_5 (task-5): running\n",
      "\n",
      "2020-12-15 04:49:17,007:102410 ThreadPoolExecutor-0_6 (task-6): running\n",
      "\n",
      "2020-12-15 04:49:17,093:102410 ThreadPoolExecutor-0_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,123:102410 ThreadPoolExecutor-0_4 (task-4): done\n",
      "\n",
      "2020-12-15 04:49:17,126:102410 ThreadPoolExecutor-0_6 (task-6): done\n",
      "\n",
      "2020-12-15 04:49:17,131:102410 ThreadPoolExecutor-0_5 (task-5): done\n",
      "\n",
      "2020-12-15 04:49:17,134:102410 ThreadPoolExecutor-0_3 (task-3): done\n",
      "\n",
      "2020-12-15 04:49:17,135:102410 ThreadPoolExecutor-0_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,138:102410 ThreadPoolExecutor-0_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,143:102410 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "extract_data_detail **End Time = 2020-12-15 04:49:17.145142\n",
      "extract_data_detail **Elapsed Time = 0:00:00.200463\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'target_a3': 'tg3',\n",
       " 'target_a7': 'tg7',\n",
       " 'target_a5': 'tg5',\n",
       " 'target_a2': 'tg2',\n",
       " 'target_a1': 'tg1',\n",
       " 'target_a6': 'tg6',\n",
       " 'target_a15': 'tg15'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Get Existing State of System'''\n",
    "tg_data = extract_data_detail(lmt_src, lmt_data, ACCESS_KEY, SECRET_KEY)\n",
    "tg_data_schema_dict = tg_data[0]\n",
    "tg_data_files_dict = tg_data[1]\n",
    "target_data_tg_dict = tg_data[2]\n",
    "\n",
    "target_data_tg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tg3': {'tg3_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg3/tg3_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"ca7090e60660707cd776ede900dc0a1a\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg3_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg3/tg3_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"ca7090e60660707cd776ede900dc0a1a\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg7': {'tg7_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg7/tg7_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 7, 23, 2, 38, tzinfo=tzlocal()),\n",
       "   'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg7_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg7/tg7_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 7, 23, 2, 38, tzinfo=tzlocal()),\n",
       "   'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg5': {'tg5_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg5/tg5_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 7, 23, 2, 38, tzinfo=tzlocal()),\n",
       "   'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg5_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg5/tg5_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 7, 23, 2, 38, tzinfo=tzlocal()),\n",
       "   'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg5_2020-11-05_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg5/tg5_2020-11-05_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg2': {'tg2_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"df46527d151cbac56cf5d648af64f146\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg2_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"df46527d151cbac56cf5d648af64f146\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg2_2020-11-04_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg2/tg2_2020-11-04_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"df46527d151cbac56cf5d648af64f146\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg1': {'tg1_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg1/tg1_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 7, 23, 2, 38, tzinfo=tzlocal()),\n",
       "   'ETag': '\"e74387593f23233a61d30b719b79a381\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg6': {'tg6_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"243cd8420357c06c56877d990740b865\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg6_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 9, 17, 26, 2, tzinfo=tzlocal()),\n",
       "   'ETag': '\"243cd8420357c06c56877d990740b865\"',\n",
       "   'Size': 48,\n",
       "   'StorageClass': 'STANDARD'}},\n",
       " 'tg15': {'tg15_2020-11-01_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg15/tg15_2020-11-01_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 4, 20, 7, 44, tzinfo=tzlocal()),\n",
       "   'ETag': '\"0497fdd31d3a2e3f3b4d12de70ef6640\"',\n",
       "   'Size': 52,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg15_2020-11-02_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg15/tg15_2020-11-02_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 4, 20, 7, 44, tzinfo=tzlocal()),\n",
       "   'ETag': '\"0497fdd31d3a2e3f3b4d12de70ef6640\"',\n",
       "   'Size': 52,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg15_2020-11-03_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg15/tg15_2020-11-03_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 4, 20, 7, 44, tzinfo=tzlocal()),\n",
       "   'ETag': '\"0497fdd31d3a2e3f3b4d12de70ef6640\"',\n",
       "   'Size': 52,\n",
       "   'StorageClass': 'STANDARD'},\n",
       "  'tg15_2020-11-04_ford.csv': {'Key': 'taxonomy_cs/test1/data/tg15/tg15_2020-11-04_ford.csv',\n",
       "   'LastModified': datetime.datetime(2020, 11, 4, 20, 7, 44, tzinfo=tzlocal()),\n",
       "   'ETag': '\"0497fdd31d3a2e3f3b4d12de70ef6640\"',\n",
       "   'Size': 52,\n",
       "   'StorageClass': 'STANDARD'}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg_data_files_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 04:49:17,164   process-id:102410 run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,164   process-id:102410 run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,241   process-id:102458   (task-0): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg0_202-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"535b60451f6d20c2826b045438a50fb9\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"7a5d08cbb4c718d16851d1f2b57ffc50\"', 'Size': 27, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 21, 21, 36, tzinfo=tzlocal()), 'ETag': '\"b19c288a2ef5e2ec6739cac3674391a6\"', 'Size': 28, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,244   process-id:102458   (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,248:102458 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,249:102458 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,250:102458 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg0_2020-11-02_ford.csv', 'FileGrp': 'tg0', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"7a5d08cbb4c718d16851d1f2b57ffc50\"', 'Size': 27, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,252:102458 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg0_2020-11-03_ford.csv', 'FileGrp': 'tg0', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 21, 21, 36, tzinfo=tzlocal()), 'ETag': '\"b19c288a2ef5e2ec6739cac3674391a6\"', 'Size': 28, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,252:102458 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,252:102458 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,253:102458 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,272   process-id:102459   (task-1): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg10_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"1aa284fe1180b5d4d776e26ff8a03358\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg10_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 30, 10, 58, 43, tzinfo=tzlocal()), 'ETag': '\"a512f9130d24ef8b5c41947c1eb72c24\"', 'Size': 52, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,276   process-id:102459   (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,280:102459 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,281:102459 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,283:102459 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg10_2020-11-01_ford.csv', 'FileGrp': 'tg10', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg10_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"1aa284fe1180b5d4d776e26ff8a03358\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,284:102459 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg10_2020-11-02_ford.csv', 'FileGrp': 'tg10', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg10_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 30, 10, 58, 43, tzinfo=tzlocal()), 'ETag': '\"a512f9130d24ef8b5c41947c1eb72c24\"', 'Size': 52, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,284:102459 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg11_2020-11-01_ford.csv', 'FileGrp': 'tg11', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,284:102459 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,292   process-id:102460   (task-2): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg14_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 23, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg14_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 38, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg15_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 10, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,295   process-id:102460   (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,298:102460 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,285:102459 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,299:102460 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,286:102459 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,300:102460 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg14_2020-11-01_ford.csv', 'FileGrp': 'tg14', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg14_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 23, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,302:102460 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg14_2020-11-02_ford.csv', 'FileGrp': 'tg14', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg14_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 38, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,302:102460 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,287:102459 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,313:102458 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,313   process-id:102461   (task-3): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg15_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 33, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg15_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 50, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg15_2020-11-05_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 7, 6, tzinfo=tzlocal()), 'ETag': '\"0521e9fc7fc22768af589b5badb6d3bd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,302:102460 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg15_2020-11-01_ford.csv', 'FileGrp': 'tg15', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg15_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 10, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 04:49:17,316   process-id:102461   (task-3): running\n",
      "\n",
      "2020-12-15 04:49:17,303:102460 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,303:102460 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,320:102461 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,321:102461 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,322:102461 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg15_2020-11-02_ford.csv', 'FileGrp': 'tg15', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg15_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 33, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,323:102461 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg15_2020-11-03_ford.csv', 'FileGrp': 'tg15', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg15_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 50, tzinfo=tzlocal()), 'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,317:102460 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,323:102461 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg15_2020-11-05_ford.csv', 'FileGrp': 'tg15', 'Date': '2020-11-05', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg15_2020-11-05_ford.csv', 'LastModified': datetime.datetime(2020, 11, 4, 19, 7, 6, tzinfo=tzlocal()), 'ETag': '\"0521e9fc7fc22768af589b5badb6d3bd\"', 'Size': 54, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,323:102461 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,324:102461 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,331:102458 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,333:102458 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,325:102461 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,334   process-id:102458   (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,343:102459 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,326:102461 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,352:102459 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,357:102459 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,359:102459 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,360   process-id:102459   (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,364   process-id:102462   (task-4): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg16_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 8, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg16_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 19, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg16_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 30, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,367   process-id:102462   (task-4): running\n",
      "\n",
      "2020-12-15 04:49:17,370:102462 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,371:102462 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,372:102462 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg16_2020-11-01_ford.csv', 'FileGrp': 'tg16', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg16_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 8, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,373:102462 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg16_2020-11-02_ford.csv', 'FileGrp': 'tg16', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg16_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 19, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,374:102460 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,374:102462 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,376:102460 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,385:102460 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,385:102461 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,384   process-id:102463   (task-5): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg17_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 22, 23, 51, tzinfo=tzlocal()), 'ETag': '\"13a19c147582aef1aa046ecef791bc75\"', 'Size': 59, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,374:102462 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg16_2020-11-03_ford.csv', 'FileGrp': 'tg16', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg16_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 30, tzinfo=tzlocal()), 'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"', 'Size': 61, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,387:102460 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,387   process-id:102463   (task-5): running\n",
      "\n",
      "2020-12-15 04:49:17,387:102461 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,388   process-id:102460   (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,374:102462 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,375:102462 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,391:102463 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,392:102463 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,394:102463 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg17_2020-11-01_ford.csv', 'FileGrp': 'tg17', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg17_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 11, 9, 22, 23, 51, tzinfo=tzlocal()), 'ETag': '\"13a19c147582aef1aa046ecef791bc75\"', 'Size': 59, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,394:102461 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,395:102463 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg1_2020-11-01_ford.csv', 'FileGrp': 'tg1', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,396:102461 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,395:102463 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg1_2020-11-02_ford.csv', 'FileGrp': 'tg1', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"e74387593f23233a61d30b719b79a381\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 04:49:17,388:102462 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,397   process-id:102461   (task-3): done\n",
      "\n",
      "2020-12-15 04:49:17,395:102463 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,405   process-id:102464   (task-6): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,408   process-id:102464   (task-6): running\n",
      "\n",
      "2020-12-15 04:49:17,395:102463 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,397:102463 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,412:102464 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,413:102464 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,415:102464 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-01_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,416:102464 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-02_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,398:102463 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,417:102464 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,424   process-id:102465   (task-7): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,417:102464 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg2_2020-11-03_ford.csv', 'FileGrp': 'tg2', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,427   process-id:102465   (task-7): running\n",
      "\n",
      "2020-12-15 04:49:17,429:102462 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,430:102465 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,431:102465 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,432:102465 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-01_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,418:102464 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,433:102465 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-02_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,418:102464 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,434:102465 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg4_2020-11-03_ford.csv', 'FileGrp': 'tg4', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,436:102462 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,434:102465 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,434:102465 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,443:102462 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,445:102462 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,432:102464 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,444   process-id:102466   (task-8): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,436:102465 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,447   process-id:102462   (task-4): done\n",
      "\n",
      "2020-12-15 04:49:17,448   process-id:102466   (task-8): running\n",
      "\n",
      "2020-12-15 04:49:17,451:102466 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,452:102466 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,454:102466 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-01_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,455:102466 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-02_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,455:102466 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 04:49:17,456:102463 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,437:102465 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,456:102466 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-03_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,465   process-id:102467   (task-9): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg5_2020-11-04_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"f87ad6041aa111ac6b6d0776be1c774f\"', 'Size': 50, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,467   process-id:102467   (task-9): running\n",
      "\n",
      "2020-12-15 04:49:17,470:102467 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,456:102466 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,471:102467 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,457:102466 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,473:102467 ThreadPoolExecutor-1_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg5_2020-11-04_ford.csv', 'FileGrp': 'tg5', 'Date': '2020-11-04', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg5_2020-11-04_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"f87ad6041aa111ac6b6d0776be1c774f\"', 'Size': 50, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,474:102467 ThreadPoolExecutor-1_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg6_2020-11-01_ford.csv', 'FileGrp': 'tg6', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,470:102463 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,474:102467 ThreadPoolExecutor-1_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg6_2020-11-02_ford.csv', 'FileGrp': 'tg6', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,475:102463 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,474:102467 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,474:102467 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,478:102463 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,470:102466 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,480   process-id:102463   (task-5): done\n",
      "\n",
      "2020-12-15 04:49:17,484:102464 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,484   process-id:102458  (task-10): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg7_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"d54b283d90621ca6b19c85f4c96d4b8f\"', 'Size': 49, 'StorageClass': 'STANDARD'}]]\n",
      "\n",
      "2020-12-15 04:49:17,485   process-id:102458  (task-10): running\n",
      "\n",
      "2020-12-15 04:49:17,487:102458 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,488:102458 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,488:102464 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,489:102458 ThreadPoolExecutor-2_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-01_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,475:102467 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,490:102458 ThreadPoolExecutor-2_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-02_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-02', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"653eec710cdbf86149efb89f21912022\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,492:102464 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,491:102458 ThreadPoolExecutor-2_2 (task-2): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg7_2020-11-03_ford.csv', 'FileGrp': 'tg7', 'Date': '2020-11-03', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg7_2020-11-03_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"d54b283d90621ca6b19c85f4c96d4b8f\"', 'Size': 49, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,491:102458 ThreadPoolExecutor-2_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,494:102464 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,495   process-id:102464   (task-6): done\n",
      "\n",
      "2020-12-15 04:49:17,491:102458 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,477:102467 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,500:102465 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,492:102458 ThreadPoolExecutor-2_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,506:102465 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,511:102465 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,513:102465 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,493:102458 ThreadPoolExecutor-2_2 (task-2): running\n",
      "\n",
      "2020-12-15 04:49:17,515   process-id:102465   (task-7): done\n",
      "\n",
      "2020-12-15 04:49:17,522:102466 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,526:102466 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,537:102458 ThreadPoolExecutor-2_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,538:102467 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,542   process-id:102410 run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,543   process-id:102459  (task-11): passed args :[[{'Key': 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"8d575874cb97b2d601ae8542aaf11431\"', 'Size': 48, 'StorageClass': 'STANDARD'}, {'Key': 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 04:49:17,540:102467 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,544   process-id:102459  (task-11): running\n",
      "\n",
      "2020-12-15 04:49:17,544:102467 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,545:102459 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,546:102467 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,546:102459 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,547   process-id:102467   (task-9): done\n",
      "\n",
      "2020-12-15 04:49:17,548:102459 ThreadPoolExecutor-2_0 (task-0): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg8_2020-11-01_ford.csv', 'FileGrp': 'tg8', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"8d575874cb97b2d601ae8542aaf11431\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,548:102459 ThreadPoolExecutor-2_1 (task-1): passed args :[{'KeyDirPath': 'taxonomy_cs/test1/src/', 'ParentDir': 'src', 'FileName': 'tg9_2020-11-01_ford.csv', 'FileGrp': 'tg9', 'Date': '2020-11-01', 'ClientName': 'ford', 'Bucket': 'qubole-ford', 'Key': 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv', 'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()), 'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"', 'Size': 48, 'StorageClass': 'STANDARD'}]\n",
      "\n",
      "2020-12-15 04:49:17,548:102459 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,549:102459 ThreadPoolExecutor-2_0 (task-0): running\n",
      "\n",
      "2020-12-15 04:49:17,551:102458 ThreadPoolExecutor-2_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,554:102458 ThreadPoolExecutor-2_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,550:102459 ThreadPoolExecutor-2_1 (task-1): running\n",
      "\n",
      "2020-12-15 04:49:17,556:102458 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,557   process-id:102458  (task-10): done\n",
      "\n",
      "2020-12-15 04:49:17,568:102466 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,570:102466 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,572   process-id:102466   (task-8): done\n",
      "\n",
      "2020-12-15 04:49:17,597:102459 ThreadPoolExecutor-2_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,600:102459 ThreadPoolExecutor-2_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,602:102459 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,603   process-id:102459  (task-11): done\n",
      "\n",
      "2020-12-15 04:49:17,606   process-id:102410 run_blocking_tasks: exiting\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'invalid_files_set': {'s3://qubole-ford/taxonomy_cs/test1/src/tg0_202-11-01_ford.csv'},\n",
       " 'invalid_schema_files': {('s3://qubole-ford/taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv',\n",
       "   'key_a0, targe_a0',\n",
       "   'At least one Target column is required! \\nAll given columns should Key or Target!')},\n",
       " 'schema_tg_dict': {'key_a9,target_a9': {'tg11', 'tg9'},\n",
       "  'key_a8,target_a7': {'tg8'},\n",
       "  'key_a152,target_a152': {'tg15'},\n",
       "  'key_a151,target_a151': {'tg15'},\n",
       "  'key_a5,target_a5': {'tg5'},\n",
       "  'key_a21,target_a21': {'tg2'},\n",
       "  'key_a6,target_a61': {'tg6'},\n",
       "  'key_a51,target_a51': {'tg5'},\n",
       "  'key_a7,target_a7': {'tg14', 'tg7'},\n",
       "  'key_a16,target_a16,target_a162': {'tg16'},\n",
       "  'key_a1,target_a1': {'tg1'},\n",
       "  'key_a16,target_a16,target_a9': {'tg17'},\n",
       "  'key_a10,target_a10': {'tg10'},\n",
       "  'key_a10,target_a9': {'tg10'},\n",
       "  'key_a4,target_a4': {'tg4'},\n",
       "  'key_a7,target_a71': {'tg7'},\n",
       "  'key_a0,target_a0': {'tg0'}},\n",
       " 'target_tg_dict': {'target_a9': {'tg10', 'tg11', 'tg17', 'tg9'},\n",
       "  'target_a7': {'tg14', 'tg7', 'tg8'},\n",
       "  'target_a152': {'tg15'},\n",
       "  'target_a151': {'tg15'},\n",
       "  'target_a5': {'tg5'},\n",
       "  'target_a21': {'tg2'},\n",
       "  'target_a61': {'tg6'},\n",
       "  'target_a51': {'tg5'},\n",
       "  'target_a16': {'tg16', 'tg17'},\n",
       "  'target_a162': {'tg16'},\n",
       "  'target_a1': {'tg1'},\n",
       "  'target_a10': {'tg10'},\n",
       "  'target_a4': {'tg4'},\n",
       "  'target_a71': {'tg7'},\n",
       "  'target_a0': {'tg0'}},\n",
       " 'new_tg_schema_dict': {'tg9': {'key_a9,target_a9'},\n",
       "  'tg8': {'key_a8,target_a7'},\n",
       "  'tg15': {'key_a151,target_a151', 'key_a152,target_a152'},\n",
       "  'tg2': {'key_a21,target_a21'},\n",
       "  'tg6': {'key_a6,target_a61'},\n",
       "  'tg5': {'key_a51,target_a51'},\n",
       "  'tg14': {'key_a7,target_a7'},\n",
       "  'tg16': {'key_a16,target_a16,target_a162'},\n",
       "  'tg17': {'key_a16,target_a16,target_a9'},\n",
       "  'tg11': {'key_a9,target_a9'},\n",
       "  'tg10': {'key_a10,target_a10', 'key_a10,target_a9'},\n",
       "  'tg4': {'key_a4,target_a4'},\n",
       "  'tg7': {'key_a7,target_a71'},\n",
       "  'tg0': {'key_a0,target_a0'}},\n",
       " 'new_tg_files_dict': {'tg9': {'tg9_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg9_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg9',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a9,target_a9'}},\n",
       "  'tg8': {'tg8_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg8_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg8',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"8d575874cb97b2d601ae8542aaf11431\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a8,target_a7'}},\n",
       "  'tg15': {'tg15_2020-11-05_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg15_2020-11-05_ford.csv',\n",
       "    'FileGrp': 'tg15',\n",
       "    'Date': '2020-11-05',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg15_2020-11-05_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 7, 6, tzinfo=tzlocal()),\n",
       "    'ETag': '\"0521e9fc7fc22768af589b5badb6d3bd\"',\n",
       "    'Size': 54,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a152,target_a152'},\n",
       "   'tg15_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg15_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg15',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg15_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 50, tzinfo=tzlocal()),\n",
       "    'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"',\n",
       "    'Size': 54,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a151,target_a151'},\n",
       "   'tg15_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg15_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg15',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg15_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 33, tzinfo=tzlocal()),\n",
       "    'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"',\n",
       "    'Size': 54,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a151,target_a151'},\n",
       "   'tg15_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg15_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg15',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg15_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 6, 10, tzinfo=tzlocal()),\n",
       "    'ETag': '\"4b12a5bf8a7aef2127357db957429ddd\"',\n",
       "    'Size': 54,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a151,target_a151'}},\n",
       "  'tg2': {'tg2_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg2_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg2',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"',\n",
       "    'Size': 50,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a21,target_a21'},\n",
       "   'tg2_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg2_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg2',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"',\n",
       "    'Size': 50,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a21,target_a21'},\n",
       "   'tg2_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg2_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg2',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"dde15e0dffb34575c1aa95f81c8867c0\"',\n",
       "    'Size': 50,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a21,target_a21'}},\n",
       "  'tg6': {'tg6_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg6_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg6',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"',\n",
       "    'Size': 49,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a6,target_a61'},\n",
       "   'tg6_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg6_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg6',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"ae64f1e8ed00a66a125cfeee7223cfa2\"',\n",
       "    'Size': 49,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a6,target_a61'}},\n",
       "  'tg5': {'tg5_2020-11-04_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg5_2020-11-04_ford.csv',\n",
       "    'FileGrp': 'tg5',\n",
       "    'Date': '2020-11-04',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg5_2020-11-04_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"f87ad6041aa111ac6b6d0776be1c774f\"',\n",
       "    'Size': 50,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a51,target_a51'}},\n",
       "  'tg14': {'tg14_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg14_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg14',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg14_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 38, tzinfo=tzlocal()),\n",
       "    'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a7,target_a7'},\n",
       "   'tg14_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg14_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg14',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg14_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 4, 19, 5, 23, tzinfo=tzlocal()),\n",
       "    'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a7,target_a7'}},\n",
       "  'tg16': {'tg16_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg16_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg16',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg16_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 30, tzinfo=tzlocal()),\n",
       "    'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"',\n",
       "    'Size': 61,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a16,target_a16,target_a162'},\n",
       "   'tg16_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg16_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg16',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg16_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 19, tzinfo=tzlocal()),\n",
       "    'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"',\n",
       "    'Size': 61,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a16,target_a16,target_a162'},\n",
       "   'tg16_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg16_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg16',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg16_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 9, 20, 58, 8, tzinfo=tzlocal()),\n",
       "    'ETag': '\"2f8cc6c2e31e3a034ddaedb8e00df7f8\"',\n",
       "    'Size': 61,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a16,target_a16,target_a162'}},\n",
       "  'tg17': {'tg17_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg17_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg17',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg17_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 9, 22, 23, 51, tzinfo=tzlocal()),\n",
       "    'ETag': '\"13a19c147582aef1aa046ecef791bc75\"',\n",
       "    'Size': 59,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a16,target_a16,target_a9'}},\n",
       "  'tg11': {'tg11_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg11_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg11',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"b83eaf4009dc42dd2a744fad592339f9\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a9,target_a9'}},\n",
       "  'tg10': {'tg10_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg10_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg10',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg10_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 11, 30, 10, 58, 43, tzinfo=tzlocal()),\n",
       "    'ETag': '\"a512f9130d24ef8b5c41947c1eb72c24\"',\n",
       "    'Size': 52,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a10,target_a10'},\n",
       "   'tg10_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg10_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg10',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg10_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"1aa284fe1180b5d4d776e26ff8a03358\"',\n",
       "    'Size': 49,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a10,target_a9'}},\n",
       "  'tg4': {'tg4_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg4_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg4',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a4,target_a4'},\n",
       "   'tg4_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg4_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg4',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a4,target_a4'},\n",
       "   'tg4_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg4_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg4',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"ee6aeaee97c71bc6e4c3cea71ad78e35\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a4,target_a4'}},\n",
       "  'tg7': {'tg7_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg7_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg7',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg7_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"d54b283d90621ca6b19c85f4c96d4b8f\"',\n",
       "    'Size': 49,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a7,target_a71'}},\n",
       "  'tg0': {'tg0_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg0_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg0',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"7a5d08cbb4c718d16851d1f2b57ffc50\"',\n",
       "    'Size': 27,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a0,target_a0'}}},\n",
       " 'existing_tg_files_dict': {'tg5': {'tg5_2020-11-03_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg5_2020-11-03_ford.csv',\n",
       "    'FileGrp': 'tg5',\n",
       "    'Date': '2020-11-03',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a5,target_a5'},\n",
       "   'tg5_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg5_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg5',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a5,target_a5'},\n",
       "   'tg5_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg5_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg5',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"471c56a20b21f692659b2f5c68c0b713\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a5,target_a5'}},\n",
       "  'tg1': {'tg1_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg1_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg1',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"e74387593f23233a61d30b719b79a381\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a1,target_a1'},\n",
       "   'tg1_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg1_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg1',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"e74387593f23233a61d30b719b79a381\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a1,target_a1'}},\n",
       "  'tg7': {'tg7_2020-11-02_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg7_2020-11-02_ford.csv',\n",
       "    'FileGrp': 'tg7',\n",
       "    'Date': '2020-11-02',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a7,target_a7'},\n",
       "   'tg7_2020-11-01_ford.csv': {'KeyDirPath': 'taxonomy_cs/test1/src/',\n",
       "    'ParentDir': 'src',\n",
       "    'FileName': 'tg7_2020-11-01_ford.csv',\n",
       "    'FileGrp': 'tg7',\n",
       "    'Date': '2020-11-01',\n",
       "    'ClientName': 'ford',\n",
       "    'Bucket': 'qubole-ford',\n",
       "    'Key': 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv',\n",
       "    'LastModified': datetime.datetime(2020, 10, 29, 19, 53, 45, tzinfo=tzlocal()),\n",
       "    'ETag': '\"653eec710cdbf86149efb89f21912022\"',\n",
       "    'Size': 48,\n",
       "    'StorageClass': 'STANDARD',\n",
       "    'Schema': 'key_a7,target_a7'}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_delta = extract_src_detail()\n",
    "src_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_target_splitter(schema = ''):\n",
    "    tokens = schema.split(',')\n",
    "    key_cols = []\n",
    "    target_cols = []\n",
    "    for t in tokens:\n",
    "        t = t.strip()\n",
    "        if re.match(TARGET_REGEX, t):\n",
    "            target_cols.append(t)\n",
    "        elif re.match(KEY_REGEX, t):\n",
    "            key_cols.append(t)\n",
    "        else:\n",
    "            raise Exception(\"Not a valid schema\")\n",
    "    return [{'target_cols': target_cols, 'key_cols' : key_cols}]\n",
    "\n",
    "\n",
    "@decorator.box_logged\n",
    "def log_report(list_of_row_dict=[], columns:list=[], header_align = 'left', sort_by= None, ascending = True, report_title='', ):\n",
    "    pd.set_option(\"display.colheader_justify\", header_align)\n",
    "    df = pd.DataFrame(list_of_row_dict, columns=columns) \n",
    "    if sort_by is not None:\n",
    "        df = df.sort_values(by=sort_by, ascending=ascending)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=['index'])\n",
    "    #df = df.set_index(' **      ' + df.index.astype(str) )\n",
    "    df = df.rename(' **      {}'.format)\n",
    "    \n",
    "#   df.style.set_properties(**{'text-align': 'left'}).set_table_styles([ dict(selector='td', props=[('text-align', 'left')] ) ])\n",
    "#     df1 = df.reindex(columns=['Taxonomy_Grp','File','Date', 'Schema'])\n",
    "    #df[df.columns[new_order]]\n",
    "    #df = df.transpose()\n",
    "    if report_title != '': \n",
    "        report_titled(report_title)\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"\")\n",
    "    logging.info(str(df))\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"\")\n",
    "\n",
    "\n",
    "def left_justified(df):\n",
    "    formatters = {}\n",
    "    for li in list(df.columns):\n",
    "        max = df[li].str.len().max()\n",
    "        form = \"{{:<{}s}}\".format(max)\n",
    "        formatters[li] = functools.partial(str.format, form)\n",
    "    return df.to_string(formatters=formatters, index=False)   \n",
    "    \n",
    "@decorator.box_titled\n",
    "def report_titled(title:str=''):\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"    \"+title)\n",
    "    logging.info(\"\")\n",
    "    \n",
    "    \n",
    "# class Taxonomy_Grp:\n",
    "    \n",
    "#     def __init__(self, tg_name, key_cols=[], target_col='', data_location=''):\n",
    "#         self.tg_name = tg_name\n",
    "#         self.key_cols = key_cols\n",
    "#         self.target_col = target_col\n",
    "#         self.location =os.path.join(data_location, tg_name)\n",
    "        \n",
    "#     def get_dict(self):\n",
    "#         if self.target_col == '':\n",
    "#             return {'tg_name': self.tg_name}\n",
    "        \n",
    "#         return {'tg_name': self.tg_name, \n",
    "#                 'key_cols': self.key_cols, \n",
    "#                 'target_col': self.target_col, \n",
    "#                 'location': self.location}\n",
    "\n",
    "#     def __str__(self):\n",
    "#         if self.target_col == '':\n",
    "#             return 'tg_name: {}'.format(self.tg_name)\n",
    "#         return 'tg_name: {}, key_cols: {}, target_col: {}, location: {}'.format(self.tg_name, self.key_cols, self.target_col,self.location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 04:49:17,658:102410 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,659:102410 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,660:102410 ThreadPoolExecutor-1_0 (task-0): passed args :['taxonomy_cs/test1/data/tg3/tg3_2020-11-01_ford.csv']\n",
      "\n",
      "2020-12-15 04:49:17,660:102410 ThreadPoolExecutor-1_1 (task-1): passed args :['taxonomy_cs/test1/data/tg3/tg3_2020-11-02_ford.csv']\n",
      "\n",
      "2020-12-15 04:49:17,660:102410 ThreadPoolExecutor-1_2 (task-2): passed args :['taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv']\n",
      "\n",
      "2020-12-15 04:49:17,661:102410 ThreadPoolExecutor-1_3 (task-3): passed args :['taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv']\n",
      "\n",
      "2020-12-15 04:49:17,661:102410 ThreadPoolExecutor-1_0 (task-0): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg3/tg3_2020-11-01_ford.csv \n",
      "2020-12-15 04:49:17,661:102410 ThreadPoolExecutor-1_4 (task-4): passed args :['taxonomy_cs/test1/data/tg2/tg2_2020-11-04_ford.csv']\n",
      "\n",
      "2020-12-15 04:49:17,663:102410 ThreadPoolExecutor-1_5 (task-5): passed args :['taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv']\n",
      "\n",
      "2020-12-15 04:49:17,663:102410 ThreadPoolExecutor-1_1 (task-1): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg3/tg3_2020-11-02_ford.csv 2020-12-15 04:49:17,663:102410 ThreadPoolExecutor-1_6 (task-6): passed args :['taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv']\n",
      "\n",
      "\n",
      "2020-12-15 04:49:17,664:102410 ThreadPoolExecutor-1_2 (task-2): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv \n",
      "2020-12-15 04:49:17,665:102410 ThreadPoolExecutor-1_3 (task-3): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv \n",
      "2020-12-15 04:49:17,666:102410 ThreadPoolExecutor-1_7 (task-7): passed args :['taxonomy_cs/test1/data/tg5/tg5_2020-11-05_ford.csv']\n",
      "\n",
      "2020-12-15 04:49:17,666:102410 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,667:102410 ThreadPoolExecutor-1_0 (task-0): done\n",
      "\n",
      "2020-12-15 04:49:17,668:102410 ThreadPoolExecutor-1_4 (task-4): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-04_ford.csv \n",
      "2020-12-15 04:49:17,668:102410 ThreadPoolExecutor-1_5 (task-5): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv \n",
      "2020-12-15 04:49:17,671:102410 ThreadPoolExecutor-1_6 (task-6): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv \n",
      "2020-12-15 04:49:17,671:102410 ThreadPoolExecutor-1_1 (task-1): done\n",
      "\n",
      "2020-12-15 04:49:17,671:102410 ThreadPoolExecutor-1_2 (task-2): done\n",
      "\n",
      "2020-12-15 04:49:17,673:102410 ThreadPoolExecutor-1_3 (task-3): done\n",
      "\n",
      "2020-12-15 04:49:17,674:102410 ThreadPoolExecutor-1_7 (task-7): running\n",
      "\n",
      "[dry_run]: S3 delete from s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-05_ford.csv \n",
      "2020-12-15 04:49:17,676:102410 ThreadPoolExecutor-1_4 (task-4): done\n",
      "\n",
      "2020-12-15 04:49:17,677:102410 ThreadPoolExecutor-1_5 (task-5): done\n",
      "\n",
      "2020-12-15 04:49:17,678:102410 ThreadPoolExecutor-1_6 (task-6): done\n",
      "\n",
      "2020-12-15 04:49:17,683:102410 ThreadPoolExecutor-1_7 (task-7): done\n",
      "\n",
      "2020-12-15 04:49:17,687:102410 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,689:102410 MainThread run_blocking_tasks: starting\n",
      "\n",
      "2020-12-15 04:49:17,689:102410 MainThread run_blocking_tasks: creating executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,691:102410 ThreadPoolExecutor-2_0 (task-0): passed args :['tg14', 'tg14_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg14_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,691:102410 ThreadPoolExecutor-2_1 (task-1): passed args :['tg14', 'tg14_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg14_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,691:102410 ThreadPoolExecutor-2_2 (task-2): passed args :['tg17', 'tg17_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg17_2020-11-01_ford.csv', 59]\n",
      "\n",
      "2020-12-15 04:49:17,692:102410 ThreadPoolExecutor-2_3 (task-3): passed args :['tg9', 'tg9_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,692:102410 ThreadPoolExecutor-2_0 (task-0): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg14_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg14/tg14_2020-11-02_ford.csv\n",
      "2020-12-15 04:49:17,692:102410 ThreadPoolExecutor-2_4 (task-4): passed args :['tg8', 'tg8_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,693:102410 ThreadPoolExecutor-2_5 (task-5): passed args :['tg6', 'tg6_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv', 49]\n",
      "\n",
      "2020-12-15 04:49:17,693:102410 ThreadPoolExecutor-2_6 (task-6): passed args :['tg6', 'tg6_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv', 49]\n",
      "\n",
      "2020-12-15 04:49:17,694:102410 ThreadPoolExecutor-2_1 (task-1): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg14_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg14/tg14_2020-11-01_ford.csv2020-12-15 04:49:17,694:102410 ThreadPoolExecutor-2_7 (task-7): passed args :['tg0', 'tg0_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv', 27]\n",
      "\n",
      "\n",
      "2020-12-15 04:49:17,695:102410 ThreadPoolExecutor-2_8 (task-8): passed args :['tg2', 'tg2_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv', 50]\n",
      "\n",
      "2020-12-15 04:49:17,696:102410 ThreadPoolExecutor-2_9 (task-9): passed args :['tg2', 'tg2_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv', 50]\n",
      "\n",
      "2020-12-15 04:49:17,697:102410 MainThread run_blocking_tasks: waiting for executor tasks\n",
      "\n",
      "2020-12-15 04:49:17,697:102410 ThreadPoolExecutor-2_2 (task-2): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg17_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg17/tg17_2020-11-01_ford.csv\n",
      "2020-12-15 04:49:17,698:102410 ThreadPoolExecutor-2_3 (task-3): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg9_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg9/tg9_2020-11-01_ford.csv2020-12-15 04:49:17,699:102410 ThreadPoolExecutor-2_0 (task-0): done\n",
      "\n",
      "\n",
      "2020-12-15 04:49:17,699:102410 ThreadPoolExecutor-2_4 (task-4): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg8_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg8/tg8_2020-11-01_ford.csv\n",
      "2020-12-15 04:49:17,700:102410 ThreadPoolExecutor-2_5 (task-5): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg6_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-02_ford.csv\n",
      "2020-12-15 04:49:17,701:102410 ThreadPoolExecutor-2_6 (task-6): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg6_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg6/tg6_2020-11-01_ford.csv2020-12-15 04:49:17,702:102410 ThreadPoolExecutor-2_1 (task-1): done\n",
      "\n",
      "\n",
      "2020-12-15 04:49:17,703:102410 ThreadPoolExecutor-2_7 (task-7): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg0_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg0/tg0_2020-11-02_ford.csv2020-12-15 04:49:17,704:102410 ThreadPoolExecutor-2_8 (task-8): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-03_ford.csv2020-12-15 04:49:17,705:102410 ThreadPoolExecutor-2_9 (task-9): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-02_ford.csv2020-12-15 04:49:17,707:102410 ThreadPoolExecutor-2_2 (task-2): done\n",
      "\n",
      "\n",
      "2020-12-15 04:49:17,708:102410 ThreadPoolExecutor-2_3 (task-3): done\n",
      "\n",
      "2020-12-15 04:49:17,708:102410 ThreadPoolExecutor-2_0 (task-10): passed args :['tg2', 'tg2_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv', 50]\n",
      "\n",
      "2020-12-15 04:49:17,709:102410 ThreadPoolExecutor-2_4 (task-4): done\n",
      "\n",
      "2020-12-15 04:49:17,710:102410 ThreadPoolExecutor-2_5 (task-5): done\n",
      "\n",
      "2020-12-15 04:49:17,711:102410 ThreadPoolExecutor-2_6 (task-6): done\n",
      "\n",
      "2020-12-15 04:49:17,711:102410 ThreadPoolExecutor-2_1 (task-11): passed args :['tg4', 'tg4_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv', 48]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 04:49:17,712:102410 ThreadPoolExecutor-2_7 (task-7): done\n",
      "\n",
      "2020-12-15 04:49:17,713:102410 ThreadPoolExecutor-2_8 (task-8): done\n",
      "\n",
      "2020-12-15 04:49:17,713:102410 ThreadPoolExecutor-2_9 (task-9): done\n",
      "\n",
      "2020-12-15 04:49:17,714:102410 ThreadPoolExecutor-2_2 (task-12): passed args :['tg4', 'tg4_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,715:102410 ThreadPoolExecutor-2_3 (task-13): passed args :['tg4', 'tg4_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,718:102410 ThreadPoolExecutor-2_0 (task-10): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg2_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg2/tg2_2020-11-01_ford.csv2020-12-15 04:49:17,719:102410 ThreadPoolExecutor-2_4 (task-14): passed args :['tg11', 'tg11_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv', 48]\n",
      "\n",
      "\n",
      "2020-12-15 04:49:17,720:102410 ThreadPoolExecutor-2_5 (task-15): passed args :['tg16', 'tg16_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg16_2020-11-03_ford.csv', 61]\n",
      "\n",
      "2020-12-15 04:49:17,721:102410 ThreadPoolExecutor-2_6 (task-16): passed args :['tg16', 'tg16_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg16_2020-11-02_ford.csv', 61]\n",
      "\n",
      "2020-12-15 04:49:17,722:102410 ThreadPoolExecutor-2_1 (task-11): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-03_ford.csv\n",
      "2020-12-15 04:49:17,723:102410 ThreadPoolExecutor-2_7 (task-17): passed args :['tg16', 'tg16_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg16_2020-11-01_ford.csv', 61]\n",
      "\n",
      "2020-12-15 04:49:17,724:102410 ThreadPoolExecutor-2_8 (task-18): passed args :['tg5', 'tg5_2020-11-03_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,725:102410 ThreadPoolExecutor-2_9 (task-19): passed args :['tg5', 'tg5_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,725:102410 ThreadPoolExecutor-2_2 (task-12): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-02_ford.csv\n",
      "2020-12-15 04:49:17,727:102410 ThreadPoolExecutor-2_3 (task-13): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg4_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg4/tg4_2020-11-01_ford.csv\n",
      "2020-12-15 04:49:17,728:102410 ThreadPoolExecutor-2_0 (task-10): done\n",
      "\n",
      "2020-12-15 04:49:17,728:102410 ThreadPoolExecutor-2_4 (task-14): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg11_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg11/tg11_2020-11-01_ford.csv\n",
      "2020-12-15 04:49:17,729:102410 ThreadPoolExecutor-2_5 (task-15): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg16_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg16/tg16_2020-11-03_ford.csv2020-12-15 04:49:17,730:102410 ThreadPoolExecutor-2_6 (task-16): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg16_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg16/tg16_2020-11-02_ford.csv\n",
      "2020-12-15 04:49:17,731:102410 ThreadPoolExecutor-2_1 (task-11): done\n",
      "\n",
      "2020-12-15 04:49:17,731:102410 ThreadPoolExecutor-2_7 (task-17): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg16_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg16/tg16_2020-11-01_ford.csv2020-12-15 04:49:17,733:102410 ThreadPoolExecutor-2_8 (task-18): running\n",
      "\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-03_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-03_ford.csv\n",
      "2020-12-15 04:49:17,733:102410 ThreadPoolExecutor-2_9 (task-19): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-02_ford.csv\n",
      "2020-12-15 04:49:17,735:102410 ThreadPoolExecutor-2_3 (task-13): done\n",
      "\n",
      "2020-12-15 04:49:17,735:102410 ThreadPoolExecutor-2_2 (task-12): done\n",
      "\n",
      "2020-12-15 04:49:17,736:102410 ThreadPoolExecutor-2_0 (task-20): passed args :['tg5', 'tg5_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,738:102410 ThreadPoolExecutor-2_4 (task-14): done\n",
      "\n",
      "2020-12-15 04:49:17,739:102410 ThreadPoolExecutor-2_5 (task-15): done\n",
      "\n",
      "2020-12-15 04:49:17,740:102410 ThreadPoolExecutor-2_6 (task-16): done\n",
      "\n",
      "2020-12-15 04:49:17,740:102410 ThreadPoolExecutor-2_1 (task-21): passed args :['tg1', 'tg1_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,742:102410 ThreadPoolExecutor-2_7 (task-17): done\n",
      "\n",
      "2020-12-15 04:49:17,742:102410 ThreadPoolExecutor-2_8 (task-18): done\n",
      "\n",
      "2020-12-15 04:49:17,745:102410 ThreadPoolExecutor-2_9 (task-19): done\n",
      "\n",
      "2020-12-15 04:49:17,745:102410 ThreadPoolExecutor-2_3 (task-22): passed args :['tg1', 'tg1_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,746:102410 ThreadPoolExecutor-2_2 (task-23): passed args :['tg7', 'tg7_2020-11-02_ford.csv', 'taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv', 48]\n",
      "\n",
      "2020-12-15 04:49:17,747:102410 ThreadPoolExecutor-2_0 (task-20): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg5_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg5/tg5_2020-11-01_ford.csv2020-12-15 04:49:17,747:102410 ThreadPoolExecutor-2_4 (task-24): passed args :['tg7', 'tg7_2020-11-01_ford.csv', 'taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv', 48]\n",
      "\n",
      "\n",
      "2020-12-15 04:49:17,750:102410 ThreadPoolExecutor-2_1 (task-21): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg1_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg1/tg1_2020-11-02_ford.csv\n",
      "2020-12-15 04:49:17,753:102410 ThreadPoolExecutor-2_3 (task-22): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg1_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg1/tg1_2020-11-01_ford.csv\n",
      "2020-12-15 04:49:17,754:102410 ThreadPoolExecutor-2_2 (task-23): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg7_2020-11-02_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg7/tg7_2020-11-02_ford.csv2020-12-15 04:49:17,756:102410 ThreadPoolExecutor-2_0 (task-20): done\n",
      "\n",
      "\n",
      "2020-12-15 04:49:17,756:102410 ThreadPoolExecutor-2_4 (task-24): running\n",
      "\n",
      "[dry_run]: S3 copy from s3://qubole-ford/taxonomy_cs/test1/src/tg7_2020-11-01_ford.csv to s3://qubole-ford/taxonomy_cs/test1/data/tg7/tg7_2020-11-01_ford.csv\n",
      "2020-12-15 04:49:17,758:102410 ThreadPoolExecutor-2_1 (task-21): done\n",
      "\n",
      "2020-12-15 04:49:17,758:102410 ThreadPoolExecutor-2_3 (task-22): done\n",
      "\n",
      "2020-12-15 04:49:17,759:102410 ThreadPoolExecutor-2_2 (task-23): done\n",
      "\n",
      "2020-12-15 04:49:17,761:102410 ThreadPoolExecutor-2_4 (task-24): done\n",
      "\n",
      "2020-12-15 04:49:17,764:102410 MainThread run_blocking_tasks: exiting\n",
      "\n",
      "2020-12-15 04:49:17,776 INFO libs.xml_writer: \n",
      "<configroot version=\"12.1\">\n",
      "\t<set>\n",
      "\t\t<name>CS_TAXONOMY_LMT_SCHEMA_SET</name>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg14</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a7</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a7</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg14/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg17</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a16</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a16</attr>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a9</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg17/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg9</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a9</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a9</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg9/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg8</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a8</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a7</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg8/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg6</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a6</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a61</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg6/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg0</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a0</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a0</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg0/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg2</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a21</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a21</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg2/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg4</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a4</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a4</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg4/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg11</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a9</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a9</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg11/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg16</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a16</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a16</attr>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a162</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg16/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg1</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a1</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a1</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg1/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg7</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a7</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a7</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg7/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg5</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a5</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a5</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t\t<partitionby_columns>\n",
      "\t\t\t</partitionby_columns>\n",
      "\t\t\t<row_delimiter>\n",
      "\t\t\t\t<!-- Row separator -->\n",
      "\t\t\t\t<val>'\\n'</val>\n",
      "\t\t\t</row_delimiter>\n",
      "\t\t\t<column_delimiter>\n",
      "\t\t\t\t<!-- Column separator -->\n",
      "\t\t\t\t<val>','</val>\n",
      "\t\t\t</column_delimiter>\n",
      "\t\t\t<serde>\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\n",
      "\t\t\t</serde>\n",
      "\t\t\t<serde_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</serde_properties>\n",
      "\t\t\t<table_properties>\n",
      "\t\t\t\t<val/>\n",
      "\t\t\t</table_properties>\n",
      "\t\t\t<storage_type>\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\n",
      "\t\t\t\t<val>TEXTFILE</val>\n",
      "\t\t\t</storage_type>\n",
      "\t\t\t<location>\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg5/</val>\n",
      "\t\t\t</location>\n",
      "\t\t</elements>\n",
      "        \n",
      "    </set>\n",
      "   \n",
      "    \n",
      "    <set>\n",
      "\t\t<name>DROP_CS_TAXONOMY_LMT_SCHEMA_SET</name>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg3</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr>key_a3</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr>target_a3</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg2</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr>key_a2</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr>target_a2</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg6</val>\n",
      "\t\t\t</subsource_name>\n",
      "\t\t\t<key-columns>\n",
      "                \n",
      "\t\t\t\t<attr>key_a6</attr>\n",
      "                \n",
      "\t\t\t</key-columns>\n",
      "\t\t\t<target-columns>\n",
      "                \n",
      "\t\t\t\t<attr>target_a6</attr>\n",
      "                \n",
      "\t\t\t</target-columns>\n",
      "\t\t</elements>\n",
      "        \n",
      "    </set>\n",
      "    \n",
      "   <set>\n",
      "\t\t<name>CS_TAXONOMY_GROUP_ORDERED_SET</name>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg0</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>1</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg1</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>2</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg11</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>3</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg14</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>4</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg16</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>5</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg17</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>6</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg2</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>7</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg4</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>8</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg5</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>9</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg6</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>10</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg7</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>11</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg8</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>12</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "\t\t<elements>\n",
      "\t\t\t<subsource_name>\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\n",
      "\t\t\t\t<val>tg9</val>\n",
      "\t\t\t</subsource_name>\n",
      "\n",
      "            <precedence_order>              \n",
      "\t\t\t\t<val>13</val>\n",
      "\t\t\t</precedence_order>\n",
      "\n",
      "\t\t</elements>\n",
      "        \n",
      "    </set>\n",
      "    \n",
      "</configroot>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 04:49:17,778 INFO libs.xml_writer: test.xml has been generated.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Extract Info needed to expose configs and show in logs and reports'''\n",
    "\n",
    "invalid_files_set = src_delta['invalid_files_set']\n",
    "invalid_schema_files = src_delta['invalid_schema_files']\n",
    "\n",
    "tg_data = { k for k in tg_data_files_dict.keys()}\n",
    "tg_existing = { k for k in src_delta['existing_tg_files_dict'].keys()}\n",
    "tg_new ={ k for k in src_delta['new_tg_files_dict'].keys()}\n",
    "tg_all = tg_new.union(tg_existing)\n",
    "\n",
    "many_tg4schema_check_gen = (v for k, v in src_delta['schema_tg_dict'].items() if len(v) > 1)\n",
    "many_tg4target_check_gen = (v for k, v in src_delta['target_tg_dict'].items() if len(v) > 1)\n",
    "newTg4schema = {k for k, v in src_delta['new_tg_schema_dict'].items() if len(v) > 1}\n",
    "\n",
    "\n",
    "tg4schema = set()\n",
    "[tg4schema.update(i) for i in many_tg4schema_check_gen]\n",
    "tg4target = set()\n",
    "[tg4target.update(i) for i in many_tg4target_check_gen]\n",
    "\n",
    "\n",
    "# invalid_tg_with_dup_schema = (tg4schema.union(newTg4schema)).difference(tg_existing)\n",
    "\n",
    "# invalid_tg_with_dup_target = tg4target.difference(tg_existing)\n",
    "\n",
    "# invalid_tg_all = invalid_tg_with_dup_schema.union(invalid_tg_with_dup_target)\n",
    "\n",
    "invalid_tg_all = newTg4schema.difference(tg_existing)\n",
    "\n",
    "tg_delta = tg_new.difference(invalid_tg_all)\n",
    "\n",
    "tg_delta_create = tg_delta.difference(tg_data)\n",
    "\n",
    "tg_delta_drop_n_create = (tg_delta.intersection(tg_data)).difference(tg_existing)\n",
    "\n",
    "tg_dropped = tg_data.difference(tg_all)\n",
    "\n",
    "tg_dropped_all = tg_dropped.union(tg_delta_drop_n_create)\n",
    "tg_create_all = tg_delta_create.union(tg_delta_drop_n_create)\n",
    "\n",
    "''' File Sync'''\n",
    "files_to_be_dropped = [[f['Key']] for i in  tg_dropped_all \n",
    "                       for fn, f in tg_data_files_dict.get(i).items()]\n",
    "files_not_retained_existing_tg =[[tg_data_files_dict.get(tg).get(fn)['Key']]\n",
    "                                 for tg in tg_existing \n",
    "                                 for fn in set(tg_data_files_dict.get(tg).keys()).difference(set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "file_drop_args = []\n",
    "file_drop_args.extend(files_to_be_dropped )\n",
    "file_drop_args.extend(files_not_retained_existing_tg )\n",
    "\n",
    "\n",
    "files_to_be_created = [[i, f['FileName'], f['Key'], f['Size']] \n",
    "                       for i in  tg_create_all \n",
    "                       for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "files_to_be_copied = [[k, f_dict['FileName'], f_dict['Key'], f_dict['Size']] \n",
    "                      for k, v in src_delta['existing_tg_files_dict'].items() \n",
    "                      for f, f_dict in v.items()] #['Key']]\n",
    "file_copy_args = []\n",
    "file_copy_args.extend(files_to_be_created )\n",
    "file_copy_args.extend(files_to_be_copied )\n",
    "\n",
    "collected = NIO.decorated_run_io(task=s3_remove_at_data_loc_task, task_n_args_list=file_drop_args, \n",
    "                                 is_kernal_thread=False,)\n",
    "\n",
    "collected = NIO.decorated_run_io(task=s3_copy_into_data_loc_task, task_n_args_list=file_copy_args, \n",
    "                                 is_kernal_thread=False,)\n",
    "\n",
    "\n",
    "\n",
    "''' Expose details to generate configs'''\n",
    "tg_create_all_n_schema = {tg: schema \n",
    "                          for tg in tg_create_all \n",
    "                          for schema in src_delta['new_tg_schema_dict'].get(tg)}\n",
    "tg_retain_all_n_schema = {tg: tg_data_schema_dict.get(tg) \n",
    "                          for tg in tg_existing}\n",
    "tg_dropped_all_n_schema = {tg: tg_data_schema_dict.get(tg) \n",
    "                           for tg in tg_dropped_all }\n",
    "\n",
    "\n",
    "'''New and Drop_n_create(With new attributes like schema) Taxonomy Grps'''\n",
    "exposed_tg_all = [Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target_cols'], lmt_data) \n",
    "                  for tg ,schema in tg_create_all_n_schema.items() \n",
    "                  for schema_dict in key_target_splitter(schema)]\n",
    "'''Retaining Taxonomy Grps with either NO CHANGES or Create and Drop some files in a retained group'''\n",
    "exposed_tg_all.extend([Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target_cols'], lmt_data) \n",
    "                       for tg ,schema in tg_retain_all_n_schema.items() \n",
    "                       for schema_dict in key_target_splitter(schema)])\n",
    "\n",
    "\n",
    "'''Dropped and Drop_n_create(With old attributes like schema) Taxonomy Grps'''\n",
    "exposed_dropped_tg_all = [Taxonomy_Grp(tg,schema_dict['key_cols'], schema_dict['target_cols'], lmt_data) \n",
    "                          for tg ,schema in tg_dropped_all_n_schema.items() \n",
    "                          for schema_dict in key_target_splitter(schema)]\n",
    "\n",
    "'''Exposed Tg grp with precedence order'''\n",
    "exposed_tg_name_list = [i.tg_name for i in exposed_tg_all]\n",
    "\n",
    "exposed_tg_name_ordered_list = sorted(exposed_tg_name_list, key=str.lower)\n",
    "\n",
    "\n",
    "''' Generating output config xml'''\n",
    "xml_writer.generate_output_config(exposed_tg_all, exposed_dropped_tg_all, exposed_tg_name_ordered_list, \n",
    "                                  dn_version, config_input_loc, config_file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ''' Report '''\n",
    "# # invalid files Schema delails\n",
    "# # To Be\n",
    "\n",
    "# # TG level\n",
    "# tg_create_n_schema = [(i, src_delta['new_tg_schema_dict'].get(i)) for i in tg_delta_create]\n",
    "# tg_drop_create_n_schema = [(tg, tg_data_schema_dict.get(tg), schema_new) for tg in tg_delta_drop_n_create for schema_new in src_delta['new_tg_schema_dict'].get(tg)]\n",
    "# tg_drop_n_schema = [(i, tg_data_schema_dict.get(i)) for i in tg_dropped]\n",
    "# tg_retain_n_schema = [(i, tg_data_schema_dict.get(i)) for i in tg_existing]\n",
    "\n",
    "# # File Level\n",
    "# files_to_be_dropped = [ f['Key'] for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items()]\n",
    "# files_to_be_dropped_schema_change = [ f['Key'] for i in  tg_delta_drop_n_create for fn, f in tg_data_files_dict.get(i).items()]\n",
    "# files_to_be_created = {f['Key'] :f['Schema'] for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()}\n",
    "# files_to_be_created_schema_change = {f['Key'] :f['Schema'] for i in  tg_delta_drop_n_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'tg10', 'tg11', 'tg17', 'tg9'}, 'target_a9'),\n",
       " ({'tg14', 'tg7', 'tg8'}, 'target_a7'),\n",
       " ({'tg15'}, 'target_a152'),\n",
       " ({'tg15'}, 'target_a151'),\n",
       " ({'tg5'}, 'target_a5'),\n",
       " ({'tg2'}, 'target_a21'),\n",
       " ({'tg6'}, 'target_a61'),\n",
       " ({'tg5'}, 'target_a51'),\n",
       " ({'tg16', 'tg17'}, 'target_a16'),\n",
       " ({'tg16'}, 'target_a162'),\n",
       " ({'tg1'}, 'target_a1'),\n",
       " ({'tg10'}, 'target_a10'),\n",
       " ({'tg4'}, 'target_a4'),\n",
       " ({'tg7'}, 'target_a71'),\n",
       " ({'tg0'}, 'target_a0')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "many_tg4target_check_gen = (v for k, v in src_delta['target_tg_dict'].items() if len(v) > 1)\n",
    "tg4target = set()\n",
    "[tg4target.update(i) for i in many_tg4target_check_gen]\n",
    "tg4target\n",
    "[(v,k) for k, v in src_delta['target_tg_dict'].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tg14', ['key_a7'], ['target_a7']),\n",
       " ('tg17', ['key_a16'], ['target_a16', 'target_a9']),\n",
       " ('tg9', ['key_a9'], ['target_a9']),\n",
       " ('tg8', ['key_a8'], ['target_a7']),\n",
       " ('tg6', ['key_a6'], ['target_a61']),\n",
       " ('tg0', ['key_a0'], ['target_a0']),\n",
       " ('tg2', ['key_a21'], ['target_a21']),\n",
       " ('tg4', ['key_a4'], ['target_a4']),\n",
       " ('tg11', ['key_a9'], ['target_a9']),\n",
       " ('tg16', ['key_a16'], ['target_a16', 'target_a162']),\n",
       " ('tg1', ['key_a1'], ['target_a1']),\n",
       " ('tg7', ['key_a7'], ['target_a7']),\n",
       " ('tg5', ['key_a5'], ['target_a5'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i.tg_name, i.key_cols, i.target_cols) for i in exposed_tg_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tg15', 'tg10']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(i) for i in invalid_tg_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tg0', 'tg11', 'tg14', 'tg16', 'tg17', 'tg2', 'tg4', 'tg6', 'tg8', 'tg9'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tg_create_all\n",
    "# tg_create_all_n_dates = {tg: file_attr.get('Date') \n",
    "#                           for tg in tg_create_all \n",
    "#                           for f, file_attr in src_delta['new_tg_files_dict'].get(tg).items()}\n",
    "# tg_create_all_n_dates\n",
    "# src_delta['new_tg_files_dict']\n",
    "tg_create_all\n",
    "# a_list = None\n",
    "# a_list = {'d','b', 'c', 'a', 'e'}\n",
    "# b_list = list(a_list)\n",
    "# t = sorted(b_list, key=str.lower)\n",
    "# [i a_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tg14',\n",
       " 'tg17',\n",
       " 'tg9',\n",
       " 'tg8',\n",
       " 'tg6',\n",
       " 'tg0',\n",
       " 'tg2',\n",
       " 'tg4',\n",
       " 'tg11',\n",
       " 'tg16',\n",
       " 'tg1',\n",
       " 'tg7',\n",
       " 'tg5']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exposed_tg_name_list = [i.tg_name for i in exposed_tg_all]\n",
    "\n",
    "# exposed_tg_name_ordered_list = sorted(exposed_tg_name_list, key=str.lower)\n",
    "exposed_tg_name_ordered_list\n",
    "exposed_tg_name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **            tg_name key_cols   target_cols                location                                     \n",
      " **      0    tg0     [key_a0]                [target_a0]   s3://qubole-ford/taxonomy_cs/test1/data/tg0\n",
      " **      1    tg1     [key_a1]                [target_a1]   s3://qubole-ford/taxonomy_cs/test1/data/tg1\n",
      " **      2   tg11     [key_a9]                [target_a9]  s3://qubole-ford/taxonomy_cs/test1/data/tg11\n",
      " **      3   tg14     [key_a7]                [target_a7]  s3://qubole-ford/taxonomy_cs/test1/data/tg14\n",
      " **      4   tg16    [key_a16]  [target_a16, target_a162]  s3://qubole-ford/taxonomy_cs/test1/data/tg16\n",
      " **      5   tg17    [key_a16]    [target_a16, target_a9]  s3://qubole-ford/taxonomy_cs/test1/data/tg17\n",
      " **      6    tg2    [key_a21]               [target_a21]   s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      7    tg4     [key_a4]                [target_a4]   s3://qubole-ford/taxonomy_cs/test1/data/tg4\n",
      " **      8    tg5     [key_a5]                [target_a5]   s3://qubole-ford/taxonomy_cs/test1/data/tg5\n",
      " **      9    tg6     [key_a6]               [target_a61]   s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **      10   tg7     [key_a7]                [target_a7]   s3://qubole-ford/taxonomy_cs/test1/data/tg7\n",
      " **      11   tg8     [key_a8]                [target_a7]   s3://qubole-ford/taxonomy_cs/test1/data/tg8\n",
      " **      12   tg9     [key_a9]                [target_a9]   s3://qubole-ford/taxonomy_cs/test1/data/tg9\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           tg_name key_cols  target_cols  location                                    \n",
      " **      0  tg2     [key_a2]  [target_a2]  s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      1  tg3     [key_a3]  [target_a3]  s3://qubole-ford/taxonomy_cs/test1/data/tg3\n",
      " **      2  tg6     [key_a6]  [target_a6]  s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                 Date        Schema               \n",
      " **      0  tg15         tg15_2020-11-05_ford.csv  2020-11-05  key_a152,target_a152\n",
      " **      1  tg15         tg15_2020-11-03_ford.csv  2020-11-03  key_a151,target_a151\n",
      " **      2  tg15         tg15_2020-11-02_ford.csv  2020-11-02  key_a151,target_a151\n",
      " **      3  tg15         tg15_2020-11-01_ford.csv  2020-11-01  key_a151,target_a151\n",
      " **      4  tg10         tg10_2020-11-02_ford.csv  2020-11-02    key_a10,target_a10\n",
      " **      5  tg10         tg10_2020-11-01_ford.csv  2020-11-01     key_a10,target_a9\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema              Grp_Schema       \n",
      " **      0  tg7          tg7_2020-11-03_ford.csv  2020-11-03   key_a7,target_a71  key_a7,target_a7\n",
      " **      1  tg5          tg5_2020-11-04_ford.csv  2020-11-04  key_a51,target_a51  key_a5,target_a5\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name                                                     \n",
      " **      0  s3://qubole-ford/taxonomy_cs/test1/src/tg0_202-11-01_ford.csv\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name                                                       Schema            Reason                                                                            \n",
      " **      0  s3://qubole-ford/taxonomy_cs/test1/src/tg0_2020-11-03_ford.csv  key_a0, targe_a0  At least one Target column is required! \\nAll given columns should Key or Target!\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema           \n",
      " **      0  tg3          tg3_2020-11-01_ford.csv  2020-11-01  key_a3,target_a3\n",
      " **      1  tg3          tg3_2020-11-02_ford.csv  2020-11-02  key_a3,target_a3\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema         \n",
      " **      0  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21\n",
      " **      2  tg2  tg2_2020-11-04_ford.csv  2020-11-04  key_a2,target_a2  key_a21,target_a21\n",
      " **      3  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61\n",
      " **      4  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **            Taxonomy_Grp File_Name                 Date        Schema                         \n",
      " **      0   tg14         tg14_2020-11-02_ford.csv  2020-11-02                key_a7,target_a7\n",
      " **      1   tg14         tg14_2020-11-01_ford.csv  2020-11-01                key_a7,target_a7\n",
      " **      2   tg17         tg17_2020-11-01_ford.csv  2020-11-01    key_a16,target_a16,target_a9\n",
      " **      3    tg9          tg9_2020-11-01_ford.csv  2020-11-01                key_a9,target_a9\n",
      " **      4    tg8          tg8_2020-11-01_ford.csv  2020-11-01                key_a8,target_a7\n",
      " **      5    tg0          tg0_2020-11-02_ford.csv  2020-11-02                key_a0,target_a0\n",
      " **      6    tg4          tg4_2020-11-03_ford.csv  2020-11-03                key_a4,target_a4\n",
      " **      7    tg4          tg4_2020-11-02_ford.csv  2020-11-02                key_a4,target_a4\n",
      " **      8    tg4          tg4_2020-11-01_ford.csv  2020-11-01                key_a4,target_a4\n",
      " **      9   tg11         tg11_2020-11-01_ford.csv  2020-11-01                key_a9,target_a9\n",
      " **      10  tg16         tg16_2020-11-03_ford.csv  2020-11-03  key_a16,target_a16,target_a162\n",
      " **      11  tg16         tg16_2020-11-02_ford.csv  2020-11-02  key_a16,target_a16,target_a162\n",
      " **      12  tg16         tg16_2020-11-01_ford.csv  2020-11-01  key_a16,target_a16,target_a162\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema          Desc         \n",
      " **      0  tg2  tg2_2020-11-03_ford.csv  2020-11-03               NAN  key_a21,target_a21      New File\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      2  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      3  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **      4  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61  Re-delivered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema            Desc     \n",
      " **      0  tg1          tg1_2020-11-01_ford.csv  2020-11-01  key_a1,target_a1  Retained\n",
      " **      1  tg1          tg1_2020-11-02_ford.csv  2020-11-02  key_a1,target_a1  New File\n",
      " **      2  tg5          tg5_2020-11-01_ford.csv  2020-11-01  key_a5,target_a5  Retained\n",
      " **      3  tg5          tg5_2020-11-02_ford.csv  2020-11-02  key_a5,target_a5  Retained\n",
      " **      4  tg5          tg5_2020-11-03_ford.csv  2020-11-03  key_a5,target_a5  New File\n",
      " **      5  tg5          tg5_2020-11-05_ford.csv  2020-11-05  key_a5,target_a5   Dropped\n",
      " **      6  tg7          tg7_2020-11-01_ford.csv  2020-11-01  key_a7,target_a7  Retained\n",
      " **      7  tg7          tg7_2020-11-02_ford.csv  2020-11-02  key_a7,target_a7  Retained\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ''' Report '''\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "\n",
    "'''  Exposed TG Report  '''\n",
    "\n",
    "exposed_tg_report_data = [i.get_dict() for i in exposed_tg_all]\n",
    "log_report(exposed_tg_report_data,  columns=['tg_name', 'key_cols','target_cols', 'location'], sort_by='tg_name')\n",
    "\n",
    "\n",
    "'''  Exposed Dropped TG Report '''\n",
    "\n",
    "exposed_tg_dropped_report_data = [i.get_dict() for i in exposed_dropped_tg_all]\n",
    "log_report(exposed_tg_dropped_report_data, columns=['tg_name', 'key_cols','target_cols', 'location'], sort_by='tg_name')\n",
    "\n",
    "\n",
    "# '''  Invalid TG due to schema conflict/already used Report '''\n",
    "\n",
    "# invalid_tg_with_dup_schema_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "#                     for i in  invalid_tg_with_dup_schema for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "# log_report(invalid_tg_with_dup_schema_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "# '''  Invalid TG due to Target Column conflict/already used Report '''\n",
    "\n",
    "# invalid_tg_with_dup_target_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "#                     for i in  invalid_tg_with_dup_target for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "# log_report(invalid_tg_with_dup_target_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''  Invalid New TG due to Schema conflict among files Report '''\n",
    "\n",
    "invalid_tg_with_schema_conflict_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                    for i in  invalid_tg_all for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(invalid_tg_with_schema_conflict_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'])\n",
    "\n",
    "\n",
    "'''  Invalid files from retained grp due to schema or target mismatch with previously delivered files for same'''\n",
    "\n",
    "partially_invalid_tg_set = tg_new.intersection(tg_existing)\n",
    "partially_invalid_tg_report = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Grp_Schema': tg_data_schema_dict[i] } \n",
    "                               for i in partially_invalid_tg_set  \n",
    "                               for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(partially_invalid_tg_report, columns=['Taxonomy_Grp','File_Name','Date', 'Schema','Grp_Schema']) \n",
    "\n",
    "\n",
    "'''   Invalid file not match with required file pattern '''\n",
    "\n",
    "invalid_files_report_data = [{'File_Name' : i} for i in invalid_files_set]\n",
    "log_report(invalid_files_report_data,  columns=['File_Name'])\n",
    "\n",
    "\n",
    "'''   Invalid file not match with required schema pattern '''\n",
    "\n",
    "invalid_schema_files_rep_data = [{'File_Name' : i[0], 'Schema': i[1], 'Reason' : i[2]} for i in invalid_schema_files]\n",
    "log_report(invalid_schema_files_rep_data,  columns=['File_Name', 'Schema','Reason'], header_align='left')\n",
    "\n",
    "\n",
    "'''   Dropped TG Completely '''\n",
    "\n",
    "tg_dropped_rep_gen =(extract_info(f['Key']) for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items())\n",
    "tg_dropped_report_dict =  [{'Taxonomy_Grp':i['FileGrp'], 'File_Name':i['FileName'], 'Date':i['Date'], 'Schema': tg_data_schema_dict[i['FileGrp']] }\n",
    "                           for i in tg_dropped_rep_gen] \n",
    "log_report(list_of_row_dict=tg_dropped_report_dict,columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''   Dropped TG to change schema '''\n",
    "\n",
    "tg_drop_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), schema_new, extract_info(f['Key'])) \n",
    "                             for tg in tg_delta_drop_n_create \n",
    "                             for schema_new in src_delta['new_tg_schema_dict'].get(tg)\n",
    "                             for fn, f in tg_data_files_dict.get(tg).items())\n",
    "tg_drop_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2]} \n",
    "                                     for i in tg_drop_schema_change_rep]\n",
    "log_report(list_of_row_dict=tg_drop_schema_change_report_dict,columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema']) \n",
    "\n",
    "\n",
    "'''   Created TG Absolute New '''\n",
    "\n",
    "tg_newly_created_report_data = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                    for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "log_report(tg_newly_created_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema']) \n",
    "\n",
    "\n",
    "'''   Created TG to change schema(with new Schema) '''\n",
    "\n",
    "tg_re_created_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), f['Schema'], extract_info(f['Key']), 'Re-delivered') \n",
    "                                   if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                   else (tg, 'NAN', f['Schema'], extract_info(f['Key']), 'New File')\n",
    "                                    \n",
    "                                   for tg in tg_delta_drop_n_create \n",
    "                                   #for schema_new in src_delta['new_tg_schema_dict'].get(tg) \n",
    "                                   \n",
    "                                   for fn, f in src_delta['new_tg_files_dict'].get(tg).items() \n",
    "                                   )\n",
    "\n",
    "tg_recreated_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2], 'Desc': i[4]} \n",
    "                                          for i in tg_re_created_schema_change_rep]\n",
    "\n",
    "log_report(list_of_row_dict=tg_recreated_schema_change_report_dict,columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema', 'Desc'])\n",
    "\n",
    "\n",
    "'''   Retained TG with retained files, new files and dropped files '''\n",
    "\n",
    "tg_retained_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'Retained' }\n",
    "                           \n",
    "                            if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                            else {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'New File' }\n",
    "                            for tg in tg_existing \n",
    "                            for fn, f in src_delta['existing_tg_files_dict'].get(tg).items()]\n",
    "\n",
    "tg_retained_dropped_files = [extract_info(tg_data_files_dict.get(tg).get(fn)['Key']) \n",
    "                             for tg in tg_existing \n",
    "                             for fn in set(tg_data_files_dict.get(tg).keys()).difference(set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "                             \n",
    "tg_retained_dropped_files_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': tg_data_schema_dict[f['FileGrp']], 'Desc' : 'Dropped' }\n",
    "                                          for f in tg_retained_dropped_files]\n",
    "\n",
    "tg_retained_report_data.extend(tg_retained_dropped_files_report_data)\n",
    "log_report(tg_retained_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema', 'Desc'], sort_by = ['Taxonomy_Grp','Date']) \n",
    "\n",
    "#''' End '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute.use_bottleneck : bool\n",
      "    Use the bottleneck library to accelerate if it is installed,\n",
      "    the default is True\n",
      "    Valid values: False,True\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "compute.use_numexpr : bool\n",
      "    Use the numexpr library to accelerate computation if it is installed,\n",
      "    the default is True\n",
      "    Valid values: False,True\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.chop_threshold : float or None\n",
      "    if set to a float value, all float values smaller then the given threshold\n",
      "    will be displayed as exactly 0 by repr and friends.\n",
      "    [default: None] [currently: None]\n",
      "\n",
      "display.colheader_justify : 'left'/'right'\n",
      "    Controls the justification of column headers. used by DataFrameFormatter.\n",
      "    [default: right] [currently: left]\n",
      "\n",
      "display.column_space No description available.\n",
      "    [default: 12] [currently: 12]\n",
      "\n",
      "display.date_dayfirst : boolean\n",
      "    When True, prints and parses dates with the day first, eg 20/01/2005\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.date_yearfirst : boolean\n",
      "    When True, prints and parses dates with the year first, eg 2005/01/20\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.encoding : str/unicode\n",
      "    Defaults to the detected encoding of the console.\n",
      "    Specifies the encoding to be used for strings returned by to_string,\n",
      "    these are generally strings meant to be displayed on the console.\n",
      "    [default: UTF-8] [currently: UTF-8]\n",
      "\n",
      "display.expand_frame_repr : boolean\n",
      "    Whether to print out the full DataFrame repr for wide DataFrames across\n",
      "    multiple lines, `max_columns` is still respected, but the output will\n",
      "    wrap-around across multiple \"pages\" if its width exceeds `display.width`.\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.float_format : callable\n",
      "    The callable should accept a floating point number and return\n",
      "    a string with the desired format of the number. This is used\n",
      "    in some places like SeriesFormatter.\n",
      "    See formats.format.EngFormatter for an example.\n",
      "    [default: None] [currently: None]\n",
      "\n",
      "display.html.border : int\n",
      "    A ``border=value`` attribute is inserted in the ``<table>`` tag\n",
      "    for the DataFrame HTML repr.\n",
      "    [default: 1] [currently: 1]\n",
      "\n",
      "display.html.table_schema : boolean\n",
      "    Whether to publish a Table Schema representation for frontends\n",
      "    that support it.\n",
      "    (default: False)\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.html.use_mathjax : boolean\n",
      "    When True, Jupyter notebook will process table contents using MathJax,\n",
      "    rendering mathematical expressions enclosed by the dollar symbol.\n",
      "    (default: True)\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.large_repr : 'truncate'/'info'\n",
      "    For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can\n",
      "    show a truncated table (the default from 0.13), or switch to the view from\n",
      "    df.info() (the behaviour in earlier versions of pandas).\n",
      "    [default: truncate] [currently: truncate]\n",
      "\n",
      "display.latex.escape : bool\n",
      "    This specifies if the to_latex method of a Dataframe uses escapes special\n",
      "    characters.\n",
      "    Valid values: False,True\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.latex.longtable :bool\n",
      "    This specifies if the to_latex method of a Dataframe uses the longtable\n",
      "    format.\n",
      "    Valid values: False,True\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.latex.multicolumn : bool\n",
      "    This specifies if the to_latex method of a Dataframe uses multicolumns\n",
      "    to pretty-print MultiIndex columns.\n",
      "    Valid values: False,True\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.latex.multicolumn_format : bool\n",
      "    This specifies if the to_latex method of a Dataframe uses multicolumns\n",
      "    to pretty-print MultiIndex columns.\n",
      "    Valid values: False,True\n",
      "    [default: l] [currently: l]\n",
      "\n",
      "display.latex.multirow : bool\n",
      "    This specifies if the to_latex method of a Dataframe uses multirows\n",
      "    to pretty-print MultiIndex rows.\n",
      "    Valid values: False,True\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.latex.repr : boolean\n",
      "    Whether to produce a latex DataFrame representation for jupyter\n",
      "    environments that support it.\n",
      "    (default: False)\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.max_categories : int\n",
      "    This sets the maximum number of categories pandas should output when\n",
      "    printing out a `Categorical` or a Series of dtype \"category\".\n",
      "    [default: 8] [currently: 8]\n",
      "\n",
      "display.max_columns : int\n",
      "    If max_cols is exceeded, switch to truncate view. Depending on\n",
      "    `large_repr`, objects are either centrally truncated or printed as\n",
      "    a summary view. 'None' value means unlimited.\n",
      "\n",
      "    In case python/IPython is running in a terminal and `large_repr`\n",
      "    equals 'truncate' this can be set to 0 and pandas will auto-detect\n",
      "    the width of the terminal and print a truncated object which fits\n",
      "    the screen width. The IPython notebook, IPython qtconsole, or IDLE\n",
      "    do not run in a terminal and hence it is not possible to do\n",
      "    correct auto-detection.\n",
      "    [default: 20] [currently: None]\n",
      "\n",
      "display.max_colwidth : int\n",
      "    The maximum width in characters of a column in the repr of\n",
      "    a pandas data structure. When the column overflows, a \"...\"\n",
      "    placeholder is embedded in the output.\n",
      "    [default: 50] [currently: 1000]\n",
      "\n",
      "display.max_info_columns : int\n",
      "    max_info_columns is used in DataFrame.info method to decide if\n",
      "    per column information will be printed.\n",
      "    [default: 100] [currently: 100]\n",
      "\n",
      "display.max_info_rows : int or None\n",
      "    df.info() will usually show null-counts for each column.\n",
      "    For large frames this can be quite slow. max_info_rows and max_info_cols\n",
      "    limit this null check only to frames with smaller dimensions than\n",
      "    specified.\n",
      "    [default: 1690785] [currently: 1690785]\n",
      "\n",
      "display.max_rows : int\n",
      "    If max_rows is exceeded, switch to truncate view. Depending on\n",
      "    `large_repr`, objects are either centrally truncated or printed as\n",
      "    a summary view. 'None' value means unlimited.\n",
      "\n",
      "    In case python/IPython is running in a terminal and `large_repr`\n",
      "    equals 'truncate' this can be set to 0 and pandas will auto-detect\n",
      "    the height of the terminal and print a truncated object which fits\n",
      "    the screen height. The IPython notebook, IPython qtconsole, or\n",
      "    IDLE do not run in a terminal and hence it is not possible to do\n",
      "    correct auto-detection.\n",
      "    [default: 60] [currently: None]\n",
      "\n",
      "display.max_seq_items : int or None\n",
      "    when pretty-printing a long sequence, no more then `max_seq_items`\n",
      "    will be printed. If items are omitted, they will be denoted by the\n",
      "    addition of \"...\" to the resulting string.\n",
      "\n",
      "    If set to None, the number of items to be printed is unlimited.\n",
      "    [default: 100] [currently: 100]\n",
      "\n",
      "display.memory_usage : bool, string or None\n",
      "    This specifies if the memory usage of a DataFrame should be displayed when\n",
      "    df.info() is called. Valid values True,False,'deep'\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.multi_sparse : boolean\n",
      "    \"sparsify\" MultiIndex display (don't display repeated\n",
      "    elements in outer levels within groups)\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.notebook_repr_html : boolean\n",
      "    When True, IPython notebook will use html representation for\n",
      "    pandas objects (if it is available).\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "display.pprint_nest_depth : int\n",
      "    Controls the number of nested levels to process when pretty-printing\n",
      "    [default: 3] [currently: 3]\n",
      "\n",
      "display.precision : int\n",
      "    Floating point output precision (number of significant digits). This is\n",
      "    only a suggestion\n",
      "    [default: 6] [currently: 6]\n",
      "\n",
      "display.show_dimensions : boolean or 'truncate'\n",
      "    Whether to print out dimensions at the end of DataFrame repr.\n",
      "    If 'truncate' is specified, only print out the dimensions if the\n",
      "    frame is truncated (e.g. not display all rows and/or columns)\n",
      "    [default: truncate] [currently: truncate]\n",
      "\n",
      "display.unicode.ambiguous_as_wide : boolean\n",
      "    Whether to use the Unicode East Asian Width to calculate the display text\n",
      "    width.\n",
      "    Enabling this may affect to the performance (default: False)\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.unicode.east_asian_width : boolean\n",
      "    Whether to use the Unicode East Asian Width to calculate the display text\n",
      "    width.\n",
      "    Enabling this may affect to the performance (default: False)\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "display.width : int\n",
      "    Width of the display in characters. In case python/IPython is running in\n",
      "    a terminal this can be set to None and pandas will correctly auto-detect\n",
      "    the width.\n",
      "    Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a\n",
      "    terminal and hence it is not possible to correctly detect the width.\n",
      "    [default: 80] [currently: 1000]\n",
      "\n",
      "html.border : int\n",
      "    A ``border=value`` attribute is inserted in the ``<table>`` tag\n",
      "    for the DataFrame HTML repr.\n",
      "    [default: 1] [currently: 1]\n",
      "    (Deprecated, use `display.html.border` instead.)\n",
      "\n",
      "io.excel.xls.writer : string\n",
      "    The default Excel writer engine for 'xls' files. Available options:\n",
      "    auto, xlwt.\n",
      "    [default: auto] [currently: auto]\n",
      "\n",
      "io.excel.xlsm.writer : string\n",
      "    The default Excel writer engine for 'xlsm' files. Available options:\n",
      "    auto, openpyxl.\n",
      "    [default: auto] [currently: auto]\n",
      "\n",
      "io.excel.xlsx.writer : string\n",
      "    The default Excel writer engine for 'xlsx' files. Available options:\n",
      "    auto, openpyxl, xlsxwriter.\n",
      "    [default: auto] [currently: auto]\n",
      "\n",
      "io.hdf.default_format : format\n",
      "    default format writing format, if None, then\n",
      "    put will default to 'fixed' and append will default to 'table'\n",
      "    [default: None] [currently: None]\n",
      "\n",
      "io.hdf.dropna_table : boolean\n",
      "    drop ALL nan rows when appending to a table\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "io.parquet.engine : string\n",
      "    The default parquet reader/writer engine. Available options:\n",
      "    'auto', 'pyarrow', 'fastparquet', the default is 'auto'\n",
      "    [default: auto] [currently: auto]\n",
      "\n",
      "mode.chained_assignment : string\n",
      "    Raise an exception, warn, or no action if trying to use chained assignment,\n",
      "    The default is warn\n",
      "    [default: warn] [currently: warn]\n",
      "\n",
      "mode.sim_interactive : boolean\n",
      "    Whether to simulate interactive mode for purposes of testing\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "mode.use_inf_as_na : boolean\n",
      "    True means treat None, NaN, INF, -INF as NA (old way),\n",
      "    False means None and NaN are null, but INF, -INF are not NA\n",
      "    (new way).\n",
      "    [default: False] [currently: False]\n",
      "\n",
      "mode.use_inf_as_null : boolean\n",
      "    use_inf_as_null had been deprecated and will be removed in a future\n",
      "    version. Use `use_inf_as_na` instead.\n",
      "    [default: False] [currently: False]\n",
      "    (Deprecated, use `mode.use_inf_as_na` instead.)\n",
      "\n",
      "plotting.matplotlib.register_converters : bool\n",
      "    Whether to register converters with matplotlib's units registry for\n",
      "    dates, times, datetimes, and Periods. Toggling to False will remove\n",
      "    the converters, restoring any converters that pandas overwrote.\n",
      "    [default: True] [currently: True]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.describe_option()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gen_Report(exposed_tg_all = None, \n",
    "               exposed_dropped_tg_all = None,\n",
    "#                invalid_tg_with_dup_schema = None, \n",
    "#                invalid_tg_with_dup_target = None,\n",
    "               invalid_new_tg_schema_conflict = None,\n",
    "               src_delta  = None,\n",
    "               tg_new  = None, \n",
    "               tg_existing = None,\n",
    "               invalid_files_set = None,\n",
    "               invalid_schema_files = None,\n",
    "               tg_data_files_dict = None, \n",
    "               tg_data_schema_dict = None, \n",
    "               tg_delta_drop_n_create = None, \n",
    "               tg_delta_create = None):\n",
    "\n",
    "    ''' Report '''\n",
    "    \n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "    '''  Exposed TG Report  '''\n",
    "\n",
    "    exposed_tg_report_data = [i.get_dict() for i in exposed_tg_all]\n",
    "    report_title = 'Exposed TG Report'\n",
    "    log_report(exposed_tg_report_data,  columns=['tg_name', 'key_cols', 'target_cols', 'location'], \n",
    "               sort_by='tg_name', report_title=report_title)\n",
    "\n",
    "\n",
    "    '''  Exposed Dropped TG Report '''\n",
    "\n",
    "    exposed_tg_dropped_report_data = [i.get_dict() for i in exposed_dropped_tg_all]\n",
    "    report_title = 'Exposed Dropped TG Report'\n",
    "    log_report(exposed_tg_dropped_report_data, columns=['tg_name', 'key_cols', 'target_cols', 'location'], \n",
    "               sort_by='tg_name', report_title=report_title)\n",
    "\n",
    "\n",
    "    \n",
    "    '''   Dropped TG Completely '''\n",
    "\n",
    "    tg_dropped_rep_gen =(extract_info(f['Key']) for i in  tg_dropped for fn, f in tg_data_files_dict.get(i).items())\n",
    "    tg_dropped_report_dict =  [{'Taxonomy_Grp':i['FileGrp'], 'File_Name':i['FileName'], 'Date':i['Date'], 'Schema': tg_data_schema_dict[i['FileGrp']] }\n",
    "                               for i in tg_dropped_rep_gen] \n",
    "    report_title = 'Dropped TG Completely'\n",
    "    log_report(list_of_row_dict=tg_dropped_report_dict, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Dropped TG to change schema '''\n",
    "\n",
    "    tg_drop_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), schema_new, extract_info(f['Key'])) \n",
    "                                 for tg in tg_delta_drop_n_create \n",
    "                                 for schema_new in src_delta['new_tg_schema_dict'].get(tg)\n",
    "                                 for fn, f in tg_data_files_dict.get(tg).items())\n",
    "    tg_drop_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2]} \n",
    "                                         for i in tg_drop_schema_change_rep]\n",
    "    report_title = 'Dropped TG to change schema'\n",
    "    log_report(list_of_row_dict=tg_drop_schema_change_report_dict, \n",
    "               columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema'], \n",
    "               sort_by = ['Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Created TG Absolute New '''\n",
    "\n",
    "    tg_newly_created_report_data = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                        for i in  tg_delta_create for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Created TG Absolute New'\n",
    "    log_report(tg_newly_created_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Created TG to change schema(with new Schema) '''\n",
    "\n",
    "    tg_re_created_schema_change_rep = ((tg, tg_data_schema_dict.get(tg), f['Schema'], extract_info(f['Key']), 'Re-delivered') \n",
    "                                       if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                       else (tg, 'NAN', f['Schema'], extract_info(f['Key']), 'New File')\n",
    "\n",
    "                                       for tg in tg_delta_drop_n_create \n",
    "                                       #for schema_new in src_delta['new_tg_schema_dict'].get(tg) \n",
    "\n",
    "                                       for fn, f in src_delta['new_tg_files_dict'].get(tg).items() \n",
    "                                       )\n",
    "\n",
    "    tg_recreated_schema_change_report_dict = [{'Grp' : i[0], 'File_Name': i[3]['FileName'], 'Date': i[3]['Date'], 'Old_Schema' : i[1], 'New_Schema' : i[2], 'Desc': i[4]} \n",
    "                                              for i in tg_re_created_schema_change_rep]\n",
    "    report_title = 'Created TG to change schema(with new Schema)'\n",
    "    log_report(list_of_row_dict=tg_recreated_schema_change_report_dict, \n",
    "               columns=['Grp','File_Name','Date', 'Old_Schema', 'New_Schema', 'Desc'], \n",
    "               sort_by = ['Grp','Date'], report_title=report_title)\n",
    "\n",
    "\n",
    "    '''   Retained TG with retained files, new files and dropped files '''\n",
    "\n",
    "    tg_retained_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'Retained' }\n",
    "\n",
    "                                if tg_data_files_dict.get(tg).get(fn) is not None \n",
    "                                else {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Desc' : 'New File' }\n",
    "                                for tg in tg_existing \n",
    "                                for fn, f in src_delta['existing_tg_files_dict'].get(tg).items()]\n",
    "\n",
    "    tg_retained_dropped_files = [extract_info(tg_data_files_dict.get(tg).get(fn)['Key']) \n",
    "                                 for tg in tg_existing \n",
    "                                 for fn in set(tg_data_files_dict.get(tg).keys()).difference(\n",
    "                                     set(src_delta['existing_tg_files_dict'].get(tg).keys()))]\n",
    "\n",
    "\n",
    "    tg_retained_dropped_files_report_data = [{'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': tg_data_schema_dict[f['FileGrp']], 'Desc' : 'Dropped' }\n",
    "                                              for f in tg_retained_dropped_files]\n",
    "\n",
    "    tg_retained_report_data.extend(tg_retained_dropped_files_report_data)\n",
    "    report_title = 'Retained TG with retained files, new files and dropped files'\n",
    "    log_report(tg_retained_report_data, columns=['Taxonomy_Grp','File_Name','Date', 'Schema', 'Desc'], \n",
    "               header_align='left', sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "    \n",
    "#     '''  Invalid TG due to schema conflict/already used Report '''\n",
    "\n",
    "#     invalid_tg_with_dup_schema_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "#                                       for i in  invalid_tg_with_dup_schema \n",
    "#                                       for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "#     report_title = 'Invalid TG due to schema conflict/already used Report'\n",
    "#     log_report(invalid_tg_with_dup_schema_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "#                sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "#     '''  Invalid TG due to Target Column conflict/already used Report '''\n",
    "\n",
    "#     invalid_tg_with_dup_target_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "#                         for i in  invalid_tg_with_dup_target for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "#     report_title = 'Invalid TG due to Target Column conflict/already used Report'\n",
    "#     log_report(invalid_tg_with_dup_target_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "#                sort_by = ['Taxonomy_Grp','Date'], report_title=report_title)\n",
    "    \n",
    "    \n",
    "    '''  Invalid New TG due to Schema conflict among files Report '''\n",
    "\n",
    "    invalid_tg_with_schema_conflict_rep = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'] }\n",
    "                        for i in  invalid_new_tg_schema_conflict for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Invalid New TG due to Schema conflict among files Report'\n",
    "    log_report(invalid_tg_with_schema_conflict_rep, columns=['Taxonomy_Grp','File_Name','Date', 'Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title)\n",
    "\n",
    "\n",
    "    ''' Invalid files from retained grp due to schema or target mismatch with previously delivered files for same'''\n",
    "\n",
    "    partially_invalid_tg_set = tg_new.intersection(tg_existing)\n",
    "    partially_invalid_tg_report = [ {'Taxonomy_Grp':f['FileGrp'], 'File_Name':f['FileName'], 'Date':f['Date'], 'Schema': f['Schema'], 'Grp_Schema': tg_data_schema_dict[i] } \n",
    "                                   for i in partially_invalid_tg_set  \n",
    "                                   for fn, f in src_delta['new_tg_files_dict'].get(i).items()]\n",
    "    report_title = 'Invalid files from retained grp due to schema or target mismatch'\n",
    "    log_report(partially_invalid_tg_report, columns=['Taxonomy_Grp','File_Name','Date', 'Schema','Grp_Schema'], \n",
    "               sort_by = ['Taxonomy_Grp','Date'], report_title=report_title) \n",
    "\n",
    "\n",
    "    '''   Invalid file not match with required file pattern '''\n",
    "\n",
    "    invalid_files_report_data = [{'File_Name' : filename_by_key(i)} for i in invalid_files_set]\n",
    "    report_title = 'Invalid file not match with required file pattern'\n",
    "    log_report(invalid_files_report_data,  columns=['File_Name'], \n",
    "               sort_by = ['File_Name'], report_title=report_title)\n",
    "\n",
    "\n",
    "    '''   Invalid file not match with required schema pattern '''\n",
    "\n",
    "    invalid_schema_files_rep_data = [{'File_Name' : filename_by_key(i[0]), 'Schema': i[1], 'Reason' : i[2]} for i in invalid_schema_files]\n",
    "    report_title = 'Invalid file not match with required schema pattern'\n",
    "    log_report(invalid_schema_files_rep_data,  columns=['File_Name', 'Schema','Reason'], \n",
    "               header_align='left', sort_by = ['File_Name'], report_title=report_title)\n",
    "\n",
    "\n",
    "    #''' End '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Exposed TG Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **            tg_name key_cols   target_cols                location                                     \n",
      " **      0    tg0     [key_a0]                [target_a0]   s3://qubole-ford/taxonomy_cs/test1/data/tg0\n",
      " **      1    tg1     [key_a1]                [target_a1]   s3://qubole-ford/taxonomy_cs/test1/data/tg1\n",
      " **      2   tg11     [key_a9]                [target_a9]  s3://qubole-ford/taxonomy_cs/test1/data/tg11\n",
      " **      3   tg14     [key_a7]                [target_a7]  s3://qubole-ford/taxonomy_cs/test1/data/tg14\n",
      " **      4   tg16    [key_a16]  [target_a16, target_a162]  s3://qubole-ford/taxonomy_cs/test1/data/tg16\n",
      " **      5   tg17    [key_a16]    [target_a16, target_a9]  s3://qubole-ford/taxonomy_cs/test1/data/tg17\n",
      " **      6    tg2    [key_a21]               [target_a21]   s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      7    tg4     [key_a4]                [target_a4]   s3://qubole-ford/taxonomy_cs/test1/data/tg4\n",
      " **      8    tg5     [key_a5]                [target_a5]   s3://qubole-ford/taxonomy_cs/test1/data/tg5\n",
      " **      9    tg6     [key_a6]               [target_a61]   s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **      10   tg7     [key_a7]                [target_a7]   s3://qubole-ford/taxonomy_cs/test1/data/tg7\n",
      " **      11   tg8     [key_a8]                [target_a7]   s3://qubole-ford/taxonomy_cs/test1/data/tg8\n",
      " **      12   tg9     [key_a9]                [target_a9]   s3://qubole-ford/taxonomy_cs/test1/data/tg9\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Exposed Dropped TG Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           tg_name key_cols  target_cols  location                                    \n",
      " **      0  tg2     [key_a2]  [target_a2]  s3://qubole-ford/taxonomy_cs/test1/data/tg2\n",
      " **      1  tg3     [key_a3]  [target_a3]  s3://qubole-ford/taxonomy_cs/test1/data/tg3\n",
      " **      2  tg6     [key_a6]  [target_a6]  s3://qubole-ford/taxonomy_cs/test1/data/tg6\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Dropped TG Completely\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema           \n",
      " **      0  tg3          tg3_2020-11-01_ford.csv  2020-11-01  key_a3,target_a3\n",
      " **      1  tg3          tg3_2020-11-02_ford.csv  2020-11-02  key_a3,target_a3\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Dropped TG to change schema\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema         \n",
      " **      0  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21\n",
      " **      2  tg2  tg2_2020-11-04_ford.csv  2020-11-04  key_a2,target_a2  key_a21,target_a21\n",
      " **      3  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61\n",
      " **      4  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Created TG Absolute New\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **            Taxonomy_Grp File_Name                 Date        Schema                         \n",
      " **      0    tg0          tg0_2020-11-02_ford.csv  2020-11-02                key_a0,target_a0\n",
      " **      1   tg11         tg11_2020-11-01_ford.csv  2020-11-01                key_a9,target_a9\n",
      " **      2   tg14         tg14_2020-11-01_ford.csv  2020-11-01                key_a7,target_a7\n",
      " **      3   tg14         tg14_2020-11-02_ford.csv  2020-11-02                key_a7,target_a7\n",
      " **      4   tg16         tg16_2020-11-01_ford.csv  2020-11-01  key_a16,target_a16,target_a162\n",
      " **      5   tg16         tg16_2020-11-02_ford.csv  2020-11-02  key_a16,target_a16,target_a162\n",
      " **      6   tg16         tg16_2020-11-03_ford.csv  2020-11-03  key_a16,target_a16,target_a162\n",
      " **      7   tg17         tg17_2020-11-01_ford.csv  2020-11-01    key_a16,target_a16,target_a9\n",
      " **      8    tg4          tg4_2020-11-01_ford.csv  2020-11-01                key_a4,target_a4\n",
      " **      9    tg4          tg4_2020-11-02_ford.csv  2020-11-02                key_a4,target_a4\n",
      " **      10   tg4          tg4_2020-11-03_ford.csv  2020-11-03                key_a4,target_a4\n",
      " **      11   tg8          tg8_2020-11-01_ford.csv  2020-11-01                key_a8,target_a7\n",
      " **      12   tg9          tg9_2020-11-01_ford.csv  2020-11-01                key_a9,target_a9\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Created TG to change schema(with new Schema)\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Grp  File_Name                Date        Old_Schema        New_Schema          Desc         \n",
      " **      0  tg2  tg2_2020-11-01_ford.csv  2020-11-01  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      1  tg2  tg2_2020-11-02_ford.csv  2020-11-02  key_a2,target_a2  key_a21,target_a21  Re-delivered\n",
      " **      2  tg2  tg2_2020-11-03_ford.csv  2020-11-03               NAN  key_a21,target_a21      New File\n",
      " **      3  tg6  tg6_2020-11-01_ford.csv  2020-11-01  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **      4  tg6  tg6_2020-11-02_ford.csv  2020-11-02  key_a6,target_a6   key_a6,target_a61  Re-delivered\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Retained TG with retained files, new files and dropped files\n",
      " ****          \n",
      " ******************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema            Desc     \n",
      " **      0  tg1          tg1_2020-11-01_ford.csv  2020-11-01  key_a1,target_a1  Retained\n",
      " **      1  tg1          tg1_2020-11-02_ford.csv  2020-11-02  key_a1,target_a1  New File\n",
      " **      2  tg5          tg5_2020-11-01_ford.csv  2020-11-01  key_a5,target_a5  Retained\n",
      " **      3  tg5          tg5_2020-11-02_ford.csv  2020-11-02  key_a5,target_a5  Retained\n",
      " **      4  tg5          tg5_2020-11-03_ford.csv  2020-11-03  key_a5,target_a5  New File\n",
      " **      5  tg5          tg5_2020-11-05_ford.csv  2020-11-05  key_a5,target_a5   Dropped\n",
      " **      6  tg7          tg7_2020-11-01_ford.csv  2020-11-01  key_a7,target_a7  Retained\n",
      " **      7  tg7          tg7_2020-11-02_ford.csv  2020-11-02  key_a7,target_a7  Retained\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid New TG due to Schema conflict among files Report\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                 Date        Schema               \n",
      " **      0  tg10         tg10_2020-11-01_ford.csv  2020-11-01     key_a10,target_a9\n",
      " **      1  tg10         tg10_2020-11-02_ford.csv  2020-11-02    key_a10,target_a10\n",
      " **      2  tg15         tg15_2020-11-01_ford.csv  2020-11-01  key_a151,target_a151\n",
      " **      3  tg15         tg15_2020-11-02_ford.csv  2020-11-02  key_a151,target_a151\n",
      " **      4  tg15         tg15_2020-11-03_ford.csv  2020-11-03  key_a151,target_a151\n",
      " **      5  tg15         tg15_2020-11-05_ford.csv  2020-11-05  key_a152,target_a152\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid files from retained grp due to schema or target mismatch\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           Taxonomy_Grp File_Name                Date        Schema              Grp_Schema       \n",
      " **      0  tg5          tg5_2020-11-04_ford.csv  2020-11-04  key_a51,target_a51  key_a5,target_a5\n",
      " **      1  tg7          tg7_2020-11-03_ford.csv  2020-11-03   key_a7,target_a71  key_a7,target_a7\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid file not match with required file pattern\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name              \n",
      " **      0  tg0_202-11-01_ford.csv\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      " ******************************************************************************************************\n",
      " ******************************************************************************************************\n",
      " ****          \n",
      " ****    Invalid file not match with required schema pattern\n",
      " ****          \n",
      " ******************************************************************************************************\n",
      " **          \n",
      " **          \n",
      " **           File_Name                Schema            Reason                                                                            \n",
      " **      0  tg0_2020-11-03_ford.csv  key_a0, targe_a0  At least one Target column is required! \\nAll given columns should Key or Target!\n",
      " **          \n",
      " **          \n",
      " ******************************************************************************************************\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Gen_Report(exposed_tg_all = exposed_tg_all, \n",
    "               exposed_dropped_tg_all = exposed_dropped_tg_all,\n",
    "#                invalid_tg_with_dup_schema = invalid_tg_with_dup_schema, \n",
    "#                invalid_tg_with_dup_target = invalid_tg_with_dup_target,\n",
    "               invalid_new_tg_schema_conflict = invalid_tg_all,\n",
    "               src_delta  = src_delta,\n",
    "               tg_new  = tg_new, \n",
    "               tg_existing = tg_existing,\n",
    "               invalid_files_set = invalid_files_set,\n",
    "               invalid_schema_files = invalid_schema_files,\n",
    "               tg_data_files_dict = tg_data_files_dict, \n",
    "               tg_data_schema_dict = tg_data_schema_dict, \n",
    "               tg_delta_drop_n_create = tg_delta_drop_n_create, \n",
    "               tg_delta_create = tg_delta_create\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.__str__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ~/raj_fb/anaconda3/envs/aws_util/lib/python3.7/site-packages/pandas/core/base.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version='1.0' encoding='UTF-8'?>\r\n",
      "<configroot version=\"12.1\">\r\n",
      "\t<set>\r\n",
      "\t\t<name>CS_TAXONOMY_LMT_SCHEMA_SET</name>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg9</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a9</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a9</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg9/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg2</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a21</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a21</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg2/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg0</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a0</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a0</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg0/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg16</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a16</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a16</attr>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a162</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg16/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg11</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a9</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a9</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg11/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg8</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a8</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a7</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg8/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg17</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a16</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a16</attr>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a9</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg17/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg14</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a7</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a7</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg14/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg6</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a6</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a61</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg6/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg4</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a4</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a4</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg4/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg5</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a5</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a5</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg5/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg1</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a1</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a1</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg1/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg7</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">key_a7</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr datatype=\"STRING\">target_a7</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t\t<partitionby_columns>\r\n",
      "\t\t\t</partitionby_columns>\r\n",
      "\t\t\t<row_delimiter>\r\n",
      "\t\t\t\t<!-- Row separator -->\r\n",
      "\t\t\t\t<val>'\\n'</val>\r\n",
      "\t\t\t</row_delimiter>\r\n",
      "\t\t\t<column_delimiter>\r\n",
      "\t\t\t\t<!-- Column separator -->\r\n",
      "\t\t\t\t<val>','</val>\r\n",
      "\t\t\t</column_delimiter>\r\n",
      "\t\t\t<serde>\r\n",
      "\t\t\t\t<val>'org.apache.hadoop.hive.serde2.OpenCSVSerde'</val>\r\n",
      "\t\t\t</serde>\r\n",
      "\t\t\t<serde_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</serde_properties>\r\n",
      "\t\t\t<table_properties>\r\n",
      "\t\t\t\t<val/>\r\n",
      "\t\t\t</table_properties>\r\n",
      "\t\t\t<storage_type>\r\n",
      "\t\t\t\t<!-- Storage or compression type of files, ex: TEXTFILE, ORC, PARQUET -->\r\n",
      "\t\t\t\t<val>TEXTFILE</val>\r\n",
      "\t\t\t</storage_type>\r\n",
      "\t\t\t<location>\r\n",
      "\t\t\t\t<val>s3://qubole-ford/taxonomy_cs/test1/data/tg7/</val>\r\n",
      "\t\t\t</location>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "    </set>\r\n",
      "   \r\n",
      "    \r\n",
      "    <set>\r\n",
      "\t\t<name>DROP_CS_TAXONOMY_LMT_SCHEMA_SET</name>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg3</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr>key_a3</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr>target_a3</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg2</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr>key_a2</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr>target_a2</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg6</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\t\t\t<key-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr>key_a6</attr>\r\n",
      "                \r\n",
      "\t\t\t</key-columns>\r\n",
      "\t\t\t<target-columns>\r\n",
      "                \r\n",
      "\t\t\t\t<attr>target_a6</attr>\r\n",
      "                \r\n",
      "\t\t\t</target-columns>\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "    </set>\r\n",
      "    \r\n",
      "   <set>\r\n",
      "\t\t<name>CS_TAXONOMY_GROUP_ORDERED_SET</name>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg0</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>1</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg1</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>2</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg11</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>3</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg14</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>4</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg16</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>5</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg17</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>6</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg2</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>7</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg4</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>8</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg5</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>9</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg6</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>10</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg7</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>11</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg8</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>12</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "\t\t<elements>\r\n",
      "\t\t\t<subsource_name>\r\n",
      "\t\t\t\t<!-- name of the subsource, has to be unique across all datasource -->\r\n",
      "\t\t\t\t<val>tg9</val>\r\n",
      "\t\t\t</subsource_name>\r\n",
      "\r\n",
      "\t\t\t<precedence_order>              \r\n",
      "\t\t\t\t<val>13</val>\r\n",
      "\t\t\t</precedence_order>\r\n",
      "\r\n",
      "\t\t</elements>\r\n",
      "        \r\n",
      "    </set>\r\n",
      "    \r\n",
      "</configroot>\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/vbhargava/feature_test0/temp/taxo_config_xmls/test1.xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://qubole-ford/warehouse/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data['s3_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/abc/taxonomy_cs/lmt/input/'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def path_resolve(parent_path, relative_path):\n",
    "    path_tk = parent_path.split('//')\n",
    "    if len(path_tk) == 2: \n",
    "        abc = os.path.join(path_tk[1], relative_path)\n",
    "    else:\n",
    "        abc = os.path.join(path_tk[0], relative_path)\n",
    "    folder_token_stk = abc.split('/')\n",
    "\n",
    "    cnt = 0\n",
    "    for idx, tk in enumerate(folder_token_stk):\n",
    "        if tk == '.':\n",
    "            pass\n",
    "        elif tk == '..':\n",
    "            cnt = cnt-1\n",
    "        else:\n",
    "            folder_token_stk[cnt] = tk\n",
    "            cnt = cnt+1\n",
    "\n",
    "    if len(path_tk) == 2:\n",
    "        return '{}//{}'.format(path_tk[0], '/'.join(folder_token_stk[:cnt]))\n",
    "    else:\n",
    "        return '{}'.format('/'.join(folder_token_stk[:cnt]))\n",
    "path_resolve(\"s3://qubole-ford/warehouse/yts/uiy/tac\", '../../../../../../kv')\n",
    "path_resolve(config_data['s3_location'], '../taxonomy_cs/lmt/input/')\n",
    "path_resolve('/home/abc/ddec', '../taxonomy_cs/lmt/input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "file_dir = Path( os.path.join(config_data['s3_location'], '../../abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/vbhargava/feature_test0/msaction_backend/common/BU3.0_core/util/Py_utils/taxonomy_utils/notebooks/s3:/abc')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(file_dir).resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_util",
   "language": "python",
   "name": "aws_util"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
